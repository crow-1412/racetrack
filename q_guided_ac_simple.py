"""
Q-Guided Actor-Criticç®—æ³•

ç»“åˆQ-Learningå’ŒActor-Criticä¼˜åŠ¿çš„åˆ›æ–°æ–¹æ³•

æ ¸å¿ƒæ€æƒ³ï¼š
1. ç¬¬ä¸€é˜¶æ®µï¼šç”¨Q-Learningå¿«é€Ÿå­¦ä¹ å‡†ç¡®çš„Qå€¼
2. ç¬¬äºŒé˜¶æ®µï¼šç”¨Qè¡¨æŒ‡å¯¼ç¥ç»ç½‘ç»œå­¦ä¹  
3. ç¬¬ä¸‰é˜¶æ®µï¼šç¥ç»ç½‘ç»œç‹¬ç«‹ä¼˜åŒ–ç­–ç•¥

ä¼˜åŠ¿ï¼š
- ç»“åˆQ-Learningçš„ç²¾ç¡®æ€§å’ŒActor-Criticçš„æ³›åŒ–èƒ½åŠ›
- ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å®ç°çŸ¥è¯†è¿ç§»
- åŠ¨æ€æƒé‡è°ƒæ•´ç¡®ä¿å¹³æ»‘è¿‡æ¸¡
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random
from typing import Tuple, List, Dict, Any
from collections import deque
from racetrack_env import RacetrackEnv


class QGuidedNetwork(nn.Module):
    """Q-Guidedç½‘ç»œï¼šä¸‰ä¸ªè¾“å‡ºå¤´çš„æ··åˆæ¶æ„"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        
        # å…±äº«ç‰¹å¾å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # ä¸‰ä¸ªè¾“å‡ºå¤´
        self.actor_head = nn.Linear(hidden_dim // 2, action_dim)  # ç­–ç•¥
        self.critic_head = nn.Linear(hidden_dim // 2, 1)         # çŠ¶æ€ä»·å€¼
        self.q_head = nn.Linear(hidden_dim // 2, action_dim)     # Qå€¼
        
        # åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=1.0)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        features = self.shared(state)
        action_probs = F.softmax(self.actor_head(features), dim=-1)
        value = self.critic_head(features)
        q_values = self.q_head(features)
        return action_probs, value, q_values


class QGuidedActorCritic:
    """Q-Guided Actor-Criticç®—æ³•ä¸»ç±»"""
    
    def __init__(self, env: RacetrackEnv, lr=0.002, gamma=0.98, 
                 alpha_q=0.3, epsilon=0.2):
        self.env = env
        self.gamma = gamma
        self.alpha_q = alpha_q
        self.epsilon = epsilon
        
        # ç½‘ç»œå’ŒQè¡¨
        self.network = QGuidedNetwork(10, env.n_actions)  # å¢åŠ åˆ°10ç»´çŠ¶æ€ç‰¹å¾
        self.Q_table = {}  # Q-Learningè¡¨æ ¼
        
        # ä¼˜åŒ–å™¨ï¼ˆæé«˜å­¦ä¹ ç‡ï¼‰
        self.actor_optimizer = optim.AdamW(self.network.actor_head.parameters(), lr=lr*1.0)
        self.critic_optimizer = optim.AdamW(self.network.critic_head.parameters(), lr=lr*0.8)
        self.q_optimizer = optim.AdamW(self.network.q_head.parameters(), lr=lr*1.5)
        
        # è®­ç»ƒé˜¶æ®µæ§åˆ¶ï¼ˆè°ƒæ•´æ¯”ä¾‹ï¼Œæ›´å¿«è¿›å…¥æ··åˆé˜¶æ®µï¼‰
        self.training_phase = "q_learning"
        self.phase_episodes = {"q_learning": 300, "hybrid": 400, "actor_critic": 200}
        self.current_episode = 0
        self.q_weight = 1.0    # Qè¡¨æƒé‡
        self.ac_weight = 0.0   # ç¥ç»ç½‘ç»œæƒé‡
        
        # ç»éªŒç¼“å†²ï¼ˆå¢å¤§ç¼“å†²åŒºï¼‰
        self.buffer = deque(maxlen=256)
        
        print("ğŸš€ ä¼˜åŒ–ç‰ˆQ-Guided Actor-Criticåˆå§‹åŒ–å®Œæˆ")
        print(f"è®­ç»ƒè®¡åˆ’ï¼šQ-Learning({self.phase_episodes['q_learning']}) â†’ "
              f"æ··åˆ({self.phase_episodes['hybrid']}) â†’ "
              f"Actor-Critic({self.phase_episodes['actor_critic']})")
    
    def state_to_tensor(self, state):
        """çŠ¶æ€è½¬ç‰¹å¾å‘é‡ï¼ˆå¢å¼ºåˆ°10ç»´ï¼‰"""
        x, y, vx, vy = state
        
        # åŸºç¡€å½’ä¸€åŒ–
        norm_x = x / 31.0
        norm_y = y / 16.0
        norm_vx = vx / self.env.max_speed
        norm_vy = vy / self.env.max_speed
        
        # åˆ°ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_dist = float('inf')
        goal_dir_x, goal_dir_y = 0, 0
        best_goal = None
        
        for gx, gy in self.env.goal_positions:
            dist = np.sqrt((x-gx)**2 + (y-gy)**2)
            if dist < min_dist:
                min_dist = dist
                best_goal = (gx, gy)
                if dist > 0:
                    goal_dir_x = (gx-x) / dist  # ä¿®æ­£æ–¹å‘
                    goal_dir_y = (gy-y) / dist
        
        norm_dist = min_dist / np.sqrt(31**2 + 16**2)
        
        # é€Ÿåº¦å¯¹é½åº¦ï¼ˆæœå‘ç›®æ ‡ï¼‰
        vel_align = 0.0
        if min_dist > 0:
            vel_mag = np.sqrt(vx**2 + vy**2)
            if vel_mag > 0:
                vel_align = (vx*goal_dir_x + vy*goal_dir_y) / vel_mag
        
        # æ–°å¢ç‰¹å¾ï¼šé€Ÿåº¦å¤§å°å’Œåˆ°ç›®æ ‡çš„æ›¼å“ˆé¡¿è·ç¦»
        vel_magnitude = np.sqrt(vx**2 + vy**2) / (self.env.max_speed * np.sqrt(2))
        manhattan_dist = (abs(x - best_goal[0]) + abs(y - best_goal[1])) / (31 + 16) if best_goal else 1.0
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_dist, goal_dir_x, goal_dir_y, vel_align,
            vel_magnitude, manhattan_dist  # æ–°å¢ç‰¹å¾
        ], dtype=torch.float32)
    
    def get_q_value(self, state, action):
        """ä»Qè¡¨è·å–Qå€¼"""
        key = (state, action)
        return self.Q_table.get(key, 0.0)
    
    def set_q_value(self, state, action, value):
        """è®¾ç½®Qè¡¨ä¸­çš„Qå€¼"""
        self.Q_table[(state, action)] = value
    
    def update_phase(self):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µå’Œæƒé‡"""
        if self.current_episode < self.phase_episodes["q_learning"]:
            self.training_phase = "q_learning"
            self.q_weight, self.ac_weight = 1.0, 0.0
        elif self.current_episode < sum(list(self.phase_episodes.values())[:2]):
            self.training_phase = "hybrid"
            # æ›´å¹³æ»‘çš„è¿‡æ¸¡ï¼Œæ›´æ—©å¼€å§‹ç¥ç»ç½‘ç»œè®­ç»ƒ
            progress = (self.current_episode - self.phase_episodes["q_learning"]) / self.phase_episodes["hybrid"]
            self.q_weight = 1.0 - 0.8 * progress
            self.ac_weight = 0.8 * progress
        else:
            self.training_phase = "actor_critic"
            self.q_weight, self.ac_weight = 0.15, 1.0  # ä¿ç•™æ›´å¤šQè¡¨çŸ¥è¯†
    
    def select_action(self, state, training=True):
        """æ··åˆåŠ¨ä½œé€‰æ‹©ç­–ç•¥"""
        state_tensor = self.state_to_tensor(state)
        
        if training:
            if self.training_phase == "q_learning":
                # çº¯Q-Learningç­–ç•¥
                if random.random() < self.epsilon:
                    action = random.randint(0, self.env.n_actions - 1)
                else:
                    q_vals = [self.get_q_value(state, a) for a in range(self.env.n_actions)]
                    action = int(np.argmax(q_vals))
                
                # è®¡ç®—ç¥ç»ç½‘ç»œçš„log_probï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
                with torch.no_grad():
                    action_probs, _, _ = self.network(state_tensor)
                    action_dist = torch.distributions.Categorical(action_probs)
                    log_prob = action_dist.log_prob(torch.tensor(action))
                    
            elif self.training_phase == "hybrid":
                # æ··åˆç­–ç•¥ï¼šQè¡¨ + ç¥ç»ç½‘ç»œ
                with torch.no_grad():
                    action_probs, _, nn_q_values = self.network(state_tensor)
                
                table_q_values = torch.tensor([self.get_q_value(state, a) for a in range(self.env.n_actions)])
                combined_q = self.q_weight * table_q_values + self.ac_weight * nn_q_values
                
                if random.random() < self.epsilon:
                    action = random.randint(0, self.env.n_actions - 1)
                else:
                    action = int(torch.argmax(combined_q))
                
                action_dist = torch.distributions.Categorical(action_probs)
                log_prob = action_dist.log_prob(torch.tensor(action))
                
            else:  # actor_criticé˜¶æ®µ
                # çº¯ç¥ç»ç½‘ç»œç­–ç•¥
                with torch.no_grad():
                    action_probs, _, _ = self.network(state_tensor)
                
                if random.random() < self.epsilon:
                    action = random.randint(0, self.env.n_actions - 1)
                else:
                    action = torch.argmax(action_probs).item()
                
                action_dist = torch.distributions.Categorical(action_probs)
                log_prob = action_dist.log_prob(torch.tensor(action))
        else:
            # æµ‹è¯•æ¨¡å¼ï¼šä¼˜åŒ–ç­–ç•¥ï¼Œå®Œå…¨ä½¿ç”¨Qè¡¨ï¼ˆå¦‚æœè¶³å¤Ÿå¤§ï¼‰
            with torch.no_grad():
                action_probs, _, nn_q_values = self.network(state_tensor)
                
                if len(self.Q_table) > 200:  # å½“Qè¡¨è¶³å¤Ÿå¤§æ—¶ï¼Œå®Œå…¨ä¾èµ–Qè¡¨
                    table_q_values = torch.tensor([self.get_q_value(state, a) for a in range(self.env.n_actions)])
                    # å¦‚æœQè¡¨æœ‰è¶³å¤ŸçŸ¥è¯†ï¼Œç›´æ¥ä½¿ç”¨Qè¡¨ï¼Œå¦åˆ™ç»“åˆç¥ç»ç½‘ç»œ
                    if max(table_q_values) > 0:  # Qè¡¨æœ‰æ­£å€¼ç»éªŒ
                        action = int(torch.argmax(table_q_values))
                    else:
                        combined_q = 0.8 * table_q_values + 0.2 * nn_q_values
                        action = int(torch.argmax(combined_q))
                else:
                    # Qè¡¨ä¸å¤Ÿå¤§æ—¶ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œ
                    action = torch.argmax(action_probs).item()
                
                action_dist = torch.distributions.Categorical(action_probs)
                log_prob = action_dist.log_prob(torch.tensor(action))
        
        return action, log_prob
    
    def q_learning_update(self, state, action, reward, next_state, done):
        """Q-Learningè¡¨æ ¼æ›´æ–°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        current_q = self.get_q_value(state, action)
        
        if done:
            if reward == 100:  # æˆåŠŸåˆ°è¾¾ç›®æ ‡
                target_q = reward
            else:  # å…¶ä»–ç»ˆæ­¢æƒ…å†µ
                target_q = reward
        else:
            next_q_values = [self.get_q_value(next_state, a) for a in range(self.env.n_actions)]
            target_q = reward + self.gamma * max(next_q_values)
        
        # åŠ¨æ€å­¦ä¹ ç‡ï¼šæˆåŠŸç»éªŒä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
        dynamic_alpha = self.alpha_q * 1.5 if reward == 100 else self.alpha_q
        new_q = current_q + dynamic_alpha * (target_q - current_q)
        self.set_q_value(state, action, new_q)
    
    def train_episode(self, episode_num):
        """è®­ç»ƒå•ä¸ªepisode"""
        self.current_episode = episode_num
        self.update_phase()
        
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        max_steps = 300
        
        while steps < max_steps:
            action, log_prob = self.select_action(state, training=True)
            next_state, reward, done = self.env.step(action)
            total_reward += reward
            steps += 1
            
            # Q-Learningæ›´æ–°ï¼ˆå‰ä¸¤é˜¶æ®µï¼‰
            if self.training_phase in ["q_learning", "hybrid"]:
                self.q_learning_update(state, action, reward, next_state, done)
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.append({
                'state': state,
                'action': action, 
                'reward': reward,
                'next_state': next_state,
                'done': done,
                'log_prob': log_prob
            })
            
            if done:
                break
            state = next_state
        
        # ç¥ç»ç½‘ç»œæ›´æ–°ï¼ˆåä¸¤é˜¶æ®µï¼‰
        if self.training_phase in ["hybrid", "actor_critic"] and len(self.buffer) >= 32:
            self.update_networks()
        
        # æ¢ç´¢ç‡è¡°å‡ï¼ˆæ›´å¿«è¡°å‡ï¼‰
        if episode_num % 8 == 0:  # æ›´é¢‘ç¹çš„è¡°å‡
            if self.training_phase == "q_learning":
                decay_rate = 0.995
            elif self.training_phase == "hybrid":
                decay_rate = 0.992  # æ··åˆé˜¶æ®µæ›´å¿«è¡°å‡
            else:
                decay_rate = 0.990  # ACé˜¶æ®µæœ€å¿«è¡°å‡
            self.epsilon = max(0.01, self.epsilon * decay_rate)  # æ›´ä½çš„æœ€å°æ¢ç´¢ç‡
        
        success = (done and reward == 100)
        
        # é˜¶æ®µè½¬æ¢æç¤º
        if episode_num == self.phase_episodes["q_learning"]:
            print(f"ğŸ”„ è¿›å…¥æ··åˆè®­ç»ƒé˜¶æ®µ (Episode {episode_num})")
            print(f"   Qè¡¨å¤§å°: {len(self.Q_table)}")
        elif episode_num == sum(list(self.phase_episodes.values())[:2]):
            print(f"ğŸ­ è¿›å…¥Actor-Criticç²¾è°ƒé˜¶æ®µ (Episode {episode_num})")
        
        return total_reward, steps, success
    
    def update_networks(self):
        """æ›´æ–°ç¥ç»ç½‘ç»œ"""
        if len(self.buffer) < 32:
            return
        
        # å‡†å¤‡æ•°æ®
        batch = list(self.buffer)[-32:]
        states = torch.stack([self.state_to_tensor(exp['state']) for exp in batch])
        actions = torch.tensor([exp['action'] for exp in batch])
        rewards = torch.tensor([exp['reward'] for exp in batch], dtype=torch.float32)
        next_states = torch.stack([self.state_to_tensor(exp['next_state']) for exp in batch])
        dones = torch.tensor([exp['done'] for exp in batch], dtype=torch.float32)
        
        # å‰å‘ä¼ æ’­
        action_probs, values, q_values = self.network(states)
        _, next_values, _ = self.network(next_states)
        
        values = values.squeeze()
        next_values = next_values.squeeze()
        
        # 1. æ›´æ–°Qç½‘ç»œï¼ˆå­¦ä¹ Qè¡¨çŸ¥è¯†ï¼‰
        if self.training_phase == "hybrid" and len(self.Q_table) > 50:
            table_targets = []
            for exp in batch:
                table_q = self.get_q_value(exp['state'], exp['action'])
                table_targets.append(table_q)
            
            table_targets = torch.tensor(table_targets, dtype=torch.float32)
            current_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()
            q_loss = F.mse_loss(current_q, table_targets.detach())
            
            self.q_optimizer.zero_grad()
            q_loss.backward(retain_graph=True)
            self.q_optimizer.step()
        
        # 2. æ›´æ–°Critic
        td_targets = rewards + self.gamma * next_values * (1 - dones)
        critic_loss = F.mse_loss(values, td_targets.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward(retain_graph=True)
        self.critic_optimizer.step()
        
        # 3. æ›´æ–°Actor
        advantages = td_targets - values
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        action_dist = torch.distributions.Categorical(action_probs)
        log_probs = action_dist.log_prob(actions)
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
    
    def test_episode(self, render=False):
        """æµ‹è¯•episode"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]
        
        while steps < 300:
            action, _ = self.select_action(state, training=False)
            next_state, reward, done = self.env.step(action)
            total_reward += reward
            steps += 1
            path.append(next_state[:2])
            
            if done:
                break
            state = next_state
        
        success = (done and reward == 100)
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success


class OptimizedQGuidedActorCritic(QGuidedActorCritic):
    """ä¸“é—¨ä¼˜åŒ–æ­¥æ•°çš„Q-Guided Actor-Criticç‰ˆæœ¬"""
    
    def __init__(self, env: RacetrackEnv, lr=0.003, gamma=0.99, 
                 alpha_q=0.4, epsilon=0.3):
        super().__init__(env, lr, gamma, alpha_q, epsilon)
        
        # æ›´æ¿€è¿›çš„è®­ç»ƒé…ç½®ï¼Œé‡ç‚¹åœ¨Q-Learningé˜¶æ®µ
        self.phase_episodes = {"q_learning": 600, "hybrid": 200, "actor_critic": 100}
        
        print("ğŸš€ ä¸“é—¨ä¼˜åŒ–æ­¥æ•°çš„Q-Guided Actor-Critic")
        print(f"è®­ç»ƒè®¡åˆ’ï¼šQ-Learning({self.phase_episodes['q_learning']}) â†’ "
              f"æ··åˆ({self.phase_episodes['hybrid']}) â†’ "
              f"Actor-Critic({self.phase_episodes['actor_critic']})")
    
    def get_step_bonus(self, steps_taken, max_steps=300):
        """åŸºäºæ­¥æ•°çš„å¥–åŠ±ä¿®æ­£"""
        # æ­¥æ•°è¶Šå°‘ï¼Œé¢å¤–å¥–åŠ±è¶Šå¤š
        step_efficiency = (max_steps - steps_taken) / max_steps
        return step_efficiency * 20  # æœ€å¤š20åˆ†çš„æ•ˆç‡å¥–åŠ±
    
    def train_episode(self, episode_num):
        """æ”¹è¿›çš„è®­ç»ƒepisodeï¼Œé‡ç‚¹ä¼˜åŒ–æ­¥æ•°"""
        self.current_episode = episode_num
        self.update_phase()
        
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        max_steps = 300
        
        while steps < max_steps:
            action, log_prob = self.select_action(state, training=True)
            next_state, reward, done = self.env.step(action)
            
            # ä¿®æ­£å¥–åŠ±ï¼šå¯¹çŸ­è·¯å¾„ç»™äºˆé¢å¤–å¥–åŠ±
            if done and reward == 100:
                step_bonus = self.get_step_bonus(steps + 1)
                reward += step_bonus
            
            total_reward += reward
            steps += 1
            
            # Q-Learningæ›´æ–°ï¼ˆå‰ä¸¤é˜¶æ®µï¼‰
            if self.training_phase in ["q_learning", "hybrid"]:
                self.q_learning_update(state, action, reward, next_state, done)
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.append({
                'state': state,
                'action': action, 
                'reward': reward,
                'next_state': next_state,
                'done': done,
                'log_prob': log_prob
            })
            
            if done:
                break
            state = next_state
        
        # ç¥ç»ç½‘ç»œæ›´æ–°
        if self.training_phase in ["hybrid", "actor_critic"] and len(self.buffer) >= 64:
            self.update_networks()
        
        # æ›´å¿«çš„æ¢ç´¢ç‡è¡°å‡
        if episode_num % 5 == 0:
            if self.training_phase == "q_learning":
                decay_rate = 0.993
            elif self.training_phase == "hybrid":
                decay_rate = 0.990
            else:
                decay_rate = 0.985
            self.epsilon = max(0.005, self.epsilon * decay_rate)
        
        success = (done and reward >= 100)  # è€ƒè™‘å¥–åŠ±ä¿®æ­£åçš„æˆåŠŸåˆ¤æ–­
        
        return total_reward, steps, success
    
    def select_action(self, state, training=True):
        """ä¼˜åŒ–çš„åŠ¨ä½œé€‰æ‹©ï¼Œè®­ç»ƒæ—¶æ›´æ³¨é‡æ¢ç´¢æ•ˆç‡"""
        state_tensor = self.state_to_tensor(state)
        
        if training:
            if self.training_phase == "q_learning":
                # æ”¹è¿›çš„æ¢ç´¢ç­–ç•¥ï¼šåŸºäºQå€¼å·®å¼‚çš„æ¢ç´¢
                q_vals = [self.get_q_value(state, a) for a in range(self.env.n_actions)]
                
                if len(q_vals) > 0 and max(q_vals) > min(q_vals):
                    # å¦‚æœæœ‰æ˜æ˜¾çš„Qå€¼å·®å¼‚ï¼Œé™ä½æ¢ç´¢ç‡
                    effective_epsilon = self.epsilon * 0.5
                else:
                    # å¦‚æœQå€¼ç›¸è¿‘ï¼Œä¿æŒæ¢ç´¢
                    effective_epsilon = self.epsilon
                
                if random.random() < effective_epsilon:
                    action = random.randint(0, self.env.n_actions - 1)
                else:
                    action = int(np.argmax(q_vals))
                
                with torch.no_grad():
                    action_probs, _, _ = self.network(state_tensor)
                    action_dist = torch.distributions.Categorical(action_probs)
                    log_prob = action_dist.log_prob(torch.tensor(action))
                    
            else:
                # å…¶ä»–é˜¶æ®µä½¿ç”¨åŸæ¥çš„ç­–ç•¥
                return super().select_action(state, training)
        else:
            # æµ‹è¯•æ¨¡å¼ï¼šå®Œå…¨è´ªå¿ƒï¼Œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            with torch.no_grad():
                action_probs, _, nn_q_values = self.network(state_tensor)
                
                if len(self.Q_table) > 100:
                    table_q_values = torch.tensor([self.get_q_value(state, a) for a in range(self.env.n_actions)])
                    # å¦‚æœQè¡¨æœ‰ç»éªŒï¼Œå®Œå…¨ä½¿ç”¨Qè¡¨
                    action = int(torch.argmax(table_q_values))
                else:
                    action = torch.argmax(action_probs).item()
                
                action_dist = torch.distributions.Categorical(action_probs)
                log_prob = action_dist.log_prob(torch.tensor(action))
        
        return action, log_prob


class StartPositionOptimizedQGAC(OptimizedQGuidedActorCritic):
    """é’ˆå¯¹èµ·ç‚¹ä½ç½®ä¼˜åŒ–çš„Q-Guided AC"""
    
    def __init__(self, env: RacetrackEnv, best_starts=None, lr=0.003, gamma=0.99, 
                 alpha_q=0.4, epsilon=0.3):
        super().__init__(env, lr, gamma, alpha_q, epsilon)
        
        # è®¾ç½®æœ€ä¼˜èµ·ç‚¹ï¼ˆå¦‚æœæä¾›ï¼‰
        self.best_starts = best_starts or [(31, 11), (31, 16), (31, 15)]  # æ ¹æ®åˆ†æç»“æœ
        self.biased_training = True  # åå‘è®­ç»ƒæœ€ä¼˜èµ·ç‚¹
        
        print("ğŸ¯ é’ˆå¯¹æœ€ä¼˜èµ·ç‚¹ä¼˜åŒ–çš„Q-Guided AC")
        print(f"ä¼˜å…ˆè®­ç»ƒèµ·ç‚¹: {self.best_starts}")
    
    def reset_with_bias(self):
        """å¸¦åå‘çš„é‡ç½®ï¼Œæ›´å¤šä½¿ç”¨æœ€ä¼˜èµ·ç‚¹"""
        if self.biased_training and random.random() < 0.7:  # 70%æ¦‚ç‡ä½¿ç”¨æœ€ä¼˜èµ·ç‚¹
            start_pos = random.choice(self.best_starts)
            self.env.state = (start_pos[0], start_pos[1], 0, 0)
            return self.env.state
        else:
            return self.env.reset()  # æ­£å¸¸éšæœºé‡ç½®
    
    def train_episode(self, episode_num):
        """æ”¹è¿›çš„è®­ç»ƒï¼Œåå‘æœ€ä¼˜èµ·ç‚¹"""
        self.current_episode = episode_num
        self.update_phase()
        
        # ä½¿ç”¨åå‘é‡ç½®
        state = self.reset_with_bias()
        total_reward = 0.0
        steps = 0
        max_steps = 300
        
        while steps < max_steps:
            action, log_prob = self.select_action(state, training=True)
            next_state, reward, done = self.env.step(action)
            
            # åŠ å¼ºå¥–åŠ±ä¿®æ­£
            if done and reward == 100:
                step_bonus = self.get_step_bonus(steps + 1)
                # å¯¹æœ€ä¼˜èµ·ç‚¹ç»™äºˆé¢å¤–å¥–åŠ±
                if state[:2] in self.best_starts:
                    step_bonus *= 1.5  # æœ€ä¼˜èµ·ç‚¹é¢å¤–50%å¥–åŠ±
                reward += step_bonus
            
            total_reward += reward
            steps += 1
            
            # Q-Learningæ›´æ–°
            if self.training_phase in ["q_learning", "hybrid"]:
                self.q_learning_update(state, action, reward, next_state, done)
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.append({
                'state': state,
                'action': action, 
                'reward': reward,
                'next_state': next_state,
                'done': done,
                'log_prob': log_prob
            })
            
            if done:
                break
            state = next_state
        
        # ç¥ç»ç½‘ç»œæ›´æ–°
        if self.training_phase in ["hybrid", "actor_critic"] and len(self.buffer) >= 64:
            self.update_networks()
        
        # åœ¨è®­ç»ƒåæœŸå‡å°‘èµ·ç‚¹åå‘
        if episode_num > 400:
            self.biased_training = False
        
        # æ¢ç´¢ç‡è¡°å‡
        if episode_num % 5 == 0:
            if self.training_phase == "q_learning":
                decay_rate = 0.993
            elif self.training_phase == "hybrid":
                decay_rate = 0.990
            else:
                decay_rate = 0.985
            self.epsilon = max(0.005, self.epsilon * decay_rate)
        
        success = (done and reward >= 100)
        return total_reward, steps, success


class UltraOptimizedQGAC(StartPositionOptimizedQGAC):
    """è¶…çº§ä¼˜åŒ–ç‰ˆQ-Guided AC - æ¿€è¿›ä¼˜åŒ–æ­¥æ•°"""
    
    def __init__(self, env: RacetrackEnv, best_starts=None, lr=0.004, gamma=0.995, 
                 alpha_q=0.5, epsilon=0.4):
        super().__init__(env, best_starts, lr, gamma, alpha_q, epsilon)
        
        # é‡æ–°åˆå§‹åŒ–ç½‘ç»œä»¥æ”¯æŒ15ç»´ç‰¹å¾
        self.network = QGuidedNetwork(15, env.n_actions)  # å‡çº§åˆ°15ç»´çŠ¶æ€ç‰¹å¾
        
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.AdamW(self.network.actor_head.parameters(), lr=lr*1.0)
        self.critic_optimizer = optim.AdamW(self.network.critic_head.parameters(), lr=lr*0.8)
        self.q_optimizer = optim.AdamW(self.network.q_head.parameters(), lr=lr*1.5)
        
        # æ¿€è¿›çš„è®­ç»ƒé…ç½® - å¤§å¹…å»¶é•¿Q-Learningé˜¶æ®µ
        self.phase_episodes = {"q_learning": 1200, "hybrid": 150, "actor_critic": 50}
        
        # å¤šå±‚çº§å¥–åŠ±ç³»ç»Ÿ
        self.step_bonus_multiplier = 3.0  # æ›´å¼ºçš„æ­¥æ•°å¥–åŠ±
        self.efficiency_memory = deque(maxlen=100)  # è®°å½•æ•ˆç‡å†å²
        
        print("ğŸš€ è¶…çº§ä¼˜åŒ–ç‰ˆQ-Guided Actor-Critic - æ¿€è¿›æ­¥æ•°ä¼˜åŒ–")
        print(f"è®­ç»ƒè®¡åˆ’ï¼šQ-Learning({self.phase_episodes['q_learning']}) â†’ "
              f"æ··åˆ({self.phase_episodes['hybrid']}) â†’ "
              f"Actor-Critic({self.phase_episodes['actor_critic']})")
    
    def enhanced_step_bonus(self, steps_taken, max_steps=300):
        """å¢å¼ºçš„æ­¥æ•°å¥–åŠ±ç³»ç»Ÿ"""
        # åŸºç¡€æ•ˆç‡å¥–åŠ±
        efficiency_ratio = (max_steps - steps_taken) / max_steps
        base_bonus = efficiency_ratio * 50 * self.step_bonus_multiplier
        
        # å†å²æ•ˆç‡å¯¹æ¯”å¥–åŠ±
        if self.efficiency_memory:
            avg_historical_steps = np.mean(self.efficiency_memory)
            if steps_taken < avg_historical_steps:
                improvement_bonus = (avg_historical_steps - steps_taken) * 2.0
                base_bonus += improvement_bonus
        
        # æçŸ­è·¯å¾„çš„çˆ†ç‚¸å¥–åŠ±
        if steps_taken <= 15:
            base_bonus += 100 * (16 - steps_taken)  # 15æ­¥å†…æœ‰çˆ†ç‚¸å¥–åŠ±
        elif steps_taken <= 20:
            base_bonus += 50 * (21 - steps_taken)   # 20æ­¥å†…æœ‰å·¨é¢å¥–åŠ±
        elif steps_taken <= 25:
            base_bonus += 20 * (26 - steps_taken)   # 25æ­¥å†…æœ‰å¤§é¢å¥–åŠ±
        
        return base_bonus
    
    def advanced_state_features(self, state):
        """é«˜çº§çŠ¶æ€ç‰¹å¾å·¥ç¨‹ - å¢å¼ºåˆ°15ç»´"""
        x, y, vx, vy = state
        
        # åŸºç¡€å½’ä¸€åŒ–ç‰¹å¾
        norm_x = x / 31.0
        norm_y = y / 16.0
        norm_vx = vx / self.env.max_speed
        norm_vy = vy / self.env.max_speed
        
        # å¤šç›®æ ‡è·ç¦»å’Œæ–¹å‘åˆ†æ
        distances_to_goals = []
        directions_to_goals = []
        
        for gx, gy in self.env.goal_positions:
            dist = np.sqrt((x-gx)**2 + (y-gy)**2)
            distances_to_goals.append(dist)
            if dist > 0:
                directions_to_goals.append(((gx-x)/dist, (gy-y)/dist))
            else:
                directions_to_goals.append((0, 0))
        
        # æœ€è¿‘ç›®æ ‡ç›¸å…³ç‰¹å¾
        min_dist_idx = np.argmin(distances_to_goals)
        min_dist = distances_to_goals[min_dist_idx]
        best_dir_x, best_dir_y = directions_to_goals[min_dist_idx]
        
        # é€Ÿåº¦å¯¹é½åº¦ï¼ˆæœå‘æœ€ä½³ç›®æ ‡ï¼‰
        vel_magnitude = np.sqrt(vx**2 + vy**2)
        vel_align = 0.0
        if vel_magnitude > 0 and min_dist > 0:
            vel_align = (vx*best_dir_x + vy*best_dir_y) / vel_magnitude
        
        # è·¯å¾„è§„åˆ’ç‰¹å¾
        norm_min_dist = min_dist / np.sqrt(31**2 + 16**2)
        manhattan_to_best = (abs(x - self.env.goal_positions[min_dist_idx][0]) + 
                           abs(y - self.env.goal_positions[min_dist_idx][1])) / (31 + 16)
        
        # åŠ¨æ€ç‰¹å¾
        momentum_x = vx * norm_x  # ä½ç½®-é€Ÿåº¦äº¤äº’
        momentum_y = vy * norm_y
        
        # æˆ˜æœ¯ç‰¹å¾
        is_near_boundary = min(x, 31-x, y, 16-y) / 16.0  # è¾¹ç•Œè·ç¦»
        speed_efficiency = vel_magnitude / (self.env.max_speed * np.sqrt(2))
        
        # æ–°å¢ï¼šå¤šæ­¥é¢„æµ‹ç‰¹å¾
        future_x = x - vx  # é¢„æµ‹ä¸‹ä¸€æ­¥ä½ç½®
        future_y = y + vy
        future_dist_to_goal = min([np.sqrt((future_x-gx)**2 + (future_y-gy)**2) 
                                 for gx, gy in self.env.goal_positions])
        future_dist_norm = future_dist_to_goal / np.sqrt(31**2 + 16**2)
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,           # åŸºç¡€çŠ¶æ€ (4)
            norm_min_dist, best_dir_x, best_dir_y,      # ç›®æ ‡ç›¸å…³ (3)
            vel_align, manhattan_to_best,               # å¯¹é½å’Œè·¯å¾„ (2)
            momentum_x, momentum_y,                     # åŠ¨æ€ç‰¹å¾ (2)
            is_near_boundary, speed_efficiency,         # æˆ˜æœ¯ç‰¹å¾ (2)
            future_dist_norm, vel_magnitude             # é¢„æµ‹ç‰¹å¾ (2)
        ], dtype=torch.float32)  # æ€»å…±15ç»´
    
    def state_to_tensor(self, state):
        """ä½¿ç”¨é«˜çº§ç‰¹å¾"""
        return self.advanced_state_features(state)
    
    def ultra_smart_action_selection(self, state, training=True):
        """è¶…æ™ºèƒ½åŠ¨ä½œé€‰æ‹©ç­–ç•¥"""
        state_tensor = self.state_to_tensor(state)
        
        if training:
            if self.training_phase == "q_learning":
                # Q-Learningé˜¶æ®µï¼šå¢å¼ºçš„æ™ºèƒ½æ¢ç´¢
                q_vals = [self.get_q_value(state, a) for a in range(self.env.n_actions)]
                
                # åŸºäºQå€¼æ–¹å·®çš„åŠ¨æ€æ¢ç´¢
                q_std = np.std(q_vals) if len(q_vals) > 1 else 0
                if q_std > 1.0:  # Qå€¼å·®å¼‚æ˜æ˜¾æ—¶ï¼Œæ›´å¤šåˆ©ç”¨
                    effective_epsilon = self.epsilon * 0.3
                else:  # Qå€¼ç›¸è¿‘æ—¶ï¼Œä¿æŒæ¢ç´¢
                    effective_epsilon = self.epsilon
                
                # è·ç¦»å¯¼å‘çš„åŠ¨ä½œåå¥½
                if random.random() < effective_epsilon:
                    # æ™ºèƒ½æ¢ç´¢ï¼šåå‘æœå‘ç›®æ ‡çš„åŠ¨ä½œ
                    x, y, vx, vy = state
                    goal_distances = []
                    for gx, gy in self.env.goal_positions:
                        goal_distances.append(np.sqrt((x-gx)**2 + (y-gy)**2))
                    
                    best_goal_idx = np.argmin(goal_distances)
                    gx, gy = self.env.goal_positions[best_goal_idx]
                    
                    # è®¡ç®—æœå‘ç›®æ ‡çš„ç†æƒ³é€Ÿåº¦å˜åŒ–
                    ideal_dvx = -1 if x > gx else (1 if x < gx else 0)
                    ideal_dvy = 1 if y < gy else (-1 if y > gy else 0)
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘ç†æƒ³æ–¹å‘çš„åŠ¨ä½œ
                    best_action = 4  # é»˜è®¤ä¸å˜
                    best_score = -1
                    
                    for action_idx, (ax, ay) in enumerate(self.env.actions):
                        # è®¡ç®—åŠ¨ä½œä¸ç†æƒ³æ–¹å‘çš„åŒ¹é…åº¦
                        score = ax * ideal_dvx + ay * ideal_dvy
                        if score > best_score:
                            best_score = score
                            best_action = action_idx
                    
                    action = best_action if random.random() < 0.7 else random.randint(0, self.env.n_actions - 1)
                else:
                    action = int(np.argmax(q_vals))
                
                # è®¡ç®—log_prob
                with torch.no_grad():
                    action_probs, _, _ = self.network(state_tensor)
                    action_dist = torch.distributions.Categorical(action_probs)
                    log_prob = action_dist.log_prob(torch.tensor(action))
            
            else:
                # å…¶ä»–é˜¶æ®µä½¿ç”¨åŸç­–ç•¥
                return super().select_action(state, training)
        
        else:
            # æµ‹è¯•æ¨¡å¼ï¼šç»å¯¹è´ªå¿ƒ + æ™ºèƒ½å›é€€
            with torch.no_grad():
                action_probs, _, nn_q_values = self.network(state_tensor)
                
                if len(self.Q_table) > 500:  # Qè¡¨è¶³å¤Ÿå¤§æ—¶
                    table_q_values = torch.tensor([self.get_q_value(state, a) for a in range(self.env.n_actions)])
                    
                    # å¤šç­–ç•¥èåˆ
                    if max(table_q_values) > 5:  # Qè¡¨æœ‰é«˜è´¨é‡ç»éªŒ
                        action = int(torch.argmax(table_q_values))
                    else:
                        # èåˆQè¡¨å’Œç¥ç»ç½‘ç»œï¼Œåå‘Qè¡¨
                        combined_q = 0.9 * table_q_values + 0.1 * nn_q_values
                        action = int(torch.argmax(combined_q))
                else:
                    # Qè¡¨è¾ƒå°æ—¶ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œ
                    action = torch.argmax(action_probs).item()
                
                action_dist = torch.distributions.Categorical(action_probs)
                log_prob = action_dist.log_prob(torch.tensor(action))
        
        return action, log_prob
    
    def select_action(self, state, training=True):
        """é‡å†™åŠ¨ä½œé€‰æ‹©"""
        return self.ultra_smart_action_selection(state, training)
    
    def train_episode(self, episode_num):
        """è¶…çº§ä¼˜åŒ–çš„è®­ç»ƒepisode"""
        self.current_episode = episode_num
        self.update_phase()
        
        # ä½¿ç”¨åå‘é‡ç½®ï¼ˆå‰æœŸæ›´å¤šæœ€ä¼˜èµ·ç‚¹ï¼‰
        bias_probability = 0.9 if episode_num < 800 else 0.7
        if self.biased_training and random.random() < bias_probability:
            start_pos = random.choice(self.best_starts)
            self.env.state = (start_pos[0], start_pos[1], 0, 0)
            state = self.env.state
        else:
            state = self.env.reset()
        
        total_reward = 0.0
        steps = 0
        max_steps = 300
        
        while steps < max_steps:
            action, log_prob = self.select_action(state, training=True)
            next_state, reward, done = self.env.step(action)
            
            # è¶…çº§å¼ºåŒ–çš„å¥–åŠ±ä¿®æ­£
            if done and reward == 100:
                step_bonus = self.enhanced_step_bonus(steps + 1)
                
                # æœ€ä¼˜èµ·ç‚¹é¢å¤–å¥–åŠ±
                if state[:2] in self.best_starts:
                    step_bonus *= 2.0  # æœ€ä¼˜èµ·ç‚¹åŒå€å¥–åŠ±
                
                reward += step_bonus
                
                # è®°å½•æ•ˆç‡
                self.efficiency_memory.append(steps + 1)
            elif reward == -10:  # ç¢°æ’
                reward -= 30  # æ›´ä¸¥å‰çš„ç¢°æ’æƒ©ç½š
            
            total_reward += reward
            steps += 1
            
            # å¼ºåŒ–Q-Learningæ›´æ–°
            if self.training_phase in ["q_learning", "hybrid"]:
                # ä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡
                if reward > 100:  # æˆåŠŸä¸”æœ‰å¥–åŠ±
                    dynamic_alpha = self.alpha_q * 2.0
                elif reward == 100:  # æ™®é€šæˆåŠŸ
                    dynamic_alpha = self.alpha_q * 1.5  
                else:
                    dynamic_alpha = self.alpha_q
                
                # ä¿å­˜åŸå§‹alphaå¹¶ä¸´æ—¶ä¿®æ”¹
                original_alpha = self.alpha_q
                self.alpha_q = dynamic_alpha
                self.q_learning_update(state, action, reward, next_state, done)
                self.alpha_q = original_alpha
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.append({
                'state': state,
                'action': action, 
                'reward': reward,
                'next_state': next_state,
                'done': done,
                'log_prob': log_prob
            })
            
            if done:
                break
            state = next_state
        
        # ç¥ç»ç½‘ç»œæ›´æ–°ï¼ˆæ›´é¢‘ç¹ï¼‰
        if self.training_phase in ["hybrid", "actor_critic"] and len(self.buffer) >= 32:
            self.update_networks()
        
        # æ›´ç§¯æçš„æ¢ç´¢ç‡è¡°å‡
        if episode_num % 3 == 0:  # æ›´é¢‘ç¹è¡°å‡
            if self.training_phase == "q_learning":
                decay_rate = 0.996 if episode_num < 600 else 0.992
            elif self.training_phase == "hybrid":
                decay_rate = 0.985
            else:
                decay_rate = 0.980
            self.epsilon = max(0.002, self.epsilon * decay_rate)  # æ›´ä½æœ€å°å€¼
        
        # åŠ¨æ€è°ƒæ•´èµ·ç‚¹åå‘
        if episode_num > 900:
            self.biased_training = False
        
        success = (done and reward >= 100)
        return total_reward, steps, success


def ultra_optimization_demo():
    """æ¼”ç¤ºè¶…çº§ä¼˜åŒ–ç‰ˆQ-Guided Actor-Critic"""
    print("ğŸš€ è¶…çº§ä¼˜åŒ–ç‰ˆQ-Guided Actor-Criticæ¼”ç¤º")
    print("=" * 60)
    
    # 1. å¿«é€Ÿåˆ†ææœ€ä¼˜èµ·ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # ä½¿ç”¨ä¹‹å‰åˆ†æçš„æœ€ä¼˜èµ·ç‚¹
    best_starts = [(31, 10), (31, 13), (31, 16), (31, 3), (31, 6)]
    
    print(f"ğŸ¯ ä½¿ç”¨é¢„åˆ†æçš„æœ€ä¼˜èµ·ç‚¹: {best_starts}")
    
    # 2. åˆ›å»ºè¶…çº§ä¼˜åŒ–æ™ºèƒ½ä½“
    ultra_agent = UltraOptimizedQGAC(env, best_starts=best_starts)
    
    # 3. è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è¶…çº§ä¼˜åŒ–è®­ç»ƒ...")
    total_episodes = sum(ultra_agent.phase_episodes.values())
    
    training_results = []
    
    for episode in range(total_episodes):
        total_reward, steps, success = ultra_agent.train_episode(episode)
        
        if success:
            training_results.append(steps)
        
        # è¿›åº¦æŠ¥å‘Š
        if (episode + 1) % 200 == 0:
            recent_successes = [s for s in training_results[-50:]]
            if recent_successes:
                avg_recent = np.mean(recent_successes)
                min_recent = min(recent_successes)
                print(f"Episode {episode+1} ({ultra_agent.training_phase}): "
                      f"Qè¡¨={len(ultra_agent.Q_table)}, "
                      f"Îµ={ultra_agent.epsilon:.4f}, "
                      f"è¿‘50æ¬¡æˆåŠŸå¹³å‡={avg_recent:.1f}æ­¥, "
                      f"æœ€ä½³={min_recent}æ­¥")
            else:
                print(f"Episode {episode+1} ({ultra_agent.training_phase}): "
                      f"Qè¡¨={len(ultra_agent.Q_table)}, "
                      f"Îµ={ultra_agent.epsilon:.4f}")
    
    # 4. æœ€ç»ˆæµ‹è¯•
    print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½æµ‹è¯•:")
    
    # æœ€ä¼˜èµ·ç‚¹æµ‹è¯•
    best_start_results = []
    for best_start in best_starts[:3]:
        test_results = []
        for _ in range(20):
            ultra_agent.env.state = (best_start[0], best_start[1], 0, 0)
            reward, steps, _, success = ultra_agent.test_episode()
            if success:
                test_results.append(steps)
        
        if test_results:
            avg_steps = np.mean(test_results)
            min_steps = min(test_results)
            best_start_results.extend(test_results)
            print(f"èµ·ç‚¹{best_start}: å¹³å‡{avg_steps:.1f}æ­¥, æœ€ä½³{min_steps}æ­¥ ({len(test_results)}/20æˆåŠŸ)")
    
    # éšæœºèµ·ç‚¹æµ‹è¯•
    random_results = []
    for _ in range(50):
        reward, steps, _, success = ultra_agent.test_episode()
        if success:
            random_results.append(steps)
    
    # æ€»ç»“
    print(f"\nğŸ‰ è¶…çº§ä¼˜åŒ–ç»“æœ:")
    if best_start_results:
        best_avg = np.mean(best_start_results)
        best_min = min(best_start_results) 
        print(f"âœ… æœ€ä¼˜èµ·ç‚¹å¹³å‡: {best_avg:.1f}æ­¥ Â± {np.std(best_start_results):.1f}")
        print(f"âœ… æœ€ä¼˜èµ·ç‚¹æœ€ä½³: {best_min}æ­¥")
    
    if random_results:
        random_avg = np.mean(random_results)
        random_min = min(random_results)
        print(f"ğŸ“Š éšæœºèµ·ç‚¹å¹³å‡: {random_avg:.1f}æ­¥ Â± {np.std(random_results):.1f}")
        print(f"ğŸ“Š éšæœºèµ·ç‚¹æœ€ä½³: {random_min}æ­¥")
    
    # å¯¹æ¯”ä¹‹å‰çš„ç»“æœ
    if best_start_results and random_results:
        improvement_avg = 26.0 - random_avg  # å¯¹æ¯”ä¹‹å‰çš„26.0æ­¥
        improvement_best = 23.0 - best_avg   # å¯¹æ¯”ä¹‹å‰çš„23.0æ­¥
        print(f"\nğŸ“ˆ æ”¹è¿›å¹…åº¦:")
        print(f"éšæœºèµ·ç‚¹æ”¹è¿›: {improvement_avg:.1f}æ­¥")
        print(f"æœ€ä¼˜èµ·ç‚¹æ”¹è¿›: {improvement_best:.1f}æ­¥")
        
        if best_min <= 12:
            print(f"ğŸ† å®ç°äº†æ¥è¿‘ç†è®ºæœ€ä¼˜çš„{best_min}æ­¥ï¼")
    
    return ultra_agent, best_start_results, random_results


# ä¿®æ”¹demoå‡½æ•°
def demo():
    """æ¼”ç¤ºå…¨é¢ä¼˜åŒ–çš„Q-Guided Actor-Criticç®—æ³•"""
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. åŸç‰ˆå…¨é¢åˆ†æ + æ ‡å‡†ä¼˜åŒ–")
    print("2. è¶…çº§ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ¿€è¿›æ­¥æ•°ä¼˜åŒ–ï¼‰")
    
    # ç”±äºæ˜¯è‡ªåŠ¨è¿è¡Œï¼Œç›´æ¥ä½¿ç”¨è¶…çº§ä¼˜åŒ–ç‰ˆæœ¬
    print("ğŸš€ è‡ªåŠ¨é€‰æ‹©è¶…çº§ä¼˜åŒ–ç‰ˆæœ¬")
    return ultra_optimization_demo()


if __name__ == "__main__":
    demo() 
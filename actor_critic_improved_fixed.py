"""
Actor-Critic å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜çš„ä¼˜åŒ–ç‰ˆæœ¬

æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ªé’ˆå¯¹èµ›è½¦è½¨é“ç¯å¢ƒçš„Actor-Criticæ™ºèƒ½ä½“ï¼Œ
ä¸»è¦è§£å†³äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½é€€åŒ–é—®é¢˜ã€‚

æ ¸å¿ƒæ”¹è¿›ï¼š
1. åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥
2. æœ€ä½³æ¨¡å‹ä¿æŠ¤æœºåˆ¶  
3. ææ…¢æ¢ç´¢ç‡è¡°å‡
4. åˆ†ç¦»çš„Actor-Criticä¼˜åŒ–å™¨
5. æ€§èƒ½ç›‘æ§ä¸è‡ªåŠ¨æ¢å¤

ä½œè€…ï¼šAI Assistant
æœ€åæ›´æ–°ï¼š2024å¹´
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Tuple, List, Dict
import random
from collections import deque
from racetrack_env import RacetrackEnv

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
# ç¡®ä¿PyTorchçš„ç¡®å®šæ€§è¡Œä¸º
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print(f"ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º: {RANDOM_SEED}")


class SharedNetwork(nn.Module):
    """
    Actor-Criticå…±äº«ç½‘ç»œæ¶æ„
    
    é‡‡ç”¨å…±äº«åº•å±‚ç‰¹å¾æå– + åˆ†ç¦»å¤´éƒ¨çš„è®¾è®¡ï¼š
    - å…±äº«å±‚ï¼šæå–ç¯å¢ƒçŠ¶æ€çš„é€šç”¨ç‰¹å¾
    - Actorå¤´éƒ¨ï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    - Criticå¤´éƒ¨ï¼šä¼°è®¡çŠ¶æ€ä»·å€¼
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(SharedNetwork, self).__init__()
        
        # å…±äº«çš„åº•å±‚ç‰¹å¾æå–ç½‘ç»œ
        # ä½¿ç”¨ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œé€æ¸å‹ç¼©ç‰¹å¾ç»´åº¦
        self.shared_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),  # å‹ç¼©åˆ°ä¸€åŠç»´åº¦
            nn.ReLU()
        )
        
        # Actorå¤´éƒ¨ï¼šè¾“å‡ºåŠ¨ä½œlogits
        self.actor_head = nn.Linear(hidden_dim // 2, action_dim)
        
        # Criticå¤´éƒ¨ï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼ä¼°è®¡
        self.critic_head = nn.Linear(hidden_dim // 2, 1)
        
    def forward(self, state):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            state: çŠ¶æ€å¼ é‡ [batch_size, state_dim] æˆ– [state_dim]
            
        Returns:
            action_probs: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ [batch_size, action_dim] æˆ– [action_dim]
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡ [batch_size, 1] æˆ– [1]
        """
        # æå–å…±äº«ç‰¹å¾
        shared_features = self.shared_layers(state)
        
        # è®¡ç®—åŠ¨ä½œlogitså¹¶è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        action_logits = self.actor_head(shared_features)
        action_probs = F.softmax(action_logits, dim=-1)
        
        # è®¡ç®—çŠ¶æ€ä»·å€¼
        value = self.critic_head(shared_features)
        
        return action_probs, value


class Experience:
    """
    ç»éªŒå›æ”¾ä¸­çš„å•ä¸ªç»éªŒæ ·æœ¬
    
    å­˜å‚¨ä¸€ä¸ªå®Œæ•´çš„çŠ¶æ€è½¬ç§»å››å…ƒç»„ï¼š(s, a, r, s', done)
    å¤–åŠ ç”¨äºç­–ç•¥æ¢¯åº¦çš„log_prob
    """
    def __init__(self, state, action, reward, next_state, done, log_prob):
        self.state = state          # å½“å‰çŠ¶æ€
        self.action = action        # æ‰§è¡Œçš„åŠ¨ä½œ
        self.reward = reward        # è·å¾—çš„å¥–åŠ±
        self.next_state = next_state # ä¸‹ä¸€ä¸ªçŠ¶æ€
        self.done = done            # æ˜¯å¦ç»ˆæ­¢
        self.log_prob = log_prob    # åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡


class OptimizedActorCriticAgent:
    """
    ä¼˜åŒ–çš„Actor-Criticæ™ºèƒ½ä½“
    
    ä¸»è¦ç‰¹æ€§ï¼š
    1. è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜ï¼šé€šè¿‡ææ…¢æ¢ç´¢è¡°å‡å’Œæœ€ä½³æ¨¡å‹ä¿æŠ¤
    2. ç¨³å®šçš„ä»·å€¼å‡½æ•°å­¦ä¹ ï¼šåˆ†ç¦»ä¼˜åŒ–å™¨ï¼Œé™ä½Criticå­¦ä¹ ç‡
    3. æ”¹è¿›çš„çŠ¶æ€è¡¨ç¤ºï¼šåŒ…å«ç›®æ ‡æ–¹å‘å’Œé€Ÿåº¦å¯¹é½ä¿¡æ¯
    4. ä¸¥æ ¼çš„åŠ¨ä½œæ©ç ï¼šå®Œå…¨ç¦æ­¢ç¢°æ’åŠ¨ä½œ
    5. ç®€åŒ–çš„å¥–åŠ±å¡‘å½¢ï¼šé¿å…è¿‡åº¦å·¥ç¨‹åŒ–
    """
    
    def __init__(self, env: RacetrackEnv, lr=0.001, gamma=0.99, 
                 hidden_dim=128, buffer_size=128, gae_lambda=0.95):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            env: èµ›è½¦è½¨é“ç¯å¢ƒ
            lr: å­¦ä¹ ç‡ï¼ˆä¸»è¦ç”¨äºå…¼å®¹æ€§ï¼Œå®é™…ä½¿ç”¨åˆ†ç¦»çš„ä¼˜åŒ–å™¨ï¼‰
            gamma: æŠ˜æ‰£å› å­
            hidden_dim: éšè—å±‚ç»´åº¦
            buffer_size: ç»éªŒç¼“å†²åŒºå¤§å°
            gae_lambda: GAEçš„Î»å‚æ•°
        """
        self.env = env
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.buffer_size = buffer_size
        
        # çŠ¶æ€ç‰¹å¾ç»´åº¦ï¼š8ç»´ç²¾å¿ƒè®¾è®¡çš„ç‰¹å¾
        # [norm_x, norm_y, norm_vx, norm_vy, norm_distance, 
        #  goal_direction_x, goal_direction_y, velocity_alignment]
        self.state_dim = 8
        self.action_dim = env.n_actions
        
        # åˆ›å»ºå…±äº«ç½‘ç»œ
        self.network = SharedNetwork(self.state_dim, self.action_dim, hidden_dim)
        
        # ç»éªŒç¼“å†²åŒºï¼šä½¿ç”¨åŒç«¯é˜Ÿåˆ—å®ç°å›ºå®šå¤§å°ç¼“å†²
        self.buffer = deque(maxlen=buffer_size)
        
        # æ¢ç´¢ç­–ç•¥å‚æ•°
        self.epsilon = 0.5              # åˆå§‹æ¢ç´¢ç‡ï¼ˆé€‚ä¸­ï¼‰
        self.epsilon_min = 0.15         # æœ€å°æ¢ç´¢ç‡ï¼ˆä¿æŒè¶³å¤Ÿæ¢ç´¢ï¼‰
        self.epsilon_decay = 0.9998     # ææ…¢è¡°å‡ï¼ˆé˜²æ­¢è¿‡æ—©æ”¶æ•›ï¼‰
        
        # è®­ç»ƒç¨³å®šæ€§å‚æ•°
        self.entropy_coef = 0.05        # ç†µæ­£åˆ™åŒ–ç³»æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.update_frequency = 32      # æ‰¹é‡æ›´æ–°é¢‘ç‡
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        self.success_rate: List[float] = []
        self.losses: List[float] = []
        self.value_losses: List[float] = []
        self.policy_losses: List[float] = []
        
        # å…³é”®æ”¹è¿›ï¼šåˆ†ç¦»çš„Actor-Criticä¼˜åŒ–å™¨
        # Actorä½¿ç”¨è¾ƒé«˜å­¦ä¹ ç‡ï¼ˆç­–ç•¥å­¦ä¹ ï¼‰
        self.actor_optimizer = optim.AdamW(
            self.network.actor_head.parameters(), 
            lr=0.0005, 
            weight_decay=1e-5
        )
        # Criticä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡ï¼ˆé˜²æ­¢ä»·å€¼å‡½æ•°è¿‡æ‹Ÿåˆï¼‰
        self.critic_optimizer = optim.AdamW(
            self.network.critic_head.parameters(), 
            lr=0.0003,
            weight_decay=1e-5
        )
    
    def state_to_tensor(self, state: Tuple[int, int, int, int]) -> torch.Tensor:
        """
        å°†ç¯å¢ƒçŠ¶æ€è½¬æ¢ä¸ºç¥ç»ç½‘ç»œè¾“å…¥å¼ é‡
        
        è®¾è®¡8ç»´ç‰¹å¾å‘é‡ï¼ŒåŒ…å«ï¼š
        1. ä½ç½®ä¿¡æ¯ï¼ˆå½’ä¸€åŒ–ï¼‰
        2. é€Ÿåº¦ä¿¡æ¯ï¼ˆå½’ä¸€åŒ–ï¼‰ 
        3. ç›®æ ‡è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰
        4. ç›®æ ‡æ–¹å‘ï¼ˆå•ä½å‘é‡ï¼‰
        5. é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        
        Args:
            state: ç¯å¢ƒçŠ¶æ€ (x, y, vx, vy)
            
        Returns:
            torch.Tensor: 8ç»´ç‰¹å¾å‘é‡
        """
        x, y, vx, vy = state
        
        # 1. åŸºç¡€ç‰¹å¾å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
        norm_x = x / 31.0               # xåæ ‡å½’ä¸€åŒ–
        norm_y = y / 16.0               # yåæ ‡å½’ä¸€åŒ–
        norm_vx = vx / self.env.max_speed  # xæ–¹å‘é€Ÿåº¦å½’ä¸€åŒ–
        norm_vy = vy / self.env.max_speed  # yæ–¹å‘é€Ÿåº¦å½’ä¸€åŒ–
        
        # 2. è®¡ç®—åˆ°æœ€è¿‘ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_distance = float('inf')
        goal_direction_x, goal_direction_y = 0, 0
        
        # éå†æ‰€æœ‰ç»ˆç‚¹ï¼Œæ‰¾åˆ°æœ€è¿‘çš„
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            if distance < min_distance:
                min_distance = distance
                if distance > 0:
                    # è®¡ç®—æŒ‡å‘ç›®æ ‡çš„å•ä½æ–¹å‘å‘é‡
                    # æ³¨æ„ï¼šä½¿ç”¨ç¯å¢ƒçš„åæ ‡ç³»ç»Ÿ
                    goal_direction_x = -(goal_x - x) / distance  # å‘ä¸Šä¸ºæ­£
                    goal_direction_y = (goal_y - y) / distance   # å‘å³ä¸ºæ­£
        
        # 3. è·ç¦»å½’ä¸€åŒ–ï¼ˆä½¿ç”¨å¯¹è§’çº¿è·ç¦»ä½œä¸ºæœ€å¤§å€¼ï¼‰
        max_distance = np.sqrt(31**2 + 16**2)
        norm_distance = min_distance / max_distance
        
        # 4. è®¡ç®—é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        velocity_alignment = 0.0
        if min_distance > 0:
            velocity_mag = np.sqrt(vx**2 + vy**2)
            if velocity_mag > 0:
                # å½“å‰é€Ÿåº¦çš„å•ä½æ–¹å‘å‘é‡
                vel_dir_x = vx / velocity_mag
                vel_dir_y = vy / velocity_mag
                # è®¡ç®—å¯¹é½åº¦ï¼ˆç‚¹ç§¯ï¼ŒèŒƒå›´[-1,1]ï¼Œå–éè´Ÿéƒ¨åˆ†ï¼‰
                velocity_alignment = max(0, vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y)
        
        # è¿”å›8ç»´ç‰¹å¾å‘é‡
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_distance, goal_direction_x, goal_direction_y, 
            velocity_alignment
        ], dtype=torch.float32)
    
    def select_action(self, state: Tuple[int, int, int, int], training=True) -> Tuple[int, torch.Tensor]:
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆæ”¯æŒè®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼ï¼‰
        
        è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥ï¼Œåœ¨æœ‰æ•ˆåŠ¨ä½œä¸­æ¢ç´¢
        æµ‹è¯•æ¨¡å¼ï¼šçº¯è´ªå¿ƒç­–ç•¥ï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œç´¢å¼•
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆç”¨äºç­–ç•¥æ¢¯åº¦ï¼‰
        """
        # è®¾ç½®ç½‘ç»œä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutç­‰ï¼‰
        self.network.eval()
        
        # è½¬æ¢çŠ¶æ€ä¸ºå¼ é‡
        state_tensor = self.state_to_tensor(state)
        
        # å‰å‘ä¼ æ’­è·å–åŠ¨ä½œæ¦‚ç‡
        if training:
            action_probs, _ = self.network(state_tensor)
        else:
            with torch.no_grad():  # æµ‹è¯•æ—¶ä¸éœ€è¦æ¢¯åº¦
                action_probs, _ = self.network(state_tensor)
        
        # åº”ç”¨åŠ¨ä½œæ©ç ï¼ˆç¦æ­¢ç¢°æ’åŠ¨ä½œï¼‰
        action_probs = self._apply_strict_action_mask(state, action_probs)
        
        # åŠ¨ä½œé€‰æ‹©ç­–ç•¥
        if training and random.random() < self.epsilon:
            # è®­ç»ƒæ¨¡å¼ + éšæœºæ¢ç´¢ï¼šåœ¨æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
            valid_actions = (action_probs > 0).nonzero().squeeze(-1)
            if len(valid_actions) > 0:
                action = valid_actions[random.randint(0, len(valid_actions)-1)]
            else:
                action = torch.argmax(action_probs)
        else:
            # è´ªå¿ƒç­–ç•¥ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ
            action = torch.argmax(action_probs)
        
        # è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆç”¨äºç­–ç•¥æ¢¯åº¦ï¼‰
        action_dist = torch.distributions.Categorical(action_probs)
        log_prob = action_dist.log_prob(action)
        
        return action.item(), log_prob
    
    def _apply_strict_action_mask(self, state: Tuple[int, int, int, int], 
                                action_probs: torch.Tensor) -> torch.Tensor:
        """
        åº”ç”¨ä¸¥æ ¼çš„åŠ¨ä½œæ©ç ï¼Œå®Œå…¨ç¦æ­¢ä¼šå¯¼è‡´ç¢°æ’çš„åŠ¨ä½œ
        
        è¿™æ˜¯å®‰å…¨æœºåˆ¶ï¼Œç¡®ä¿æ™ºèƒ½ä½“ä¸ä¼šé€‰æ‹©æ˜æ˜¾é”™è¯¯çš„åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            action_probs: åŸå§‹åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            
        Returns:
            torch.Tensor: æ©ç åçš„åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        """
        x, y, vx, vy = state
        mask = torch.ones_like(action_probs)
        
        # éå†æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œ
        for i, (ax, ay) in enumerate(self.env.actions):
            # é¢„æµ‹æ‰§è¡ŒåŠ¨ä½œåçš„æ–°é€Ÿåº¦
            new_vx = max(0, min(self.env.max_speed, vx + ax))
            new_vy = max(0, min(self.env.max_speed, vy + ay))
            
            # å¤„ç†é€Ÿåº¦ä¸º0çš„ç‰¹æ®Šæƒ…å†µï¼ˆç¯å¢ƒè§„åˆ™ï¼‰
            if new_vx == 0 and new_vy == 0 and (x, y) not in self.env.start_positions:
                new_vx = 1
                new_vy = 1
            
            # é¢„æµ‹ä¸‹ä¸€æ­¥ä½ç½®
            new_x = x - new_vx  # å‘ä¸Šç§»åŠ¨ï¼ˆxå‡å°ï¼‰
            new_y = y + new_vy  # å‘å³ç§»åŠ¨ï¼ˆyå¢å¤§ï¼‰
            
            # æ£€æŸ¥æ˜¯å¦ä¼šå‘ç”Ÿç¢°æ’
            if self.env._check_collision(x, y, new_x, new_y):
                mask[i] = 0.0  # ç¦æ­¢æ­¤åŠ¨ä½œ
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªåŠ¨ä½œå¯é€‰ï¼ˆå®‰å…¨æªæ–½ï¼‰
        if mask.sum() == 0:
            mask.fill_(1.0)
        
        # é‡æ–°å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ
        masked_probs = action_probs * mask
        return masked_probs / (masked_probs.sum() + 1e-8)
    
    def _improved_reward_shaping(self, state, next_state, reward, done, steps):
        """
        ç®€åŒ–çš„å¥–åŠ±å¡‘å½¢
        
        è®¾è®¡åŸåˆ™ï¼š
        1. ä¿æŒç®€å•ï¼Œé¿å…è¿‡åº¦å·¥ç¨‹åŒ–
        2. åªå¯¹å…³é”®äº‹ä»¶ç»™äºˆå¥–åŠ±/æƒ©ç½š
        3. è½»å¾®çš„è¿›æ­¥å¥–åŠ±ï¼Œé¿å…å¼•å¯¼åå·®
        
        Args:
            state: å½“å‰çŠ¶æ€
            next_state: ä¸‹ä¸€çŠ¶æ€
            reward: åŸå§‹å¥–åŠ±
            done: æ˜¯å¦ç»ˆæ­¢
            steps: å½“å‰æ­¥æ•°
            
        Returns:
            float: å¡‘å½¢åçš„å¥–åŠ±
        """
        bonus = 0.0
        
        # 1. æˆåŠŸ/å¤±è´¥çš„æ˜ç¡®å¥–åŠ±
        if done and reward > 0:
            bonus += 100    # æˆåŠŸåˆ°è¾¾ç»ˆç‚¹
        elif reward == -10:  # ç¢°æ’
            bonus -= 50     # ç¢°æ’æƒ©ç½š
        
        # 2. ç®€å•çš„è¿›æ­¥å¥–åŠ±ï¼ˆè·ç¦»å‡å°‘ï¼‰
        x, y, _, _ = state
        next_x, next_y, _, _ = next_state
        
        # è®¡ç®—åˆ°æœ€è¿‘ç›®æ ‡çš„æ›¼å“ˆé¡¿è·ç¦»
        curr_dist = min([abs(x - gx) + abs(y - gy) for gx, gy in self.env.goal_positions])
        next_dist = min([abs(next_x - gx) + abs(next_y - gy) for gx, gy in self.env.goal_positions])
        
        # åªæœ‰æ˜¾è‘—è¿›æ­¥æ‰ç»™å¥–åŠ±ï¼ˆé¿å…å™ªå£°ï¼‰
        if curr_dist - next_dist > 1:
            bonus += 2.0
        
        # 3. è½»å¾®çš„æ­¥æ•°æƒ©ç½šï¼ˆé¼“åŠ±æ•ˆç‡ï¼‰
        bonus -= 0.1
        
        return reward + bonus
    
    def _batch_update(self):
        """
        æ‰¹é‡æ›´æ–°ç½‘ç»œå‚æ•°
        
        ä½¿ç”¨Actor-Criticç®—æ³•ï¼š
        1. è®¡ç®—GAEä¼˜åŠ¿ä¼°è®¡
        2. åˆ†åˆ«æ›´æ–°Criticï¼ˆä»·å€¼å‡½æ•°ï¼‰å’ŒActorï¼ˆç­–ç•¥ï¼‰
        3. ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        """
        if len(self.buffer) < self.update_frequency:
            return
        
        # è®¾ç½®ç½‘ç»œä¸ºè®­ç»ƒæ¨¡å¼
        self.network.train()
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        states = []
        actions = []
        rewards = []
        next_states = []
        dones = []
        log_probs = []
        
        for exp in self.buffer:
            states.append(self.state_to_tensor(exp.state))
            actions.append(exp.action)
            rewards.append(exp.reward)
            next_states.append(self.state_to_tensor(exp.next_state))
            dones.append(exp.done)
            log_probs.append(exp.log_prob)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.stack(states)
        actions = torch.tensor(actions)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_states = torch.stack(next_states)
        dones = torch.tensor(dones, dtype=torch.float32)
        log_probs = torch.stack(log_probs)
        
        # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥ä¸‹çš„ä»·å€¼å’Œæ¦‚ç‡
        action_probs_batch, values = self.network(states)
        _, next_values = self.network(next_states)
        
        values = values.squeeze()
        next_values = next_values.squeeze()
        
        # è®¡ç®—GAEä¼˜åŠ¿
        advantages = self._compute_gae_fixed(rewards, values, next_values, dones)
        
        # ä¼˜åŠ¿æ ‡å‡†åŒ–ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
        if len(advantages) > 1 and advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            # è£å‰ªä¼˜åŠ¿å€¼ï¼Œé˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°
            advantages = torch.clamp(advantages, -5.0, 5.0)
        
        # è®¡ç®—ä»·å€¼ç›®æ ‡ï¼ˆTDç›®æ ‡ï¼‰
        clipped_rewards = torch.clamp(rewards, -20, 20)  # å¥–åŠ±è£å‰ª
        
        td_targets = torch.zeros_like(clipped_rewards)
        for t in range(len(clipped_rewards)):
            if t == len(clipped_rewards) - 1:
                next_value = 0 if dones[t] else next_values[t].detach()
            else:
                next_value = values[t + 1].detach() * (1 - dones[t])
            td_targets[t] = clipped_rewards[t] + self.gamma * next_value
        
        value_targets = td_targets
        
        # 1. æ›´æ–°Criticï¼ˆä»·å€¼å‡½æ•°ï¼‰
        critic_loss = F.mse_loss(values, value_targets)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(self.network.critic_head.parameters(), 1.0)
        self.critic_optimizer.step()
        
        # 2. æ›´æ–°Actorï¼ˆç­–ç•¥ï¼‰
        # é‡æ–°è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆå½“å‰ç­–ç•¥ï¼‰
        action_dist = torch.distributions.Categorical(action_probs_batch)
        new_log_probs = action_dist.log_prob(actions)
        
        # ç­–ç•¥æŸå¤±ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        actor_loss = -(new_log_probs * advantages.detach()).mean()
        
        # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = action_dist.entropy().mean()
        actor_total_loss = actor_loss - self.entropy_coef * entropy
        
        self.actor_optimizer.zero_grad()
        actor_total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.actor_head.parameters(), 1.0)
        self.actor_optimizer.step()
        
        # è®°å½•æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
        total_loss = actor_total_loss + 0.3 * critic_loss
        self.losses.append(total_loss.item())
        self.value_losses.append(critic_loss.item())
        self.policy_losses.append(actor_loss.item())
        
        # éƒ¨åˆ†æ¸…ç©ºç¼“å†²åŒºï¼Œä¿ç•™ä¸€äº›ç»éªŒ
        for _ in range(self.update_frequency // 2):
            if len(self.buffer) > 0:
                self.buffer.popleft()
    
    def _compute_gae_fixed(self, rewards, values, next_values, dones):
        """
        è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
        
        GAEç»“åˆäº†TDè¯¯å·®å’Œè’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œå¹³è¡¡äº†åå·®å’Œæ–¹å·®
        
        Args:
            rewards: å¥–åŠ±åºåˆ—
            values: ä»·å€¼ä¼°è®¡åºåˆ—
            next_values: ä¸‹ä¸€çŠ¶æ€ä»·å€¼ä¼°è®¡åºåˆ—
            dones: ç»ˆæ­¢æ ‡å¿—åºåˆ—
            
        Returns:
            torch.Tensor: GAEä¼˜åŠ¿ä¼°è®¡
        """
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        # ä»åå‘å‰è®¡ç®—GAE
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0 if dones[t] else next_values[t]
            else:
                next_value = values[t + 1] * (1 - dones[t])
            
            # TDè¯¯å·®
            delta = rewards[t] + self.gamma * next_value - values[t]
            # GAEé€’æ¨å…¬å¼
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages[t] = gae
        
        return advantages
    
    def train_episode(self, episode_num: int) -> Tuple[float, int, bool]:
        """
        è®­ç»ƒå•ä¸ªepisode
        
        Args:
            episode_num: episodeç¼–å·
            
        Returns:
            total_reward: æ€»å¥–åŠ±
            steps: æ­¥æ•°
            success: æ˜¯å¦æˆåŠŸ
        """
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        max_steps = 200  # æœ€å¤§æ­¥æ•°é™åˆ¶
        
        episode_buffer = []
        last_reward = 0
        
        while steps < max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob = self.select_action(state, training=True)
            prev_state = state
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = self.env.step(action)
            last_reward = reward
            
            # å¥–åŠ±å¡‘å½¢
            shaped_reward = self._improved_reward_shaping(prev_state, next_state, reward, done, steps)
            
            # å­˜å‚¨ç»éªŒ
            exp = Experience(prev_state, action, shaped_reward, next_state, done, log_prob)
            episode_buffer.append(exp)
            
            total_reward += reward  # ä½¿ç”¨åŸå§‹å¥–åŠ±è®¡ç®—å›æŠ¥
            steps += 1
            
            if done:
                break
                
            state = next_state
        
        # å°†episodeç»éªŒæ·»åŠ åˆ°ç¼“å†²åŒº
        self.buffer.extend(episode_buffer)
        
        # æ‰¹é‡æ›´æ–°
        if len(self.buffer) >= self.update_frequency:
            self._batch_update()
        
        # æ›´æ–°æ¢ç´¢ç‡ï¼ˆæ¯10ä¸ªepisodeæ›´æ–°ä¸€æ¬¡ï¼‰
        if episode_num % 10 == 0:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        # åˆ¤æ–­æˆåŠŸï¼ˆåˆ°è¾¾ç»ˆç‚¹ä¸”åœ¨æ­¥æ•°é™åˆ¶å†…ï¼‰
        success = (steps < max_steps and done and last_reward == 100)
        return total_reward, steps, success
    
    def test_episode(self, render: bool = False) -> Tuple[float, int, List, bool]:
        """
        æµ‹è¯•å•ä¸ªepisodeï¼ˆä¸è®­ç»ƒï¼‰
        
        Args:
            render: æ˜¯å¦æ¸²æŸ“è½¨è¿¹
            
        Returns:
            total_reward: æ€»å¥–åŠ±
            steps: æ­¥æ•°
            path: è½¨è¿¹è·¯å¾„
            success: æ˜¯å¦æˆåŠŸ
        """
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]  # è®°å½•ä½ç½®è½¨è¿¹
        max_steps = 300
        
        last_reward = 0
        with torch.no_grad():  # æµ‹è¯•æ—¶ä¸éœ€è¦æ¢¯åº¦
            while steps < max_steps:
                action, _ = self.select_action(state, training=False)
                next_state, reward, done = self.env.step(action)
                total_reward += reward
                steps += 1
                path.append(next_state[:2])
                last_reward = reward
                
                if done:
                    break
                
                state = next_state
        
        success = (steps < max_steps and done and last_reward == 100)
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒç»Ÿè®¡"""
        save_dict = {
            'network': self.network.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_rate': self.success_rate,
            'losses': self.losses,
            'value_losses': self.value_losses,
            'policy_losses': self.policy_losses,
            'epsilon': self.epsilon
        }
        torch.save(save_dict, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu')
        self.network.load_state_dict(checkpoint['network'])
        self.network.eval()


def main_fixed_degradation():
    """
    è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜çš„ä¸»è®­ç»ƒå‡½æ•°
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. åˆ†é˜¶æ®µè®­ç»ƒï¼šé«˜æ¢ç´¢ â†’ å¹³è¡¡ â†’ ç²¾è°ƒ
    2. æœ€ä½³æ¨¡å‹ä¿æŠ¤ï¼šè‡ªåŠ¨ä¿å­˜å¹¶æ¢å¤å†å²æœ€ä½³æ€§èƒ½
    3. æ€§èƒ½ç›‘æ§ï¼šå®æ—¶æ£€æµ‹é€€åŒ–å¹¶é‡‡å–æªæ–½
    4. åˆ†ç¦»ä¼˜åŒ–å™¨ï¼šActorå’ŒCriticä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
    5. éšæœºç§å­æ§åˆ¶ï¼Œç¡®ä¿å¯é‡ç°æ€§
    """
    print("=== è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜çš„è®­ç»ƒï¼ˆéšæœºç§å­ç‰ˆæœ¬ï¼‰===")
    print(f"ğŸ² ä½¿ç”¨å›ºå®šéšæœºç§å­: {RANDOM_SEED}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = OptimizedActorCriticAgent(
        env=env,
        lr=0.0003,         # åŸºç¡€å­¦ä¹ ç‡
        gamma=0.99,        # æŠ˜æ‰£å› å­
        hidden_dim=128,    # ç½‘ç»œéšè—å±‚ç»´åº¦
        buffer_size=512,   # ç»éªŒç¼“å†²åŒºå¤§å°
        gae_lambda=0.95    # GAEå‚æ•°
    )
    
    # é‡æ–°è®¾ç½®æ¢ç´¢ç­–ç•¥ï¼ˆå…³é”®æ”¹è¿›ï¼‰
    agent.epsilon = 0.5                # é€‚ä¸­çš„åˆå§‹æ¢ç´¢ç‡
    agent.epsilon_decay = 0.9998       # ææ…¢çš„è¡°å‡
    agent.epsilon_min = 0.15           # ä¿æŒè¾ƒé«˜çš„æœ€å°æ¢ç´¢ç‡
    agent.entropy_coef = 0.05          # å¢åŠ ç†µæ­£åˆ™åŒ–
    
    # æ€§èƒ½ä¿æŠ¤æœºåˆ¶
    best_success_rate = 0.0
    best_model_state = None
    patience = 0
    max_patience = 200
    performance_window = deque(maxlen=50)
    
    print(f"å…³é”®æ”¹è¿›:")
    print(f"  - ææ…¢æ¢ç´¢è¡°å‡(0.9998)ï¼Œä¿æŒé•¿æœŸæ¢ç´¢")
    print(f"  - æ›´é«˜æœ€å°æ¢ç´¢ç‡(0.15)ï¼Œé˜²æ­¢è¿‡æ—©æ”¶æ•›") 
    print(f"  - å¢å¤§ç¼“å†²åŒº(512)ï¼Œä¿ç•™æ›´å¤šç»éªŒ")
    print(f"  - å¢å¼ºç†µæ­£åˆ™åŒ–(0.05)ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ")
    print(f"  - æ·»åŠ æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢æ€§èƒ½é€€åŒ–")
    
    # è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
    print("\n=== è®­ç»ƒå‰åŸºå‡† ===")
    reward_before, steps_before, _, success_before = agent.test_episode()
    print(f"åŸºå‡†æ€§èƒ½: å¥–åŠ±={reward_before:.1f}, æ­¥æ•°={steps_before}, æˆåŠŸ={success_before}")
    
    # åˆ†é˜¶æ®µè®­ç»ƒè®¾ç½®
    print(f"\n=== å¼€å§‹æ”¹è¿›è®­ç»ƒ ===")
    n_episodes = 2500
    stage1_episodes = 800   # é˜¶æ®µ1ï¼šé«˜æ¢ç´¢ç‡è®­ç»ƒ
    stage2_episodes = 1200  # é˜¶æ®µ2ï¼šå¹³è¡¡è®­ç»ƒ  
    stage3_episodes = 500   # é˜¶æ®µ3ï¼šç²¾è°ƒè®­ç»ƒ
    
    print(f"åˆ†é˜¶æ®µè®­ç»ƒè®¡åˆ’:")
    print(f"  é˜¶æ®µ1 (0-{stage1_episodes}): é«˜æ¢ç´¢ç‡å­¦ä¹ åŸºç¡€ç­–ç•¥")
    print(f"  é˜¶æ®µ2 ({stage1_episodes}-{stage1_episodes+stage2_episodes}): å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨")
    print(f"  é˜¶æ®µ3 ({stage1_episodes+stage2_episodes}-{n_episodes}): ç²¾è°ƒä¼˜åŒ–")
    
    # è®­ç»ƒå¾ªç¯
    all_rewards = []
    all_steps = []
    all_success_rates = []
    success_window = deque(maxlen=100)
    reward_window = deque(maxlen=50)
    
    for episode in range(n_episodes):
        # åˆ†é˜¶æ®µè°ƒæ•´å‚æ•°
        if episode == stage1_episodes:
            print(f"\nğŸ”„ è¿›å…¥é˜¶æ®µ2: é™ä½Actorå­¦ä¹ ç‡ï¼Œå¢å¼ºç¨³å®šæ€§")
            for param_group in agent.actor_optimizer.param_groups:
                param_group['lr'] *= 0.7
                
        elif episode == stage1_episodes + stage2_episodes:
            print(f"\nğŸ”§ è¿›å…¥é˜¶æ®µ3: è¿›å…¥ç²¾è°ƒæ¨¡å¼")
            for param_group in agent.actor_optimizer.param_groups:
                param_group['lr'] *= 0.5
            for param_group in agent.critic_optimizer.param_groups:
                param_group['lr'] *= 0.7
        
        # è®­ç»ƒä¸€ä¸ªepisode
        reward, steps, success = agent.train_episode(episode)
        all_rewards.append(reward)
        all_steps.append(steps)
        
        success_window.append(1 if success else 0)
        reward_window.append(reward)
        current_success_rate = np.mean(success_window)
        all_success_rates.append(current_success_rate)
        
        # æ€§èƒ½ä¿æŠ¤æœºåˆ¶
        if episode >= 100:
            performance_window.append(current_success_rate)
            
            # å‘ç°æ–°çš„æœ€ä½³æ€§èƒ½æ—¶ä¿å­˜æ¨¡å‹
            if current_success_rate > best_success_rate:
                best_success_rate = current_success_rate
                best_model_state = {
                    'network': agent.network.state_dict().copy(),
                    'actor_optimizer': agent.actor_optimizer.state_dict().copy(),
                    'critic_optimizer': agent.critic_optimizer.state_dict().copy(),
                    'episode': episode,
                    'success_rate': current_success_rate
                }
                patience = 0
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: Episode {episode+1}, æˆåŠŸç‡={current_success_rate:.3f}")
            else:
                patience += 1
            
            # æ£€æµ‹æ€§èƒ½é€€åŒ–å¹¶æ¢å¤
            if len(performance_window) == 50:
                recent_performance = np.mean(list(performance_window)[-25:])
                early_performance = np.mean(list(performance_window)[:25])
                
                if recent_performance < early_performance * 0.7 and patience > max_patience:
                    print(f"\nâš ï¸ æ£€æµ‹åˆ°æ€§èƒ½é€€åŒ–ï¼")
                    print(f"   æ—©æœŸæ€§èƒ½: {early_performance:.3f}")
                    print(f"   æœ€è¿‘æ€§èƒ½: {recent_performance:.3f}")
                    print(f"   æ¢å¤æœ€ä½³æ¨¡å‹...")
                    
                    if best_model_state:
                        agent.network.load_state_dict(best_model_state['network'])
                        agent.actor_optimizer.load_state_dict(best_model_state['actor_optimizer'])
                        agent.critic_optimizer.load_state_dict(best_model_state['critic_optimizer'])
                        print(f"   å·²æ¢å¤Episode {best_model_state['episode']+1}çš„æ¨¡å‹")
                        
                        # é‡ç½®æ¢ç´¢ç‡ï¼Œç»™äºˆç¬¬äºŒæ¬¡æœºä¼š
                        agent.epsilon = max(0.3, agent.epsilon * 1.5)
                        print(f"   é‡ç½®æ¢ç´¢ç‡è‡³{agent.epsilon:.3f}")
                        patience = 0
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 25 == 0:
            avg_reward = np.mean(reward_window)
            avg_steps = np.mean(all_steps[-25:])
            avg_loss = np.mean(agent.losses[-50:]) if agent.losses else 0
            
            stage_name = "é˜¶æ®µ1" if episode < stage1_episodes else "é˜¶æ®µ2" if episode < stage1_episodes + stage2_episodes else "é˜¶æ®µ3"
            
            print(f"{stage_name} Episode {episode + 1:4d}: "
                  f"å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={avg_steps:5.1f}, "
                  f"æˆåŠŸç‡={current_success_rate:.3f}, Îµ={agent.epsilon:.3f}")
            print(f"                     æŸå¤±={avg_loss:.4f}, "
                  f"æœ€ä½³æˆåŠŸç‡={best_success_rate:.3f}, è€å¿ƒ={patience}")
            
            # æ€§èƒ½è¯Šæ–­
            if episode > 100:
                recent_window = list(success_window)[-25:]
                recent_success = np.mean(recent_window)
                if recent_success < 0.05:
                    print(f"ğŸš¨ æœ€è¿‘25è½®æˆåŠŸç‡ä»…{recent_success:.3f}ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´")
                elif recent_success > best_success_rate * 0.8:
                    print(f"âœ… è¡¨ç°è‰¯å¥½ï¼Œæ¥è¿‘æœ€ä½³æ°´å¹³")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\n=== è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆè¯„ä¼° ===")
    final_success = np.mean(all_success_rates[-100:]) if len(all_success_rates) >= 100 else 0
    print(f"æœ€ç»ˆ100å›åˆæˆåŠŸç‡: {final_success:.3f}")
    print(f"å†å²æœ€ä½³æˆåŠŸç‡: {best_success_rate:.3f}")
    
    # å¦‚æœæœ€ç»ˆæ€§èƒ½ä¸å¦‚å†å²æœ€ä½³ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹
    if best_model_state and final_success < best_success_rate * 0.8:
        print(f"\nğŸ”„ æœ€ç»ˆæ€§èƒ½ä¸å¦‚å†å²æœ€ä½³ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
        agent.network.load_state_dict(best_model_state['network'])
        
    # æœ€ç»ˆæµ‹è¯•
    test_results = []
    for i in range(50):
        reward, steps, path, success = agent.test_episode()
        test_results.append((reward, steps, success))
    
    final_test_success = np.mean([r[2] for r in test_results])
    print(f"ä¸¥æ ¼æµ‹è¯•æˆåŠŸç‡ï¼ˆ50æ¬¡ï¼‰: {final_test_success:.3f}")
    
    # è¯„ä¼°ç»“æœ
    if final_test_success > 0.6:
        print("ğŸ‰ æ€§èƒ½é€€åŒ–é—®é¢˜è§£å†³ï¼æˆåŠŸç‡è¶…è¿‡60%")
    elif final_test_success > 0.4:
        print("âœ… æ€§èƒ½æ˜æ˜¾æ”¹å–„ï¼ŒæˆåŠŸç‡è¶…è¿‡40%") 
    elif final_test_success > 0.2:
        print("âš–ï¸ æ€§èƒ½æœ‰æ‰€æ”¹å–„ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("âš ï¸ é—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œéœ€è¦æ›´æ·±å±‚çš„æ¶æ„æ”¹è¿›")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model("models/fixed_degradation_model.pth")
    print(f"æ”¹è¿›åæ¨¡å‹å·²ä¿å­˜åˆ° models/ æ–‡ä»¶å¤¹")
    
    return agent, test_results, all_success_rates


def main_advanced_tuning():
    """
    é’ˆå¯¹å·²è¾¾åˆ°60%æˆåŠŸç‡çš„æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥ç²¾è°ƒ
    
    ç­–ç•¥ï¼š
    1. åŠ è½½æœ€ä½³æ¨¡å‹ä½œä¸ºèµ·ç‚¹
    2. ä½¿ç”¨æ›´ç»†è‡´çš„å­¦ä¹ ç‡è°ƒåº¦
    3. ä¸“æ³¨äºæœ€å10-20%çš„æ€§èƒ½æå‡
    """
    print("=== é«˜çº§ç²¾è°ƒè®­ç»ƒï¼ˆåŸºäº60%æˆåŠŸç‡æ¨¡å‹ï¼‰===")
    
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    agent = OptimizedActorCriticAgent(env=env)
    
    # å°è¯•åŠ è½½ä¹‹å‰çš„æœ€ä½³æ¨¡å‹
    try:
        agent.load_model("models/fixed_degradation_model.pth")
        print("âœ… æˆåŠŸåŠ è½½ä¹‹å‰çš„è®­ç»ƒæ¨¡å‹")
    except:
        print("âš ï¸ æœªæ‰¾åˆ°ä¹‹å‰çš„æ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # ç²¾è°ƒå‚æ•°è®¾ç½®
    agent.epsilon = 0.1  # ä½æ¢ç´¢ç‡ï¼Œä¸»è¦åˆ©ç”¨å·²å­¦åˆ°çš„ç­–ç•¥
    agent.epsilon_decay = 0.9995
    agent.epsilon_min = 0.05
    
    # é™ä½å­¦ä¹ ç‡è¿›è¡Œç²¾è°ƒ
    for param_group in agent.actor_optimizer.param_groups:
        param_group['lr'] = 0.0001
    for param_group in agent.critic_optimizer.param_groups:
        param_group['lr'] = 0.00005
    
    print(f"ç²¾è°ƒå‚æ•°: æ¢ç´¢ç‡={agent.epsilon}, Actor lr=0.0001, Critic lr=0.00005")
    
    # ç²¾è°ƒè®­ç»ƒ
    best_test_success = 0.0
    for episode in range(500):  # çŸ­æœŸç²¾è°ƒ
        reward, steps, success = agent.train_episode(episode)
        
        # æ¯50ä¸ªepisodeæµ‹è¯•ä¸€æ¬¡
        if (episode + 1) % 50 == 0:
            test_results = []
            for _ in range(20):
                _, _, _, test_success = agent.test_episode()
                test_results.append(test_success)
            
            current_success = np.mean(test_results)
            print(f"ç²¾è°ƒ Episode {episode+1}: æµ‹è¯•æˆåŠŸç‡={current_success:.3f}")
            
            if current_success > best_test_success:
                best_test_success = current_success
                agent.save_model("models/advanced_tuned_model.pth")
                print(f"ğŸ’¾ ä¿å­˜æ”¹è¿›æ¨¡å‹ï¼ŒæˆåŠŸç‡æå‡è‡³{current_success:.3f}")
    
    return agent


if __name__ == "__main__":
    # è¿è¡Œä¸»è®­ç»ƒå‡½æ•°
    agent, test_results, success_rates = main_fixed_degradation()
    
    # å¦‚æœæ€§èƒ½è¿˜ä¸é”™ï¼Œå¯ä»¥å°è¯•è¿›ä¸€æ­¥ç²¾è°ƒ
    final_success = np.mean([r[2] for r in test_results])
    if final_success >= 0.5:
        print(f"\nğŸš€ å½“å‰æˆåŠŸç‡{final_success:.1%}ä¸é”™ï¼Œå°è¯•è¿›ä¸€æ­¥ç²¾è°ƒ...")
        main_advanced_tuning()
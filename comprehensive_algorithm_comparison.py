#!/usr/bin/env python3
"""
å¼ºåŒ–å­¦ä¹ ç®—æ³•å…¨é¢å¯¹æ¯”è„šæœ¬

å¯¹æ¯”ä»¥ä¸‹ç®—æ³•ï¼š
1. REINFORCE (Policy Gradient)
2. Actor-Critic  
3. PPO (Proximal Policy Optimization)
4. TRPO (Trust Region Policy Optimization)
5. Q-Learning (Value-based)
6. Sarsa(Î») (Value-based)

å¯¹æ¯”ç»´åº¦ï¼š
- æ”¶æ•›é€Ÿåº¦ (è®­ç»ƒæ›²çº¿)
- å¹³å‡æ­¥æ•° (æ‰§è¡Œæ•ˆç‡)
- ç­–ç•¥ç¨³å®šæ€§ (å¤šæ¬¡æµ‹è¯•æ–¹å·®)
- æˆåŠŸç‡ (ä»»åŠ¡å®Œæˆç‡)
- å­¦ä¹ æ•ˆç‡ (æ ·æœ¬åˆ©ç”¨ç‡)

æ§åˆ¶å˜é‡è®¾è®¡ï¼š
- æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„ç¯å¢ƒå‚æ•°
- æµ‹è¯•æ—¶ä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
- ç›¸åŒçš„æµ‹è¯•æ¬¡æ•°å’Œç¨³å®šæ€§æµ‹è¯•æ‰¹æ¬¡
- ç»Ÿä¸€çš„æ€§èƒ½è¯„ä¼°æ ‡å‡†

ä½œè€…ï¼š YuJinYue
åˆ›å»ºæ—¶é—´ï¼š2025å¹´6æœˆ19æ—¥
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import time
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict, deque
import warnings
warnings.filterwarnings('ignore')

        # å¯¼å…¥æ‰€æœ‰ç®—æ³•
from racetrack_env import RacetrackEnv
from reinforce import OptimizedREINFORCEAgent, test_saved_reinforce_model
from actor_critic import OptimizedActorCriticAgent  
from ppo import OptimizedPPORacetrackAgent
from trpo_racetrack import TRPORacetrackAgent
from q_learning import QLearningAgent
from sarsa_lambda import SarsaLambdaAgent
from q_guided_ac_simple import QGuidedActorCritic, UltraOptimizedQGAC

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class AlgorithmComparator:
    """å¼ºåŒ–å­¦ä¹ ç®—æ³•å…¨é¢å¯¹æ¯”å™¨"""
    
    def __init__(self, track_size=(32, 17), max_speed=5):
        self.env = RacetrackEnv(track_size=track_size, max_speed=max_speed)
        self.results = {}
        self.algorithms = {}
        self.trained_models = {}
        
        # å¯¹æ¯”é…ç½®
        self.test_episodes = 100  # æµ‹è¯•episodeæ•°
        self.training_episodes = 1000  # å¿«é€Ÿè®­ç»ƒepisodeæ•°
        self.stability_tests = 20  # ç¨³å®šæ€§æµ‹è¯•æ¬¡æ•°
        
        # æ§åˆ¶å˜é‡ï¼šç¡®ä¿æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æ¡ä»¶
        self.controlled_test_seeds = list(range(1000, 1000 + self.test_episodes))  # å›ºå®šç§å­
        self.controlled_stability_seeds = list(range(2000, 2000 + self.stability_tests * 10))  # ç¨³å®šæ€§æµ‹è¯•ç§å­
        
        print("ğŸš€ å¼ºåŒ–å­¦ä¹ ç®—æ³•å…¨é¢å¯¹æ¯”å™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ“ ç¯å¢ƒé…ç½®: {track_size}, æœ€å¤§é€Ÿåº¦: {max_speed}")
    
    def load_or_train_all_algorithms(self):
        """åŠ è½½æˆ–è®­ç»ƒæ‰€æœ‰ç®—æ³•"""
        print("\n" + "="*60)
        print("ğŸ”„ åŠ è½½æˆ–è®­ç»ƒæ‰€æœ‰ç®—æ³•")
        print("="*60)
        
        # 1. REINFORCE
        print("\n1ï¸âƒ£ å¤„ç†REINFORCEç®—æ³•...")
        try:
            self.algorithms['REINFORCE'] = OptimizedREINFORCEAgent(self.env)
            self.algorithms['REINFORCE'].load_model("models/optimized_reinforce_model.pth")
            print("âœ… REINFORCEæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ REINFORCEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å¼€å§‹å¿«é€Ÿè®­ç»ƒREINFORCE...")
            self.algorithms['REINFORCE'] = self._quick_train_reinforce()
        
        # 2. Actor-Critic
        print("\n2ï¸âƒ£ å¤„ç†Actor-Criticç®—æ³•...")
        try:
            self.algorithms['Actor-Critic'] = OptimizedActorCriticAgent(self.env)
            self.algorithms['Actor-Critic'].load_model("models/advanced_tuned_model.pth")
            print("âœ… Actor-Criticæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Actor-Criticæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å¼€å§‹å¿«é€Ÿè®­ç»ƒActor-Critic...")
            self.algorithms['Actor-Critic'] = self._quick_train_actor_critic()
        
        # 3. PPO
        print("\n3ï¸âƒ£ å¤„ç†PPOç®—æ³•...")
        try:
            self.algorithms['PPO'] = OptimizedPPORacetrackAgent(self.env)
            self.algorithms['PPO'].load_model("models/optimized_ppo_racetrack_model.pth")
            print("âœ… PPOæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ PPOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å¼€å§‹å¿«é€Ÿè®­ç»ƒPPO...")
            self.algorithms['PPO'] = self._quick_train_ppo()
        
        # 4. TRPO
        print("\n4ï¸âƒ£ å¤„ç†TRPOç®—æ³•...")
        try:
            self.algorithms['TRPO'] = TRPORacetrackAgent(self.env)
            self.algorithms['TRPO'].load_model("models/trpo_racetrack_model.pth")
            print("âœ… TRPOæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ TRPOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å¼€å§‹å¿«é€Ÿè®­ç»ƒTRPO...")
            self.algorithms['TRPO'] = self._quick_train_trpo()
        
        # 5. Q-Learning
        print("\n5ï¸âƒ£ å¤„ç†Q-Learningç®—æ³•...")
        print("ğŸ”„ Q-Learningéœ€è¦é‡æ–°è®­ç»ƒï¼ˆåŸºäºå€¼å‡½æ•°ï¼‰...")
        self.algorithms['Q-Learning'] = self._quick_train_qlearning()
        
        # 6. Sarsa(Î»)
        print("\n6ï¸âƒ£ å¤„ç†Sarsa(Î»)ç®—æ³•...")
        print("ğŸ”„ Sarsa(Î»)éœ€è¦é‡æ–°è®­ç»ƒï¼ˆåŸºäºå€¼å‡½æ•°ï¼‰...")
        self.algorithms['Sarsa(Î»)'] = self._quick_train_sarsa_lambda()
        
        # 7. Q-Guided Actor-Critic
        print("\n7ï¸âƒ£ å¤„ç†Q-Guided Actor-Criticç®—æ³•...")
        print("ğŸ”„ Q-Guided ACéœ€è¦é‡æ–°è®­ç»ƒï¼ˆæ··åˆæ–¹æ³•ï¼‰...")
        self.algorithms['Q-Guided AC'] = self._quick_train_q_guided_ac()
        
        # 8. Ultra-Optimized Q-Guided Actor-Critic
        print("\n8ï¸âƒ£ å¤„ç†è¶…çº§ä¼˜åŒ–Q-Guided Actor-Criticç®—æ³•...")
        print("ğŸ”„ Ultra Q-Guided ACéœ€è¦é‡æ–°è®­ç»ƒï¼ˆè¶…çº§ä¼˜åŒ–ç‰ˆï¼‰...")
        self.algorithms['Ultra Q-Guided AC'] = self._quick_train_ultra_q_guided_ac()
        
        print(f"\nâœ… æ‰€æœ‰ç®—æ³•å‡†å¤‡å®Œæˆï¼å…±{len(self.algorithms)}ä¸ªç®—æ³•")
    
    def _quick_train_reinforce(self) -> OptimizedREINFORCEAgent:
        """å¿«é€Ÿè®­ç»ƒREINFORCE"""
        agent = OptimizedREINFORCEAgent(self.env)
        
        print(f"å¼€å§‹{self.training_episodes}è½®å¿«é€Ÿè®­ç»ƒ...")
        for episode in range(self.training_episodes):
            agent.train_episode(episode)
            
            if (episode + 1) % 200 == 0:
                success_rate = np.mean([1 if r > 0 else 0 for r in agent.episode_rewards[-50:]])
                print(f"Episode {episode+1}: æˆåŠŸç‡ â‰ˆ {success_rate:.2f}")
        
        agent.save_model("models/quick_reinforce_model.pth")
        return agent
    
    def _quick_train_actor_critic(self) -> OptimizedActorCriticAgent:
        """å¿«é€Ÿè®­ç»ƒActor-Critic"""
        agent = OptimizedActorCriticAgent(self.env)
        
        print(f"å¼€å§‹{self.training_episodes}è½®å¿«é€Ÿè®­ç»ƒ...")
        for episode in range(self.training_episodes):
            agent.train_episode(episode)
            
            if (episode + 1) % 200 == 0:
                success_rate = np.mean([1 if r > 0 else 0 for r in agent.episode_rewards[-50:]])
                print(f"Episode {episode+1}: æˆåŠŸç‡ â‰ˆ {success_rate:.2f}")
        
        agent.save_model("models/quick_actor_critic_model.pth")
        return agent
    
    def _quick_train_ppo(self) -> OptimizedPPORacetrackAgent:
        """å¿«é€Ÿè®­ç»ƒPPO"""
        agent = OptimizedPPORacetrackAgent(self.env)
        
        print(f"å¼€å§‹{self.training_episodes}è½®å¿«é€Ÿè®­ç»ƒ...")
        for episode in range(self.training_episodes):
            agent.train_episode(episode)
            
            if (episode + 1) % 200 == 0:
                success_rate = np.mean([1 if r > 0 else 0 for r in agent.episode_rewards[-50:]])
                print(f"Episode {episode+1}: æˆåŠŸç‡ â‰ˆ {success_rate:.2f}")
        
        agent.save_model("models/quick_ppo_model.pth")
        return agent
    
    def _quick_train_trpo(self) -> TRPORacetrackAgent:
        """å¿«é€Ÿè®­ç»ƒTRPO"""
        agent = TRPORacetrackAgent(self.env)
        
        print(f"å¼€å§‹{self.training_episodes}è½®å¿«é€Ÿè®­ç»ƒ...")
        for episode in range(self.training_episodes):
            agent.train_episode(episode)
            
            if (episode + 1) % 200 == 0:
                success_rate = np.mean([1 if s else 0 for _, _, s in 
                                     [agent.test_episode() for _ in range(10)]])
                print(f"Episode {episode+1}: æˆåŠŸç‡ â‰ˆ {success_rate:.2f}")
        
        agent.save_model("models/quick_trpo_model.pth")
        return agent
    
    def _quick_train_qlearning(self) -> QLearningAgent:
        """å¿«é€Ÿè®­ç»ƒQ-Learning"""
        agent = QLearningAgent(self.env, alpha=0.2, gamma=0.95, epsilon=0.15)
        
        print(f"å¼€å§‹{self.training_episodes*2}è½®å¿«é€Ÿè®­ç»ƒï¼ˆQ-Learningéœ€è¦æ›´å¤šæ ·æœ¬ï¼‰...")
        rewards, steps = agent.train(n_episodes=self.training_episodes*2, verbose=False)
        
        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        for i in range(200, len(rewards), 400):
            avg_reward = np.mean(rewards[i-200:i])
            print(f"Episode {i}: å¹³å‡å¥–åŠ± = {avg_reward:.2f}")
        
        return agent
    
    def _quick_train_sarsa_lambda(self) -> SarsaLambdaAgent:
        """å¿«é€Ÿè®­ç»ƒSarsa(Î»)"""
        agent = SarsaLambdaAgent(self.env, alpha=0.15, gamma=0.95, lambda_=0.9, epsilon=0.1)
        
        print(f"å¼€å§‹{self.training_episodes*2}è½®å¿«é€Ÿè®­ç»ƒï¼ˆSarsaéœ€è¦æ›´å¤šæ ·æœ¬ï¼‰...")
        rewards, steps = agent.train(n_episodes=self.training_episodes*2, verbose=False)
        
        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        for i in range(200, len(rewards), 400):
            avg_reward = np.mean(rewards[i-200:i])
            print(f"Episode {i}: å¹³å‡å¥–åŠ± = {avg_reward:.2f}")
        
        return agent
    
    def _quick_train_q_guided_ac(self) -> QGuidedActorCritic:
        """å¿«é€Ÿè®­ç»ƒQ-Guided Actor-Critic"""
        agent = QGuidedActorCritic(self.env, lr=0.001, gamma=0.95, alpha_q=0.2, epsilon=0.3)
        
        total_episodes = sum(agent.phase_episodes.values())
        print(f"å¼€å§‹{total_episodes}è½®å¿«é€Ÿè®­ç»ƒï¼ˆä¸‰é˜¶æ®µï¼‰...")
        
        for episode in range(total_episodes):
            reward, steps, success = agent.train_episode(episode)
            
            # æ¯200ä¸ªepisodeæŠ¥å‘Š
            if (episode + 1) % 200 == 0:
                success_rate = np.mean([1 if agent.test_episode()[3] else 0 for _ in range(10)])
                print(f"Episode {episode+1} ({agent.training_phase}): "
                      f"æˆåŠŸç‡ â‰ˆ {success_rate:.2f}, Qè¡¨å¤§å°={len(agent.Q_table)}")
        
        return agent
    
    def _quick_train_ultra_q_guided_ac(self) -> UltraOptimizedQGAC:
        """å¿«é€Ÿè®­ç»ƒUltra-Optimized Q-Guided Actor-Critic"""
        # ä½¿ç”¨é¢„åˆ†æçš„æœ€ä¼˜èµ·ç‚¹
        best_starts = [(31, 10), (31, 13), (31, 16), (31, 3), (31, 6)]
        agent = UltraOptimizedQGAC(self.env, best_starts=best_starts)
        
        total_episodes = sum(agent.phase_episodes.values())
        print(f"å¼€å§‹{total_episodes}è½®å¿«é€Ÿè®­ç»ƒï¼ˆè¶…çº§ä¼˜åŒ–ä¸‰é˜¶æ®µï¼‰...")
        
        # è®°å½•è®­ç»ƒè¿›åº¦
        training_results = []
        
        for episode in range(total_episodes):
            reward, steps, success = agent.train_episode(episode)
            
            if success:
                training_results.append(steps)
            
            # æ¯200ä¸ªepisodeæŠ¥å‘Š
            if (episode + 1) % 200 == 0:
                recent_successes = [s for s in training_results[-50:]] if len(training_results) >= 50 else training_results
                if recent_successes:
                    avg_recent = np.mean(recent_successes)
                    min_recent = min(recent_successes)
                    print(f"Episode {episode+1} ({agent.training_phase}): "
                          f"æˆåŠŸç‡ â‰ˆ {len(recent_successes)/50:.2f}, "
                          f"å¹³å‡æ­¥æ•°={avg_recent:.1f}, æœ€ä½³={min_recent}æ­¥, "
                          f"Qè¡¨å¤§å°={len(agent.Q_table)}")
                else:
                    print(f"Episode {episode+1} ({agent.training_phase}): "
                          f"Qè¡¨å¤§å°={len(agent.Q_table)}, Îµ={agent.epsilon:.4f}")
        
        print(f"è¶…çº§ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆQè¡¨å¤§å°: {len(agent.Q_table)}")
        return agent
    
    def comprehensive_performance_test(self):
        """å…¨é¢æ€§èƒ½æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ§ª å¼€å§‹å…¨é¢æ€§èƒ½æµ‹è¯•")
        print("="*60)
        
        all_results = {}
        
        for alg_name, algorithm in self.algorithms.items():
            print(f"\nğŸ” æµ‹è¯• {alg_name}...")
            
            # åŸºç¡€æ€§èƒ½æµ‹è¯•
            basic_results = self._test_basic_performance(algorithm, alg_name)
            
            # ç¨³å®šæ€§æµ‹è¯•
            stability_results = self._test_stability(algorithm, alg_name)
            
            # åˆå¹¶ç»“æœ
            all_results[alg_name] = {
                **basic_results,
                **stability_results
            }
            
            print(f"âœ… {alg_name} æµ‹è¯•å®Œæˆ")
        
        self.results = all_results
        return all_results
    
    def _test_basic_performance(self, algorithm, alg_name: str) -> Dict:
        """åŸºç¡€æ€§èƒ½æµ‹è¯•"""
        results = {
            'rewards': [],
            'steps': [],
            'successes': [],
            'paths': [],
            'test_times': []
        }
        
        print(f"  è¿›è¡Œ{self.test_episodes}æ¬¡åŸºç¡€æµ‹è¯•...")
        
        for i in range(self.test_episodes):
            start_time = time.time()
            
            # æ§åˆ¶å˜é‡ï¼šä¸ºæ¯æ¬¡æµ‹è¯•è®¾ç½®å›ºå®šéšæœºç§å­
            test_seed = self.controlled_test_seeds[i]
            np.random.seed(test_seed)
            torch.manual_seed(test_seed)
            
            try:
                # æ ¹æ®ç®—æ³•ç±»å‹è°ƒç”¨ä¸åŒçš„æµ‹è¯•æ–¹æ³•
                if alg_name in ['Q-Learning', 'Sarsa(Î»)']:
                    # Q-Learning å’Œ Sarsa ç±»å‹è¿”å›3ä¸ªå€¼
                    reward, steps, path = algorithm.test_episode()
                    # æ”¹è¿›çš„æˆåŠŸåˆ¤æ–­ï¼šæ£€æŸ¥æœ€ç»ˆä½ç½®æ˜¯å¦åœ¨ç›®æ ‡åŒºåŸŸ
                    success = (len(path) > 0 and path[-1] in self.env.goal_positions)
                else:
                    # ç­–ç•¥æ–¹æ³•å’ŒQ-Guided ACè¿”å›4ä¸ªå€¼
                    test_result = algorithm.test_episode()
                    if len(test_result) == 4:
                        reward, steps, path, success_from_alg = test_result
                        # åŒé‡éªŒè¯ï¼šç®—æ³•åˆ¤æ–­å’Œä½ç½®éªŒè¯
                        position_success = (len(path) > 0 and path[-1] in self.env.goal_positions)
                        success = success_from_alg or position_success
                    else:
                        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›3ä¸ªå€¼
                        reward, steps, path = test_result
                        success = (len(path) > 0 and path[-1] in self.env.goal_positions)
                
                test_time = time.time() - start_time
                
                results['rewards'].append(reward)
                results['steps'].append(steps)
                results['successes'].append(success)
                results['paths'].append(path)
                results['test_times'].append(test_time)
                
            except Exception as e:
                print(f"    âš ï¸ ç¬¬{i+1}æ¬¡æµ‹è¯•å¤±è´¥: {e}")
                # è®°å½•å¤±è´¥çš„æµ‹è¯•
                results['rewards'].append(-300)
                results['steps'].append(300)
                results['successes'].append(False)
                results['paths'].append([])
                test_time = time.time() - start_time
                results['test_times'].append(test_time)
            
            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 20 == 0:
                current_success_rate = np.mean(results['successes'])
                print(f"    è¿›åº¦: {i+1}/{self.test_episodes}, å½“å‰æˆåŠŸç‡: {current_success_rate:.2%}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        success_rate = np.mean(results['successes'])
        avg_reward = np.mean(results['rewards'])
        avg_steps = np.mean(results['steps'])
        avg_test_time = np.mean(results['test_times'])
        
        print(f"  ğŸ“Š {alg_name} åŸºç¡€æ€§èƒ½:")
        print(f"    æˆåŠŸç‡: {success_rate:.2%}")
        print(f"    å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"    å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
        print(f"    å¹³å‡æµ‹è¯•æ—¶é—´: {avg_test_time*1000:.2f}ms")
        
        return results
    
    def _test_stability(self, algorithm, alg_name: str) -> Dict:
        """ç¨³å®šæ€§æµ‹è¯•"""
        print(f"  è¿›è¡Œ{self.stability_tests}æ¬¡ç¨³å®šæ€§æµ‹è¯•...")
        
        stability_results = []
        
        for i in range(self.stability_tests):
            batch_results = []
            
            # æ¯æ¬¡ç¨³å®šæ€§æµ‹è¯•è¿›è¡Œ10æ¬¡episode
            for j in range(10):
                # æ§åˆ¶å˜é‡ï¼šä¸ºæ¯æ¬¡ç¨³å®šæ€§æµ‹è¯•è®¾ç½®å›ºå®šéšæœºç§å­
                stability_seed = self.controlled_stability_seeds[i * 10 + j]
                np.random.seed(stability_seed)
                torch.manual_seed(stability_seed)
                
                try:
                    if alg_name in ['Q-Learning', 'Sarsa(Î»)']:
                        # Q-Learning å’Œ Sarsa ç±»å‹è¿”å›3ä¸ªå€¼
                        reward, steps, path = algorithm.test_episode()
                        success = (len(path) > 0 and path[-1] in self.env.goal_positions)
                    else:
                        # ç­–ç•¥æ–¹æ³•å’ŒQ-Guided ACè¿”å›4ä¸ªå€¼
                        test_result = algorithm.test_episode()
                        if len(test_result) == 4:
                            reward, steps, path, success_from_alg = test_result
                            # åŒé‡éªŒè¯ï¼šç®—æ³•åˆ¤æ–­å’Œä½ç½®éªŒè¯
                            position_success = (len(path) > 0 and path[-1] in self.env.goal_positions)
                            success = success_from_alg or position_success
                        else:
                            # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›3ä¸ªå€¼
                            reward, steps, path = test_result
                            success = (len(path) > 0 and path[-1] in self.env.goal_positions)
                    
                    batch_results.append({
                        'reward': reward,
                        'steps': steps,
                        'success': success
                    })
                except Exception as e:
                    print(f"    âš ï¸ ç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {e}")
                    batch_results.append({
                        'reward': -300,
                        'steps': 300,
                        'success': False
                    })
            
            # è®¡ç®—æ­¤æ‰¹æ¬¡çš„ç»Ÿè®¡é‡
            batch_success_rate = np.mean([r['success'] for r in batch_results])
            batch_avg_reward = np.mean([r['reward'] for r in batch_results])
            batch_avg_steps = np.mean([r['steps'] for r in batch_results])
            
            stability_results.append({
                'success_rate': batch_success_rate,
                'avg_reward': batch_avg_reward,
                'avg_steps': batch_avg_steps
            })
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        success_rates = [r['success_rate'] for r in stability_results]
        avg_rewards = [r['avg_reward'] for r in stability_results]
        avg_steps = [r['avg_steps'] for r in stability_results]
        
        stability_metrics = {
            'success_rate_std': np.std(success_rates),
            'reward_std': np.std(avg_rewards),
            'steps_std': np.std(avg_steps),
            'success_rate_var': np.var(success_rates),
            'reward_var': np.var(avg_rewards),
            'steps_var': np.var(avg_steps),
            'stability_batches': stability_results
        }
        
        print(f"  ğŸ“ˆ {alg_name} ç¨³å®šæ€§:")
        print(f"    æˆåŠŸç‡æ ‡å‡†å·®: {stability_metrics['success_rate_std']:.4f}")
        print(f"    å¥–åŠ±æ ‡å‡†å·®: {stability_metrics['reward_std']:.2f}")
        print(f"    æ­¥æ•°æ ‡å‡†å·®: {stability_metrics['steps_std']:.2f}")
        
        return stability_metrics
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢å¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š ç”Ÿæˆå…¨é¢å¯¹æ¯”æŠ¥å‘Š")
        print("="*60)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        
        for alg_name, results in self.results.items():
            success_rate = np.mean(results['successes'])
            avg_reward = np.mean(results['rewards'])
            avg_steps = np.mean(results['steps'])
            reward_std = np.std(results['rewards'])
            steps_std = np.std(results['steps'])
            
            comparison_data.append({
                'ç®—æ³•': alg_name,
                'æˆåŠŸç‡': f"{success_rate:.2%}",
                'å¹³å‡å¥–åŠ±': f"{avg_reward:.2f}",
                'å¹³å‡æ­¥æ•°': f"{avg_steps:.1f}",
                'å¥–åŠ±æ ‡å‡†å·®': f"{reward_std:.2f}",
                'æ­¥æ•°æ ‡å‡†å·®': f"{steps_std:.2f}",
                'ç¨³å®šæ€§(æˆåŠŸç‡æ–¹å·®)': f"{results['success_rate_var']:.6f}",
                'æ ·æœ¬æ•ˆç‡': f"{success_rate/avg_steps*1000:.2f}" if avg_steps > 0 else "0.00"
            })
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(comparison_data)
        
        print("\nğŸ† ç®—æ³•æ€§èƒ½å¯¹æ¯”è¡¨:")
        print("=" * 100)
        print(df.to_string(index=False))
        
        # æ’ååˆ†æ
        print(f"\nğŸ¥‡ å„é¡¹æŒ‡æ ‡æ’å:")
        print("=" * 50)
        
        # æˆåŠŸç‡æ’å
        success_ranking = sorted(self.results.items(), 
                               key=lambda x: np.mean(x[1]['successes']), reverse=True)
        print("ğŸ“ˆ æˆåŠŸç‡æ’å:")
        for i, (alg, results) in enumerate(success_ranking):
            success_rate = np.mean(results['successes'])
            print(f"  {i+1}. {alg}: {success_rate:.2%}")
        
        # å¹³å‡æ­¥æ•°æ’åï¼ˆè¶Šå°‘è¶Šå¥½ï¼‰
        steps_ranking = sorted([(alg, np.mean(results['steps'])) for alg, results in self.results.items()], 
                             key=lambda x: x[1])
        print(f"\nâš¡ æ‰§è¡Œæ•ˆç‡æ’åï¼ˆå¹³å‡æ­¥æ•°ï¼Œè¶Šå°‘è¶Šå¥½ï¼‰:")
        for i, (alg, avg_steps) in enumerate(steps_ranking):
            print(f"  {i+1}. {alg}: {avg_steps:.1f}æ­¥")
        
        # ç¨³å®šæ€§æ’åï¼ˆæ–¹å·®è¶Šå°è¶Šå¥½ï¼‰
        stability_ranking = sorted([(alg, results['success_rate_var']) for alg, results in self.results.items()], 
                                 key=lambda x: x[1])
        print(f"\nğŸ¯ ç¨³å®šæ€§æ’åï¼ˆæˆåŠŸç‡æ–¹å·®ï¼Œè¶Šå°è¶Šå¥½ï¼‰:")
        for i, (alg, variance) in enumerate(stability_ranking):
            print(f"  {i+1}. {alg}: {variance:.6f}")
        
        # ç»¼åˆè¯„åˆ†
        print(f"\nğŸ… ç»¼åˆè¯„åˆ†ï¼ˆæˆåŠŸç‡Ã—0.4 + æ•ˆç‡å¾—åˆ†Ã—0.3 + ç¨³å®šæ€§å¾—åˆ†Ã—0.3ï¼‰:")
        composite_scores = self._calculate_composite_scores()
        for i, (alg, score) in enumerate(composite_scores):
            print(f"  {i+1}. {alg}: {score:.3f}")
        
        return df, comparison_data
    
    def _calculate_composite_scores(self) -> List[Tuple[str, float]]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        scores = {}
        
        # è·å–æ‰€æœ‰æŒ‡æ ‡çš„å€¼
        success_rates = [(alg, np.mean(results['successes'])) for alg, results in self.results.items()]
        avg_steps = [(alg, np.mean(results['steps'])) for alg, results in self.results.items()]
        variances = [(alg, results['success_rate_var']) for alg, results in self.results.items()]
        
        # å½’ä¸€åŒ–åˆ†æ•° (0-1)
        max_success = max([s[1] for s in success_rates])
        min_steps = min([s[1] for s in avg_steps])
        max_steps = max([s[1] for s in avg_steps])
        min_var = min([v[1] for v in variances])
        max_var = max([v[1] for v in variances])
        
        for alg in self.results.keys():
            # æˆåŠŸç‡å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            success_score = next(s[1] for s in success_rates if s[0] == alg) / max_success if max_success > 0 else 0
            
            # æ•ˆç‡å¾—åˆ†ï¼ˆæ­¥æ•°è¶Šå°‘è¶Šå¥½ï¼‰
            steps_val = next(s[1] for s in avg_steps if s[0] == alg)
            efficiency_score = (max_steps - steps_val) / (max_steps - min_steps) if max_steps > min_steps else 0
            
            # ç¨³å®šæ€§å¾—åˆ†ï¼ˆæ–¹å·®è¶Šå°è¶Šå¥½ï¼‰
            var_val = next(v[1] for v in variances if v[0] == alg)
            stability_score = (max_var - var_val) / (max_var - min_var) if max_var > min_var else 0
            
            # ç»¼åˆå¾—åˆ†
            composite_score = success_score * 0.4 + efficiency_score * 0.3 + stability_score * 0.3
            scores[alg] = composite_score
        
        return sorted(scores.items(), key=lambda x: x[1], reverse=True)
    
    def create_comprehensive_visualizations(self):
        """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig = plt.figure(figsize=(20, 16))
        
        # 1. æˆåŠŸç‡å¯¹æ¯”æŸ±çŠ¶å›¾
        ax1 = plt.subplot(2, 3, 1)
        algs = list(self.results.keys())
        success_rates = [np.mean(self.results[alg]['successes']) for alg in algs]
        
        bars = ax1.bar(algs, success_rates)
        ax1.set_title('ğŸ† Success Rate Comparison', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Success Rate')
        ax1.set_ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{success_rates[i]:.2%}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        
        # 2. å¹³å‡æ­¥æ•°å¯¹æ¯”
        ax2 = plt.subplot(2, 3, 2)
        avg_steps = [np.mean(self.results[alg]['steps']) for alg in algs]
        
        bars = ax2.bar(algs, avg_steps, color='orange')
        ax2.set_title('âš¡ Average Steps Comparison', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Average Steps')
        
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + max(avg_steps)*0.01,
                    f'{avg_steps[i]:.1f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        
        # 3. å¥–åŠ±åˆ†å¸ƒç®±çº¿å›¾
        ax3 = plt.subplot(2, 3, 3)
        reward_data = [self.results[alg]['rewards'] for alg in algs]
        
        box_plot = ax3.boxplot(reward_data, labels=algs)
        ax3.set_title('ğŸ’° Reward Distribution', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Reward')
        plt.xticks(rotation=45)
        
        # 4. ç¨³å®šæ€§å¯¹æ¯”ï¼ˆæˆåŠŸç‡æ–¹å·®ï¼‰
        ax4 = plt.subplot(2, 3, 4)
        stability_vars = [self.results[alg]['success_rate_var'] for alg in algs]
        
        bars = ax4.bar(algs, stability_vars, color='green')
        ax4.set_title('ğŸ¯ Stability Comparison (Lower Variance Better)', fontsize=14, fontweight='bold')
        ax4.set_ylabel('Success Rate Variance')
        
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + max(stability_vars)*0.01,
                    f'{stability_vars[i]:.4f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        
        # 5. ç»¼åˆè¯„åˆ†é›·è¾¾å›¾
        ax5 = plt.subplot(2, 3, 5, projection='polar')
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        metrics = ['Success Rate', 'Efficiency', 'Stability']
        
        # ä¸ºæ¯ä¸ªç®—æ³•è®¡ç®—å½’ä¸€åŒ–åˆ†æ•°
        max_success = max([np.mean(self.results[alg]['successes']) for alg in algs])
        min_steps = min([np.mean(self.results[alg]['steps']) for alg in algs])
        max_steps = max([np.mean(self.results[alg]['steps']) for alg in algs])
        min_var = min([self.results[alg]['success_rate_var'] for alg in algs])
        max_var = max([self.results[alg]['success_rate_var'] for alg in algs])
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆ
        
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
        
        for i, alg in enumerate(algs[:6]):  # æœ€å¤šæ˜¾ç¤º6ä¸ªç®—æ³•
            # è®¡ç®—å½’ä¸€åŒ–åˆ†æ•°
            success_norm = np.mean(self.results[alg]['successes']) / max_success if max_success > 0 else 0
            efficiency_norm = (max_steps - np.mean(self.results[alg]['steps'])) / (max_steps - min_steps) if max_steps > min_steps else 0
            stability_norm = (max_var - self.results[alg]['success_rate_var']) / (max_var - min_var) if max_var > min_var else 0
            
            values = [success_norm, efficiency_norm, stability_norm]
            values += values[:1]  # é—­åˆ
            
            ax5.plot(angles, values, 'o-', linewidth=2, label=alg, color=colors[i])
            ax5.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax5.set_xticks(angles[:-1])
        ax5.set_xticklabels(metrics)
        ax5.set_title('ğŸ•¸ï¸ Comprehensive Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)
        ax5.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        
        # 6. å­¦ä¹ æ›²çº¿å¯¹æ¯”ï¼ˆå¦‚æœæœ‰è®­ç»ƒæ•°æ®ï¼‰
        ax6 = plt.subplot(2, 3, 6)
        
        # ç»˜åˆ¶æˆåŠŸç‡éšæ—¶é—´çš„å˜åŒ–ï¼ˆæ¨¡æ‹Ÿå­¦ä¹ æ›²çº¿ï¼‰
        for i, alg in enumerate(algs):
            # ä½¿ç”¨æˆåŠŸç‡çš„ç´¯ç§¯å¹³å‡ä½œä¸ºå­¦ä¹ æ›²çº¿
            successes = self.results[alg]['successes']
            cumulative_success = np.cumsum(successes) / np.arange(1, len(successes) + 1)
            ax6.plot(cumulative_success, label=alg, color=colors[i % len(colors)])
        
        ax6.set_title('ğŸ“ˆ Convergence Curve (Cumulative Success Rate)', fontsize=14, fontweight='bold')
        ax6.set_xlabel('Test Episode')
        ax6.set_ylabel('Cumulative Success Rate')
        ax6.legend()
        ax6.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"algorithm_comparison_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“„ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {filename}")
        
        plt.show()
    
    def save_detailed_results(self):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            'timestamp': timestamp,
            'test_config': {
                'test_episodes': self.test_episodes,
                'stability_tests': self.stability_tests,
                'track_size': self.env.track_size,
                'max_speed': self.env.max_speed
            },
            'results': {}
        }
        
        # å¤„ç†æ¯ä¸ªç®—æ³•çš„ç»“æœ
        for alg_name, results in self.results.items():
            save_data['results'][alg_name] = {
                'success_rate': float(np.mean(results['successes'])),
                'avg_reward': float(np.mean(results['rewards'])),
                'avg_steps': float(np.mean(results['steps'])),
                'reward_std': float(np.std(results['rewards'])),
                'steps_std': float(np.std(results['steps'])),
                'success_rate_variance': float(results['success_rate_var']),
                'total_tests': len(results['successes']),
                'successful_tests': sum(results['successes'])
            }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        filename = f"comprehensive_comparison_results_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")
    
    def run_full_comparison(self):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”æµç¨‹"""
        print("ğŸš€ å¼€å§‹å¼ºåŒ–å­¦ä¹ ç®—æ³•å…¨é¢å¯¹æ¯”")
        print("="*60)
        
        start_time = time.time()
        
        # 1. åŠ è½½æˆ–è®­ç»ƒæ‰€æœ‰ç®—æ³•
        self.load_or_train_all_algorithms()
        
        # 2. å…¨é¢æ€§èƒ½æµ‹è¯•
        self.comprehensive_performance_test()
        
        # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        df, comparison_data = self.generate_comprehensive_report()
        
        # 4. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        self.create_comprehensive_visualizations()
        
        # 5. ä¿å­˜è¯¦ç»†ç»“æœ
        self.save_detailed_results()
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ‰ å…¨é¢å¯¹æ¯”å®Œæˆï¼")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š å…±æµ‹è¯•äº†{len(self.algorithms)}ä¸ªç®—æ³•")
        print(f"ğŸ§ª æ¯ä¸ªç®—æ³•è¿›è¡Œäº†{self.test_episodes}æ¬¡åŸºç¡€æµ‹è¯•å’Œ{self.stability_tests}æ¬¡ç¨³å®šæ€§æµ‹è¯•")
        
        return self.results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ å¼ºåŒ–å­¦ä¹ ç®—æ³•ç»ˆæå¯¹æ¯”å¤§æˆ˜ï¼")
    print("æœ¬æ¬¡å¯¹æ¯”å°†å…¨é¢è¯„ä¼°ä»¥ä¸‹ç®—æ³•:")
    print("  1. REINFORCE (ç­–ç•¥æ¢¯åº¦)")
    print("  2. Actor-Critic (Actor-Critic)")
    print("  3. PPO (è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–)")
    print("  4. TRPO (ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–)")
    print("  5. Q-Learning (å€¼å‡½æ•°æ–¹æ³•)")
    print("  6. Sarsa(Î») (å€¼å‡½æ•°æ–¹æ³•)")
    print("  7. Q-Guided AC (æ··åˆæ–¹æ³•)")
    print("  8. Ultra Q-Guided AC (è¶…çº§ä¼˜åŒ–æ··åˆæ–¹æ³•)")
    
    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = AlgorithmComparator()
    
    # è¿è¡Œå®Œæ•´å¯¹æ¯”
    results = comparator.run_full_comparison()
    
    print("\n" + "="*60)
    print("ğŸ† ç»ˆæå¯¹æ¯”æ€»ç»“")
    print("="*60)
    
    # æ‰¾å‡ºæœ€ä½³ç®—æ³•
    best_success = max(results.items(), key=lambda x: np.mean(x[1]['successes']))
    best_efficiency = min(results.items(), key=lambda x: np.mean(x[1]['steps']))
    best_stability = min(results.items(), key=lambda x: x[1]['success_rate_var'])
    
    print(f"ğŸ¥‡ æœ€é«˜æˆåŠŸç‡: {best_success[0]} ({np.mean(best_success[1]['successes']):.2%})")
    print(f"âš¡ æœ€é«˜æ•ˆç‡: {best_efficiency[0]} ({np.mean(best_efficiency[1]['steps']):.1f}æ­¥)")
    print(f"ğŸ¯ æœ€ä½³ç¨³å®šæ€§: {best_stability[0]} (æ–¹å·®: {best_stability[1]['success_rate_var']:.6f})")
    
    # ç»¼åˆè¯„åˆ†å† å†›
    composite_scores = comparator._calculate_composite_scores()
    champion = composite_scores[0]
    print(f"ğŸ‘‘ ç»¼åˆå† å†›: {champion[0]} (å¾—åˆ†: {champion[1]:.3f})")
    
    print(f"\nğŸŠ æ­å–œï¼å¯¹æ¯”åˆ†æå®Œæˆã€‚æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Šæ–‡ä»¶ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main() 
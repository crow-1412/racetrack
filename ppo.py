"""
PPO (Proximal Policy Optimization) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - èµ›è½¦è½¨é“é—®é¢˜ - ä¼˜åŒ–ç‰ˆ

ä¼˜åŒ–æ”¹è¿›ï¼š
1. æ™ºèƒ½å¥–åŠ±å¡‘å½¢ - å¯¹å‰è¿›ç»™äºˆå¾®å¼±æ­£å¥–åŠ±ï¼Œè§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜
2. æ”¾æ¾KLæ•£åº¦é™åˆ¶ - å…è®¸æ›´å¤§çš„ç­–ç•¥æ›´æ–°
3. è°ƒæ•´å­¦ä¹ ç‡ - æé«˜åˆ°åˆç†æ°´å¹³
4. å¢å¤§æ‰¹é‡å¤§å° - æé«˜è®­ç»ƒç¨³å®šæ€§
5. æ”¹è¿›ç½‘ç»œæ¶æ„ - æ›´é€‚åˆç¦»æ•£åŠ¨ä½œç©ºé—´

ä½œè€…ï¼šAI Assistant  
æœ€åæ›´æ–°ï¼š2024å¹´ - ä¼˜åŒ–ç‰ˆ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Tuple, List, Dict
import random
from collections import deque
import matplotlib.pyplot as plt
from racetrack_env import RacetrackEnv

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print(f"ğŸ² ä¼˜åŒ–ç‰ˆPPOéšæœºç§å­å·²è®¾ç½®ä¸º: {RANDOM_SEED}")


class OptimizedPPONetwork(nn.Module):
    """
    ä¼˜åŒ–ç‰ˆPPOç½‘ç»œæ¶æ„
    
    æ”¹è¿›ï¼š
    - é€‚ä¸­çš„ç½‘ç»œè§„æ¨¡
    - æ›´å¥½çš„åˆå§‹åŒ–
    - é’ˆå¯¹ç¦»æ•£åŠ¨ä½œç©ºé—´ä¼˜åŒ–
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(OptimizedPPONetwork, self).__init__()
        
        # é€‚ä¸­çš„ç½‘ç»œç»“æ„
        self.shared_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # Actorå¤´éƒ¨
        self.actor_head = nn.Linear(hidden_dim // 2, action_dim)
        
        # Criticå¤´éƒ¨  
        self.critic_head = nn.Linear(hidden_dim // 2, 1)
        
        # åˆç†çš„å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆç†çš„å‚æ•°åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=1.0)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        shared_features = self.shared_layers(state)
        action_logits = self.actor_head(shared_features)
        value = self.critic_head(shared_features)
        return action_logits, value


class OptimizedPPOBuffer:
    """
    ä¼˜åŒ–ç‰ˆPPOç»éªŒç¼“å†²åŒº
    """
    def __init__(self, max_size: int):
        self.max_size = max_size
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.advantages = []
        self.returns = []
    
    def add(self, state, action, reward, value, log_prob, done):
        """æ·»åŠ ç»éªŒ"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)
    
    def size(self):
        """è·å–ç¼“å†²åŒºå¤§å°"""
        return len(self.states)
    
    def compute_advantages_and_returns(self, gamma: float, gae_lambda: float, next_value: float = 0):
        """
        ä¼˜åŒ–çš„GAEä¼˜åŠ¿è®¡ç®—
        """
        rewards = np.array(self.rewards, dtype=np.float32)
        values = np.array([v.detach().item() if isinstance(v, torch.Tensor) else v for v in self.values], dtype=np.float32)
        dones = np.array(self.dones, dtype=np.bool_)
        
        # åˆç†çš„å¥–åŠ±è£å‰ª
        rewards = np.clip(rewards, -50, 50)
        
        # GAEè®¡ç®—
        advantages = np.zeros_like(rewards, dtype=np.float32)
        last_gae = 0.0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - float(dones[t])
                next_value_t = next_value
            else:
                next_non_terminal = 1.0 - float(dones[t])
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * next_non_terminal - values[t]
            advantages[t] = last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae
        
        # è®¡ç®—returns
        returns = advantages + values
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if np.any(np.isnan(advantages)) or np.any(np.isinf(advantages)):
            print("âš ï¸ è­¦å‘Šï¼šGAEè®¡ç®—å¼‚å¸¸ï¼Œä½¿ç”¨ç®€å•TDè¯¯å·®")
            advantages = rewards - values
            returns = rewards.copy()
        
        self.advantages = advantages.tolist()
        self.returns = returns.tolist()
    
    def get_batch(self, batch_size: int):
        """è·å–æ‰¹é‡æ•°æ®"""
        indices = np.random.choice(self.size(), min(batch_size, self.size()), replace=False)
        
        batch_states = torch.stack([self.states[i] for i in indices])
        batch_actions = torch.tensor([self.actions[i] for i in indices], dtype=torch.long)
        batch_old_log_probs = torch.stack([self.log_probs[i].detach() for i in indices])
        batch_advantages = torch.tensor([self.advantages[i] for i in indices], dtype=torch.float32)
        batch_returns = torch.tensor([self.returns[i] for i in indices], dtype=torch.float32)
        
        return batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns


class OptimizedPPORacetrackAgent:
    """
    ä¼˜åŒ–ç‰ˆPPOèµ›è½¦è½¨é“æ™ºèƒ½ä½“
    
    ä¸»è¦ä¼˜åŒ–ï¼š
    1. æ™ºèƒ½å¥–åŠ±å¡‘å½¢
    2. æ”¾æ¾KLæ•£åº¦é™åˆ¶
    3. åˆç†çš„å­¦ä¹ ç‡å’Œæ‰¹é‡å¤§å°
    4. æ”¹è¿›çš„ç½‘ç»œæ¶æ„
    """
    
    def __init__(self, env: RacetrackEnv, gamma: float = 0.99,
                 gae_lambda: float = 0.95, clip_ratio: float = 0.2,  # æ ‡å‡†è£å‰ªæ¯”ä¾‹
                 ppo_epochs: int = 4, batch_size: int = 128,  # å¢å¤§æ‰¹é‡å’Œæ›´æ–°è½®æ•°
                 buffer_size: int = 1024, hidden_dim: int = 128):  # å¢å¤§ç¼“å†²åŒºå’Œç½‘ç»œ
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆPPOæ™ºèƒ½ä½“
        """
        self.env = env
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.buffer_size = buffer_size
        
        # çŠ¶æ€ç‰¹å¾ç»´åº¦
        self.state_dim = 8
        self.action_dim = env.n_actions
        
        # åˆ›å»ºä¼˜åŒ–ç‰ˆç½‘ç»œ
        self.network = OptimizedPPONetwork(self.state_dim, self.action_dim, hidden_dim)
        
        # åˆç†çš„å­¦ä¹ ç‡
        self.optimizer = optim.Adam(
            self.network.parameters(),
            lr=3e-4,  # æ ‡å‡†å­¦ä¹ ç‡
            eps=1e-5
        )
        
        # ä¼˜åŒ–ç‰ˆç¼“å†²åŒº
        self.buffer = OptimizedPPOBuffer(buffer_size)
        
        # åˆç†çš„æ¢ç´¢ç­–ç•¥
        self.epsilon = 0.05  # ğŸ”¥ ä»0.1é™åˆ°0.05ï¼Œå‡å°‘éšæœºæ¢ç´¢
        self.epsilon_min = 0.01  # ä»0.02é™åˆ°0.01
        self.epsilon_decay = 0.995
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        self.success_rate: List[float] = []
        self.policy_losses: List[float] = []
        self.value_losses: List[float] = []
        self.entropy_losses: List[float] = []
        self.kl_divergences: List[float] = []
        
        # æ”¾æ¾çš„è¶…å‚æ•°
        self.target_kl = 0.5  # æ”¾æ¾KLæ•£åº¦é™åˆ¶
        self.value_coef = 0.5  # æ ‡å‡†ä»·å€¼æŸå¤±æƒé‡
        self.entropy_coef = 0.01  # é€‚ä¸­çš„ç†µç³»æ•°
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤
        self.best_success_rate = 0.0
        self.best_model_state = None
        self.patience = 0
        self.max_patience = 100
        
        # å¥–åŠ±å¡‘å½¢å‚æ•°
        self.last_distance_to_goal = None
        self.progress_reward_scale = 0.1  # å‰è¿›å¥–åŠ±çš„ç¼©æ”¾å› å­
    
    def state_to_tensor(self, state: Tuple[int, int, int, int]) -> torch.Tensor:
        """çŠ¶æ€è½¬æ¢ä¸ºå¼ é‡"""
        x, y, vx, vy = state
        
        # åŸºç¡€ç‰¹å¾å½’ä¸€åŒ–
        norm_x = x / 31.0
        norm_y = y / 16.0  
        norm_vx = vx / self.env.max_speed
        norm_vy = vy / self.env.max_speed
        
        # è®¡ç®—åˆ°æœ€è¿‘ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_distance = float('inf')
        goal_direction_x, goal_direction_y = 0, 0
        
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            if distance < min_distance:
                min_distance = distance
                if distance > 0:
                    goal_direction_x = -(goal_x - x) / distance
                    goal_direction_y = (goal_y - y) / distance
        
        # è·ç¦»å½’ä¸€åŒ–
        max_distance = np.sqrt(31**2 + 16**2)
        norm_distance = min_distance / max_distance
        
        # é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        velocity_alignment = 0.0
        if min_distance > 0:
            velocity_mag = np.sqrt(vx**2 + vy**2)
            if velocity_mag > 0:
                vel_dir_x = vx / velocity_mag
                vel_dir_y = vy / velocity_mag
                velocity_alignment = max(0, vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y)
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_distance, goal_direction_x, goal_direction_y, 
            velocity_alignment
        ], dtype=torch.float32)
    
    def apply_action_mask(self, state: Tuple[int, int, int, int], 
                         action_logits: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨åŠ¨ä½œæ©ç """
        x, y, vx, vy = state
        mask = torch.zeros_like(action_logits)
        
        for i, (ax, ay) in enumerate(self.env.actions):
            new_vx = max(0, min(self.env.max_speed, vx + ax))
            new_vy = max(0, min(self.env.max_speed, vy + ay))
            
            if new_vx == 0 and new_vy == 0 and (x, y) not in self.env.start_positions:
                new_vx = 1
                new_vy = 1
            
            new_x = x - new_vx
            new_y = y + new_vy
            
            if self.env._check_collision(x, y, new_x, new_y):
                mask[i] = -1e9  # å¤§çš„è´Ÿå€¼æ©ç 
        
        masked_logits = action_logits + mask
        
        if torch.all(mask == -1e9):
            mask.fill_(0)
            masked_logits = action_logits
        
        return masked_logits
    
    def select_action(self, state: Tuple[int, int, int, int], training: bool = True):
        """é€‰æ‹©åŠ¨ä½œï¼ˆä¿®å¤ç‰ˆ - å½»åº•è§£å†³è®­ç»ƒæµ‹è¯•ä¸ä¸€è‡´é—®é¢˜ï¼‰"""
        state_tensor = self.state_to_tensor(state)
        
        with torch.no_grad() if not training else torch.enable_grad():
            action_logits, value = self.network(state_tensor)
            
            # åº”ç”¨åŠ¨ä½œæ©ç 
            masked_logits = self.apply_action_mask(state, action_logits)
            
            # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒ
            action_dist = Categorical(logits=masked_logits)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç»Ÿä¸€åŠ¨ä½œé€‰æ‹©é€»è¾‘
            if training:
                # è®­ç»ƒæ—¶ï¼šä»åˆ†å¸ƒä¸­é‡‡æ ·ï¼ˆå¸¦æœ‰ä¸€å®šéšæœºæ€§ï¼‰
                if random.random() < self.epsilon:
                    # æ¸©å’Œçš„æ¢ç´¢ï¼šä»softmaxåˆ†å¸ƒé‡‡æ ·è€Œä¸æ˜¯å®Œå…¨éšæœº
                    action = action_dist.sample()
                else:
                    # å¤§éƒ¨åˆ†æ—¶å€™ä»ä½¿ç”¨è´ªå©ªç­–ç•¥ï¼Œç¡®ä¿ç½‘ç»œå­¦ä¹ æ­£ç¡®æ–¹å‘
                    action = torch.argmax(masked_logits)
            else:
                # æµ‹è¯•æ—¶ï¼šä¹Ÿä½¿ç”¨ç›¸åŒçš„è´ªå©ªç­–ç•¥
                action = torch.argmax(masked_logits)
            
            log_prob = action_dist.log_prob(action)
        
        return action.item(), log_prob, value.squeeze()
    
    def intelligent_reward_shaping(self, prev_state, state, next_state, reward, done, steps):
        """
        ä¿®æ­£ç‰ˆå¥–åŠ±å¡‘å½¢ - å‡å°‘è¿‡åº¦ä¹è§‚ï¼Œç¡®ä¿ç½‘ç»œå­¦åˆ°çœŸå®ç­–ç•¥
        """
        shaped_reward = reward  # ä¿ç•™åŸå§‹å¥–åŠ±
        
        x, y, vx, vy = state
        
        # 1. å‰è¿›å¥–åŠ± - å¤§å¹…å‡å¼±ï¼Œé¿å…è™šå‡ä¿¡å·
        current_distance = float('inf')
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            current_distance = min(current_distance, distance)
        
        if self.last_distance_to_goal is not None:
            progress = self.last_distance_to_goal - current_distance
            if progress > 0:  # å‘ç›®æ ‡é è¿‘
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤§å¹…å‡å°‘å‰è¿›å¥–åŠ±ï¼Œé¿å…è™šå‡ç§¯æä¿¡å·
                shaped_reward += progress * 0.02  # ä»0.1é™åˆ°0.02
            elif progress < -2:  # è¿œç¦»ç›®æ ‡å¤ªå¤š
                shaped_reward -= 0.02  # è½»å¾®æƒ©ç½š
        
        self.last_distance_to_goal = current_distance
        
        # 2. é€Ÿåº¦å¥–åŠ± - å‡å¼±
        speed = np.sqrt(vx**2 + vy**2)
        if 1 <= speed <= 3:  # åˆç†é€Ÿåº¦èŒƒå›´
            shaped_reward += 0.005  # ä»0.02é™åˆ°0.005
        elif speed == 0:  # æƒ©ç½šåœæ­¢
            shaped_reward -= 0.02
        
        # 3. æ–¹å‘å¥–åŠ± - å‡å¼±ä½†ä¿ç•™
        if current_distance > 0:
            goal_direction_x = -(self.env.goal_positions[0][0] - x) / current_distance
            goal_direction_y = (self.env.goal_positions[0][1] - y) / current_distance
            
            if speed > 0:
                vel_dir_x = vx / speed
                vel_dir_y = vy / speed
                alignment = vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y
                if alignment > 0.5:  # æ–¹å‘å¯¹é½
                    shaped_reward += 0.005  # ä»0.01é™åˆ°0.005
        
        # 4. æ­¥æ•°æƒ©ç½š - ä¿æŒ
        shaped_reward -= 0.01
        
        # 5. ç‰¹æ®Šæƒ…å†µå¤„ç† - å¼ºåŒ–çœŸå®æˆåŠŸå¥–åŠ±
        if done:
            if reward == 100:  # æˆåŠŸ
                shaped_reward += 50  # å¤§å¹…å¥–åŠ±çœŸå®æˆåŠŸ
            elif reward == -10:  # ç¢°æ’
                shaped_reward -= 10   # å¢åŠ ç¢°æ’æƒ©ç½š
            else:  # è¶…æ—¶
                shaped_reward -= 5    # å¢åŠ è¶…æ—¶æƒ©ç½š
        
        return shaped_reward
    
    def collect_trajectory(self, max_steps: int = 300) -> Tuple[float, int, bool]:
        """æ”¶é›†è½¨è¿¹"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        self.last_distance_to_goal = None  # é‡ç½®è·ç¦»è·Ÿè¸ª
        
        for _ in range(max_steps):
            action, log_prob, value = self.select_action(state, training=True)
            prev_state = state
            
            next_state, reward, done = self.env.step(action)
            
            # æ™ºèƒ½å¥–åŠ±å¡‘å½¢
            shaped_reward = self.intelligent_reward_shaping(
                prev_state, state, next_state, reward, done, steps
            )
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.add(
                self.state_to_tensor(prev_state),
                action,
                shaped_reward,
                value,
                log_prob,
                done
            )
            
            total_reward += reward  # è®°å½•åŸå§‹å¥–åŠ±
            steps += 1
            
            if done:
                break
                
            state = next_state
        
        # è®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼
        if not done:
            _, _, next_value = self.select_action(state, training=True)
        else:
            next_value = 0.0
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        self.buffer.compute_advantages_and_returns(self.gamma, self.gae_lambda, next_value)
        
        success = (done and reward == 100)
        return total_reward, steps, success
    
    def update_policy(self):
        """
        ä¼˜åŒ–çš„PPOç­–ç•¥æ›´æ–°
        """
        if self.buffer.size() < self.batch_size:
            return
        
        # ä¼˜åŠ¿å½’ä¸€åŒ–
        advantages = torch.tensor(self.buffer.advantages, dtype=torch.float32)
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if torch.isnan(advantages).any() or torch.isinf(advantages).any():
            print("âš ï¸ ä¼˜åŠ¿è®¡ç®—å¼‚å¸¸ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
            self.buffer.clear()
            return
        
        # æ ‡å‡†ä¼˜åŠ¿å½’ä¸€åŒ–
        if advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        total_kl_div = 0
        update_count = 0
        
        # PPOæ›´æ–°
        for epoch in range(self.ppo_epochs):
            # è·å–æ‰¹é‡æ•°æ®
            batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns = \
                self.buffer.get_batch(self.batch_size)
            
            # ä½¿ç”¨å½’ä¸€åŒ–çš„ä¼˜åŠ¿
            batch_advantages = advantages[:len(batch_advantages)]
            
            # é‡æ–°è®¡ç®—åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼
            action_logits, values = self.network(batch_states)
            
            action_dist = Categorical(logits=action_logits)
            log_probs = action_dist.log_prob(batch_actions)
            entropy = action_dist.entropy()
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            ratio = torch.exp(log_probs - batch_old_log_probs)
            
            # æ£€æŸ¥æ¯”ç‡æ˜¯å¦å¼‚å¸¸
            if torch.isnan(ratio).any() or torch.isinf(ratio).any():
                print("âš ï¸ é‡è¦æ€§é‡‡æ ·æ¯”ç‡å¼‚å¸¸ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
                continue
            
            # PPO Clipped Surrogate Objective
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼å‡½æ•°æŸå¤±
            batch_returns_tensor = torch.tensor(self.buffer.returns[:len(batch_returns)], dtype=torch.float32)
            value_loss = F.mse_loss(values.squeeze(), batch_returns_tensor)
            
            # ç†µæŸå¤±
            entropy_loss = entropy.mean()
            
            # æ€»æŸå¤±
            total_loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy_loss
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦å¼‚å¸¸
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print("âš ï¸ æŸå¤±å‡½æ•°å¼‚å¸¸ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
                continue
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            total_entropy_loss += entropy_loss.item()
            update_count += 1
            
            # è®¡ç®—KLæ•£åº¦ï¼ˆæ”¾æ¾æ£€æŸ¥ï¼‰
            with torch.no_grad():
                kl_div = (batch_old_log_probs - log_probs).mean()
                total_kl_div += kl_div.item()
                
                # æ”¾æ¾çš„KLæ•£åº¦æ§åˆ¶
                if kl_div > 2.0 * self.target_kl:  # æ›´å®½æ¾çš„é™åˆ¶
                    print(f"ğŸ“Š KLæ•£åº¦è¾ƒå¤§ ({kl_div:.4f}), æå‰åœæ­¢epoch {epoch}")
                    break
        
        # è®°å½•å¹³å‡æŸå¤±
        if update_count > 0:
            avg_policy_loss = total_policy_loss / update_count
            avg_value_loss = total_value_loss / update_count
            avg_entropy_loss = total_entropy_loss / update_count
            avg_kl_div = total_kl_div / update_count
            
            self.policy_losses.append(avg_policy_loss)
            self.value_losses.append(avg_value_loss)
            self.entropy_losses.append(avg_entropy_loss)
            self.kl_divergences.append(avg_kl_div)
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
    
    def train_episode(self, episode_num: int) -> Tuple[float, int, bool]:
        """è®­ç»ƒå•ä¸ªepisode"""
        # æ”¶é›†è½¨è¿¹
        reward, steps, success = self.collect_trajectory()
        
        # æ›´æ–°ç­–ç•¥
        self.update_policy()
        
        # æ¢ç´¢ç‡è¡°å‡
        if episode_num % 10 == 0:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        return reward, steps, success
    
    def test_episode(self, render: bool = False) -> Tuple[float, int, List, bool]:
        """æµ‹è¯•å•ä¸ªepisodeï¼ˆä¿®å¤ç‰ˆ - ç¡®ä¿ä¸è®­ç»ƒé€»è¾‘ä¸€è‡´ï¼‰"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]
        max_steps = 300
        
        # é‡ç½®å¥–åŠ±å¡‘å½¢çŠ¶æ€
        self.last_distance_to_goal = None
        
        self.network.eval()
        with torch.no_grad():
            while steps < max_steps:
                action, _, _ = self.select_action(state, training=False)
                prev_state = state
                next_state, reward, done = self.env.step(action)
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨å¥–åŠ±å¡‘å½¢é€»è¾‘ï¼ˆä½†åªç”¨äºæˆåŠŸåˆ¤æ–­ï¼‰
                shaped_reward = self.intelligent_reward_shaping(
                    prev_state, state, next_state, reward, done, steps
                )
                
                total_reward += reward  # è®°å½•åŸå§‹å¥–åŠ±
                steps += 1
                path.append(next_state[:2])
                
                if done:
                    break
                
                state = next_state
        
        self.network.train()
        
        # ä½¿ç”¨åŸå§‹å¥–åŠ±è¿›è¡ŒæˆåŠŸåˆ¤æ–­ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        success = (done and reward == 100)
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'network': self.network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_rate': self.success_rate,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'entropy_losses': self.entropy_losses,
            'kl_divergences': self.kl_divergences,
            'epsilon': self.epsilon
        }
        torch.save(save_dict, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
        self.network.load_state_dict(checkpoint['network'])


def main_optimized_ppo_training():
    """
    ä¼˜åŒ–ç‰ˆPPOä¸»è®­ç»ƒå‡½æ•°
    
    ä¸»è¦ä¼˜åŒ–ï¼š
    1. æ™ºèƒ½å¥–åŠ±å¡‘å½¢
    2. æ”¾æ¾KLæ•£åº¦é™åˆ¶
    3. åˆç†çš„å­¦ä¹ ç‡å’Œæ‰¹é‡å¤§å°
    4. æ”¹è¿›çš„ç½‘ç»œæ¶æ„
    """
    print("=== ä¼˜åŒ–ç‰ˆPPOèµ›è½¦è½¨é“è®­ç»ƒ ===")
    print(f"ğŸ² ä½¿ç”¨å›ºå®šéšæœºç§å­: {RANDOM_SEED}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆPPOæ™ºèƒ½ä½“
    agent = OptimizedPPORacetrackAgent(
        env=env,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,     # æ ‡å‡†è£å‰ª
        ppo_epochs=4,       # æ ‡å‡†æ›´æ–°è½®æ•°
        batch_size=128,     # å¢å¤§æ‰¹é‡
        buffer_size=1024,   # å¢å¤§ç¼“å†²åŒº
        hidden_dim=128      # é€‚ä¸­ç½‘ç»œ
    )
    
    print(f"ä¼˜åŒ–ç‰ˆPPOé…ç½®:")
    print(f"  - å­¦ä¹ ç‡: 3e-4 (æ ‡å‡†)")
    print(f"  - PPOè½®æ•°: 4 (æ ‡å‡†)")
    print(f"  - æ‰¹é‡å¤§å°: 128 (å¢å¤§)")
    print(f"  - ç¼“å†²åŒºå¤§å°: 1024 (å¢å¤§)")
    print(f"  - éšè—å±‚ç»´åº¦: 128 (é€‚ä¸­)")
    print(f"  - è£å‰ªæ¯”ä¾‹: 0.2 (æ ‡å‡†)")
    print(f"  - ç›®æ ‡KLæ•£åº¦: 0.5 (æ”¾æ¾)")
    print(f"  - å¥–åŠ±å¡‘å½¢: æ™ºèƒ½å‰è¿›å¥–åŠ±")
    
    # è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
    print("\n=== è®­ç»ƒå‰åŸºå‡† ===")
    reward_before, steps_before, _, success_before = agent.test_episode()
    print(f"åŸºå‡†æ€§èƒ½: å¥–åŠ±={reward_before:.1f}, æ­¥æ•°={steps_before}, æˆåŠŸ={success_before}")
    
    # è®­ç»ƒè®¾ç½®
    n_episodes = 2000
    
    print(f"\n=== å¼€å§‹ä¼˜åŒ–ç‰ˆPPOè®­ç»ƒ ===")
    print(f"è®­ç»ƒè½®æ•°: {n_episodes}")
    
    # è®­ç»ƒç»Ÿè®¡
    success_window = deque(maxlen=100)
    reward_window = deque(maxlen=50)
    
    # æœ€ä½³æ¨¡å‹ä¿æŠ¤
    best_success_rate = 0.0
    best_model_state = None
    patience = 0
    
    for episode in range(n_episodes):
        # è®­ç»ƒä¸€ä¸ªepisode
        reward, steps, success = agent.train_episode(episode)
        
        agent.episode_rewards.append(reward)
        agent.episode_steps.append(steps)
        success_window.append(1 if success else 0)
        reward_window.append(reward)
        
        current_success_rate = np.mean(success_window)
        agent.success_rate.append(current_success_rate)
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤
        if episode >= 50 and current_success_rate > best_success_rate:
            best_success_rate = current_success_rate
            best_model_state = {
                'network': agent.network.state_dict().copy(),
                'optimizer': agent.optimizer.state_dict().copy(),
                'episode': episode,
                'success_rate': current_success_rate
            }
            patience = 0
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: Episode {episode+1}, æˆåŠŸç‡={current_success_rate:.3f}")
        else:
            patience += 1
        
        # æ€§èƒ½é€€åŒ–æ£€æµ‹
        if patience > agent.max_patience and best_model_state:
            print(f"\nâš ï¸ æ€§èƒ½åœæ»ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹...")
            agent.network.load_state_dict(best_model_state['network'])
            agent.optimizer.load_state_dict(best_model_state['optimizer'])
            print(f"   å·²æ¢å¤Episode {best_model_state['episode']+1}çš„æ¨¡å‹")
            patience = 0
            agent.epsilon = max(0.05, agent.epsilon * 1.2)  # å¢åŠ æ¢ç´¢
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(reward_window)
            avg_steps = np.mean(agent.episode_steps[-25:])
            avg_policy_loss = np.mean(agent.policy_losses[-10:]) if agent.policy_losses else 0
            avg_value_loss = np.mean(agent.value_losses[-10:]) if agent.value_losses else 0
            avg_kl_div = np.mean(agent.kl_divergences[-10:]) if agent.kl_divergences else 0
            
            print(f"Episode {episode + 1:4d}: "
                  f"å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={avg_steps:5.1f}, "
                  f"æˆåŠŸç‡={current_success_rate:.3f}, Îµ={agent.epsilon:.3f}")
            print(f"                     ç­–ç•¥æŸå¤±={avg_policy_loss:.4f}, "
                  f"ä»·å€¼æŸå¤±={avg_value_loss:.4f}, KLæ•£åº¦={avg_kl_div:.4f}")
            print(f"                     æœ€ä½³æˆåŠŸç‡={best_success_rate:.3f}, è€å¿ƒå€¼={patience}")
    
    # æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    if best_model_state:
        print(f"\nğŸ”„ æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        agent.network.load_state_dict(best_model_state['network'])
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\n=== æœ€ç»ˆè¯„ä¼° ===")
    test_results = []
    for i in range(50):
        reward, steps, path, success = agent.test_episode()
        test_results.append((reward, steps, success))
    
    final_success_rate = np.mean([r[2] for r in test_results])
    final_avg_reward = np.mean([r[0] for r in test_results])
    final_avg_steps = np.mean([r[1] for r in test_results])
    
    print(f"ä¼˜åŒ–ç‰ˆPPOæœ€ç»ˆç»“æœï¼ˆ50æ¬¡æµ‹è¯•ï¼‰:")
    print(f"  æˆåŠŸç‡: {final_success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {final_avg_reward:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {final_avg_steps:.1f}")
    
    # ä¸ä¹‹å‰ç‰ˆæœ¬å¯¹æ¯”
    print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"  åŸç‰ˆPPOæˆåŠŸç‡: 12%")
    print(f"  ç¨³å®šPPOæˆåŠŸç‡: 8%")
    print(f"  ä¼˜åŒ–PPOæˆåŠŸç‡: {final_success_rate:.1%}")
    print(f"  Actor-CriticæˆåŠŸç‡: 62%")
    print(f"  Sarsa(Î»)æˆåŠŸç‡: 90%")
    
    if final_success_rate > 0.5:
        print("ğŸ‰ ä¼˜åŒ–å¤§æˆåŠŸï¼PPOæ€§èƒ½æ˜¾è‘—æå‡")
    elif final_success_rate > 0.3:
        print("âœ… ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼ŒPPOæ€§èƒ½å¤§å¹…æ”¹å–„")
    elif final_success_rate > 0.15:
        print("âš–ï¸ ä¼˜åŒ–æœ‰æ•ˆï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
    else:
        print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # ä¿å­˜æ¨¡å‹
    agent.save_model("models/optimized_ppo_racetrack_model.pth")
    print(f"ä¼˜åŒ–ç‰ˆPPOæ¨¡å‹å·²ä¿å­˜")
    
    # å±•ç¤ºä¸€ä¸ªæˆåŠŸçš„è·¯å¾„
    if final_success_rate > 0:
        print(f"\n=== å±•ç¤ºæœ€ä¼˜è·¯å¾„ ===")
        best_reward = -float('inf')
        best_path = None
        best_steps = 0
        
        for i in range(10):
            reward, steps, path, success = agent.test_episode()
            if success and reward > best_reward:
                best_reward = reward
                best_path = path
                best_steps = steps
        
        if best_path:
            print(f"æœ€ä¼˜è·¯å¾„: å¥–åŠ±={best_reward:.1f}, æ­¥æ•°={best_steps}")
            print(f"è·¯å¾„é•¿åº¦: {len(best_path)}")
            print(f"èµ·ç‚¹: {best_path[0]}")
            print(f"ç»ˆç‚¹: {best_path[-1]}")
            
            # å¯è§†åŒ–è·¯å¾„
            agent.test_episode(render=True)
    
    return agent, test_results


if __name__ == "__main__":
    # è¿è¡Œä¼˜åŒ–ç‰ˆPPOè®­ç»ƒ
    main_optimized_ppo_training() 
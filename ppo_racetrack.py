"""
PPO (Proximal Policy Optimization) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - èµ›è½¦è½¨é“é—®é¢˜

æœ¬æ–‡ä»¶å®ç°äº†åŸºäºPPOç®—æ³•çš„èµ›è½¦è½¨é“æ™ºèƒ½ä½“ï¼Œç»“åˆäº†Actor-Criticä¼˜åŒ–ç‰ˆçš„ä¼˜ç§€ç‰¹æ€§ã€‚

PPOæ ¸å¿ƒç‰¹æ€§ï¼š
1. Clipped Surrogate Objective - é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
2. Multiple Epochs Training - å……åˆ†åˆ©ç”¨é‡‡é›†çš„æ•°æ®
3. Adaptive KL Divergence - è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ æ­¥é•¿
4. ç»§æ‰¿ä¼˜åŒ–ç‰ˆActor-Criticçš„é˜²é€€åŒ–æœºåˆ¶

æŠ€æœ¯æ”¹è¿›ï¼š
1. æ™ºèƒ½çŠ¶æ€è¡¨ç¤ºï¼ˆ8ç»´ç‰¹å¾å‘é‡ï¼‰
2. ä¸¥æ ¼åŠ¨ä½œæ©ç ï¼ˆé¿å…ç¢°æ’ï¼‰
3. åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥
4. æœ€ä½³æ¨¡å‹ä¿æŠ¤æœºåˆ¶
5. GAEä¼˜åŠ¿ä¼°è®¡

ä½œè€…ï¼šAI Assistant
æœ€åæ›´æ–°ï¼š2024å¹´
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Tuple, List, Dict
import random
from collections import deque
import matplotlib.pyplot as plt
from racetrack_env import RacetrackEnv


class PPONetwork(nn.Module):
    """
    PPOç½‘ç»œæ¶æ„ - å…±äº«ç‰¹å¾æå– + åˆ†ç¦»Actor-Criticå¤´éƒ¨
    
    ä¸Actor-Criticç±»ä¼¼çš„æ¶æ„ï¼Œä½†ä¸“é—¨ä¸ºPPOä¼˜åŒ–ï¼š
    - å…±äº«å±‚ï¼šæå–ç¯å¢ƒçŠ¶æ€çš„é€šç”¨ç‰¹å¾
    - Actorå¤´éƒ¨ï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    - Criticå¤´éƒ¨ï¼šä¼°è®¡çŠ¶æ€ä»·å€¼
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(PPONetwork, self).__init__()
        
        # å…±äº«çš„åº•å±‚ç‰¹å¾æå–ç½‘ç»œ
        self.shared_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),  # æ·»åŠ dropoutæé«˜æ³›åŒ–
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Actorå¤´éƒ¨ï¼šè¾“å‡ºåŠ¨ä½œlogits
        self.actor_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, action_dim)
        )
        
        # Criticå¤´éƒ¨ï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼ä¼°è®¡
        self.critic_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1)
        )
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """å‚æ•°åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=0.5)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        shared_features = self.shared_layers(state)
        
        # Actorè¾“å‡ºï¼šåŠ¨ä½œlogits
        action_logits = self.actor_head(shared_features)
        
        # Criticè¾“å‡ºï¼šçŠ¶æ€ä»·å€¼
        value = self.critic_head(shared_features)
        
        return action_logits, value
    
    def get_action_and_value(self, state, action=None):
        """
        è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒå’Œä»·å€¼ï¼Œç”¨äºPPOè®­ç»ƒ
        
        Args:
            state: çŠ¶æ€å¼ é‡
            action: å¦‚æœæä¾›ï¼Œè®¡ç®—è¯¥åŠ¨ä½œçš„logæ¦‚ç‡
            
        Returns:
            action: é‡‡æ ·çš„åŠ¨ä½œï¼ˆå¦‚æœæ²¡æœ‰æä¾›actionå‚æ•°ï¼‰
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            entropy: ç­–ç•¥ç†µ
            value: çŠ¶æ€ä»·å€¼
        """
        action_logits, value = self.forward(state)
        action_dist = Categorical(logits=action_logits)
        
        if action is None:
            action = action_dist.sample()
        
        log_prob = action_dist.log_prob(action)
        entropy = action_dist.entropy()
        
        return action, log_prob, entropy, value


class PPOBuffer:
    """
    PPOç»éªŒç¼“å†²åŒº
    
    å­˜å‚¨ä¸€ä¸ªå®Œæ•´episodeçš„ç»éªŒï¼Œç”¨äºPPOçš„å¤šè½®æ›´æ–°
    """
    def __init__(self, max_size: int):
        self.max_size = max_size
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.advantages = []
        self.returns = []
    
    def add(self, state, action, reward, value, log_prob, done):
        """æ·»åŠ ç»éªŒ"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)
    
    def size(self):
        """è·å–ç¼“å†²åŒºå¤§å°"""
        return len(self.states)
    
    def compute_advantages_and_returns(self, gamma: float, gae_lambda: float, next_value: float = 0):
        """
        è®¡ç®—GAEä¼˜åŠ¿å’Œå›æŠ¥
        
        Args:
            gamma: æŠ˜æ‰£å› å­
            gae_lambda: GAEçš„Î»å‚æ•°
            next_value: æœ€åçŠ¶æ€çš„ä»·å€¼ï¼ˆå¦‚æœepisodeæœªç»“æŸï¼‰
        """
        rewards = np.array(self.rewards)
        values = np.array([v.detach().item() if isinstance(v, torch.Tensor) else v for v in self.values])
        dones = np.array(self.dones)
        
        # è®¡ç®—GAEä¼˜åŠ¿
        advantages = np.zeros_like(rewards)
        last_gae = 0
        
        # ä»åå‘å‰è®¡ç®—
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - dones[t]
                next_value_t = next_value
            else:
                next_non_terminal = 1.0 - dones[t]
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * next_non_terminal - values[t]
            advantages[t] = last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae
        
        # è®¡ç®—å›æŠ¥
        returns = advantages + values
        
        self.advantages = advantages.tolist()
        self.returns = returns.tolist()
    
    def get_batch(self, batch_size: int):
        """è·å–æ‰¹é‡æ•°æ®"""
        indices = np.random.choice(self.size(), min(batch_size, self.size()), replace=False)
        
        batch_states = torch.stack([self.states[i] for i in indices])
        batch_actions = torch.tensor([self.actions[i] for i in indices])
        batch_old_log_probs = torch.stack([self.log_probs[i].detach() for i in indices])
        batch_advantages = torch.tensor([self.advantages[i] for i in indices], dtype=torch.float32)
        batch_returns = torch.tensor([self.returns[i] for i in indices], dtype=torch.float32)
        
        return batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns


class PPORacetrackAgent:
    """
    PPOèµ›è½¦è½¨é“æ™ºèƒ½ä½“
    
    ä¸»è¦ç‰¹æ€§ï¼š
    1. PPOç®—æ³•æ ¸å¿ƒï¼šClipped Surrogate Objective
    2. å¤šè½®æ›´æ–°ï¼šå……åˆ†åˆ©ç”¨é‡‡é›†çš„æ•°æ®
    3. ç»§æ‰¿Actor-Criticä¼˜åŒ–ç‰ˆçš„ä¼˜ç§€ç‰¹æ€§
    4. è‡ªé€‚åº”KLæ•£åº¦æ§åˆ¶
    """
    
    def __init__(self, env: RacetrackEnv, lr: float = 3e-4, gamma: float = 0.99,
                 gae_lambda: float = 0.95, clip_ratio: float = 0.2, 
                 ppo_epochs: int = 4, batch_size: int = 64,
                 buffer_size: int = 2048, hidden_dim: int = 128):
        """
        åˆå§‹åŒ–PPOæ™ºèƒ½ä½“
        
        Args:
            env: èµ›è½¦è½¨é“ç¯å¢ƒ
            lr: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            gae_lambda: GAEçš„Î»å‚æ•°
            clip_ratio: PPOè£å‰ªæ¯”ç‡
            ppo_epochs: PPOæ›´æ–°è½®æ•°
            batch_size: æ‰¹é‡å¤§å°
            buffer_size: ç¼“å†²åŒºå¤§å°
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        self.env = env
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.buffer_size = buffer_size
        
        # çŠ¶æ€ç‰¹å¾ç»´åº¦ï¼šç»§æ‰¿ä¼˜åŒ–ç‰ˆActor-Criticçš„8ç»´ç‰¹å¾
        self.state_dim = 8
        self.action_dim = env.n_actions
        
        # åˆ›å»ºç½‘ç»œ
        self.network = PPONetwork(self.state_dim, self.action_dim, hidden_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr, eps=1e-5)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = PPOBuffer(buffer_size)
        
        # æ¢ç´¢å‚æ•°ï¼ˆPPOé€šå¸¸ä¸éœ€è¦é¢å¤–çš„æ¢ç´¢æœºåˆ¶ï¼‰
        self.exploration_noise = 0.0
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        self.success_rate: List[float] = []
        self.policy_losses: List[float] = []
        self.value_losses: List[float] = []
        self.entropy_losses: List[float] = []
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤ï¼ˆç»§æ‰¿ä¼˜åŒ–ç‰ˆç‰¹æ€§ï¼‰
        self.best_success_rate = 0.0
        self.best_model_state = None
        self.patience = 0
        self.max_patience = 100
        
        # è‡ªé€‚åº”å‚æ•°
        self.target_kl = 0.01  # ç›®æ ‡KLæ•£åº¦
        self.lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.8, patience=50, verbose=True
        )
    
    def state_to_tensor(self, state: Tuple[int, int, int, int]) -> torch.Tensor:
        """
        å°†ç¯å¢ƒçŠ¶æ€è½¬æ¢ä¸ºç¥ç»ç½‘ç»œè¾“å…¥å¼ é‡
        ç»§æ‰¿ä¼˜åŒ–ç‰ˆActor-Criticçš„8ç»´ç‰¹å¾è®¾è®¡
        """
        x, y, vx, vy = state
        
        # 1. åŸºç¡€ç‰¹å¾å½’ä¸€åŒ–
        norm_x = x / 31.0
        norm_y = y / 16.0  
        norm_vx = vx / self.env.max_speed
        norm_vy = vy / self.env.max_speed
        
        # 2. è®¡ç®—åˆ°æœ€è¿‘ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_distance = float('inf')
        goal_direction_x, goal_direction_y = 0, 0
        
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            if distance < min_distance:
                min_distance = distance
                if distance > 0:
                    goal_direction_x = -(goal_x - x) / distance
                    goal_direction_y = (goal_y - y) / distance
        
        # 3. è·ç¦»å½’ä¸€åŒ–
        max_distance = np.sqrt(31**2 + 16**2)
        norm_distance = min_distance / max_distance
        
        # 4. é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        velocity_alignment = 0.0
        if min_distance > 0:
            velocity_mag = np.sqrt(vx**2 + vy**2)
            if velocity_mag > 0:
                vel_dir_x = vx / velocity_mag
                vel_dir_y = vy / velocity_mag
                velocity_alignment = max(0, vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y)
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_distance, goal_direction_x, goal_direction_y, 
            velocity_alignment
        ], dtype=torch.float32)
    
    def apply_action_mask(self, state: Tuple[int, int, int, int], 
                         action_logits: torch.Tensor) -> torch.Tensor:
        """
        åº”ç”¨åŠ¨ä½œæ©ç ï¼Œç»§æ‰¿ä¼˜åŒ–ç‰ˆActor-Criticçš„ä¸¥æ ¼æ©ç ç­–ç•¥
        """
        x, y, vx, vy = state
        mask = torch.zeros_like(action_logits)
        
        for i, (ax, ay) in enumerate(self.env.actions):
            new_vx = max(0, min(self.env.max_speed, vx + ax))
            new_vy = max(0, min(self.env.max_speed, vy + ay))
            
            if new_vx == 0 and new_vy == 0 and (x, y) not in self.env.start_positions:
                new_vx = 1
                new_vy = 1
            
            new_x = x - new_vx
            new_y = y + new_vy
            
            if self.env._check_collision(x, y, new_x, new_y):
                mask[i] = -1e8  # ä½¿ç”¨å¤§è´Ÿæ•°è€Œä¸æ˜¯infé¿å…NaN
        
        masked_logits = action_logits + mask
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆåŠ¨ä½œ
        if torch.all(mask == -1e8):
            # å¦‚æœæ‰€æœ‰åŠ¨ä½œéƒ½è¢«ç¦æ­¢ï¼Œé‡ç½®æ©ç 
            mask.fill_(0)
            masked_logits = action_logits
        
        return masked_logits
    
    def select_action(self, state: Tuple[int, int, int, int], training: bool = True):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆPPOç‰ˆæœ¬ï¼‰
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        state_tensor = self.state_to_tensor(state)
        
        with torch.no_grad() if not training else torch.enable_grad():
            action_logits, value = self.network(state_tensor)
            
            # åº”ç”¨åŠ¨ä½œæ©ç 
            masked_logits = self.apply_action_mask(state, action_logits)
            
            # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒå¹¶é‡‡æ ·
            action_dist = Categorical(logits=masked_logits)
            action = action_dist.sample()
            log_prob = action_dist.log_prob(action)
        
        return action.item(), log_prob, value.squeeze()
    
    def improved_reward_shaping(self, state, next_state, reward, done, steps):
        """
        ç»§æ‰¿ä¼˜åŒ–ç‰ˆActor-Criticçš„å¥–åŠ±å¡‘å½¢ç­–ç•¥
        """
        bonus = 0.0
        
        # æˆåŠŸ/å¤±è´¥çš„æ˜ç¡®å¥–åŠ±
        if done and reward > 0:
            bonus += 100
        elif reward == -10:  # ç¢°æ’
            bonus -= 50
        
        # ç®€å•çš„è¿›æ­¥å¥–åŠ±
        x, y, _, _ = state
        next_x, next_y, _, _ = next_state
        
        # è®¡ç®—åˆ°æœ€è¿‘ç›®æ ‡çš„æ›¼å“ˆé¡¿è·ç¦»
        curr_dist = min([abs(x - gx) + abs(y - gy) for gx, gy in self.env.goal_positions])
        next_dist = min([abs(next_x - gx) + abs(next_y - gy) for gx, gy in self.env.goal_positions])
        
        if curr_dist - next_dist > 1:
            bonus += 2.0
        
        # è½»å¾®çš„æ­¥æ•°æƒ©ç½š
        bonus -= 0.1
        
        return reward + bonus
    
    def collect_trajectory(self, max_steps: int = 200) -> Tuple[float, int, bool]:
        """
        æ”¶é›†ä¸€ä¸ªå®Œæ•´çš„è½¨è¿¹
        
        Returns:
            total_reward: æ€»å¥–åŠ± 
            steps: æ­¥æ•°
            success: æ˜¯å¦æˆåŠŸ
        """
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        episode_buffer = []
        
        for _ in range(max_steps):
            action, log_prob, value = self.select_action(state, training=True)
            prev_state = state
            
            next_state, reward, done = self.env.step(action)
            
            # å¥–åŠ±å¡‘å½¢
            shaped_reward = self.improved_reward_shaping(prev_state, next_state, reward, done, steps)
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.add(
                self.state_to_tensor(prev_state),
                action,
                shaped_reward,
                value,
                log_prob,
                done
            )
            
            total_reward += reward  # ä½¿ç”¨åŸå§‹å¥–åŠ±è®¡ç®—æ€»å›æŠ¥
            steps += 1
            
            if done:
                break
                
            state = next_state
        
        # å¦‚æœepisodeæœªç»“æŸï¼Œè®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼
        if not done:
            _, _, next_value = self.select_action(state, training=True)
        else:
            next_value = 0.0
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        self.buffer.compute_advantages_and_returns(self.gamma, self.gae_lambda, next_value)
        
        # åˆ¤æ–­æˆåŠŸ
        success = (done and reward == 100)
        return total_reward, steps, success
    
    def update_policy(self):
        """
        PPOç­–ç•¥æ›´æ–°
        """
        if self.buffer.size() < self.batch_size:
            return
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = torch.tensor(self.buffer.advantages, dtype=torch.float32)
        if advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        advantages = advantages.detach()  # ç¡®ä¿æ²¡æœ‰æ¢¯åº¦ä¾èµ–
        
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        total_kl_div = 0
        
        # PPOå¤šè½®æ›´æ–°
        for epoch in range(self.ppo_epochs):
            # è·å–æ‰¹é‡æ•°æ®
            batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns = \
                self.buffer.get_batch(min(self.batch_size, self.buffer.size()))
            
            # é‡æ–°è®¡ç®—åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼
            action_logits, values = self.network(batch_states)
            
            # æ‰¹é‡æ›´æ–°æ—¶ç®€åŒ–å¤„ç†ï¼Œä¸åº”ç”¨æ©ç é¿å…å¤æ‚æ€§
            # å› ä¸ºè®­ç»ƒæ•°æ®ä¸­çš„åŠ¨ä½œå·²ç»æ˜¯ç»è¿‡æ©ç é€‰æ‹©çš„æœ‰æ•ˆåŠ¨ä½œ
            
            action_dist = Categorical(logits=action_logits)
            log_probs = action_dist.log_prob(batch_actions)
            entropy = action_dist.entropy()
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            ratio = torch.exp(log_probs - batch_old_log_probs)
            
            # PPO Clipped Surrogate Objective
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼å‡½æ•°æŸå¤±
            value_loss = F.mse_loss(values.squeeze(), batch_returns)
            
            # ç†µæŸå¤±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
            entropy_loss = entropy.mean()
            
            # æ€»æŸå¤±
            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            total_entropy_loss += entropy_loss.item()
            
            # è®¡ç®—KLæ•£åº¦ï¼ˆç”¨äºè‡ªé€‚åº”è°ƒæ•´ï¼‰
            with torch.no_grad():
                kl_div = (batch_old_log_probs - log_probs).mean()
                total_kl_div += kl_div.item()
                
                # å¦‚æœKLæ•£åº¦è¿‡å¤§ï¼Œæå‰åœæ­¢æ›´æ–°
                if kl_div > 1.5 * self.target_kl:
                    break
        
        # è®°å½•å¹³å‡æŸå¤±
        self.policy_losses.append(total_policy_loss / (epoch + 1))
        self.value_losses.append(total_value_loss / (epoch + 1))
        self.entropy_losses.append(total_entropy_loss / (epoch + 1))
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
    
    def train_episode(self, episode_num: int) -> Tuple[float, int, bool]:
        """
        è®­ç»ƒå•ä¸ªepisode
        """
        # æ”¶é›†è½¨è¿¹
        reward, steps, success = self.collect_trajectory()
        
        # æ›´æ–°ç­–ç•¥
        self.update_policy()
        
        return reward, steps, success
    
    def test_episode(self, render: bool = False) -> Tuple[float, int, List, bool]:
        """
        æµ‹è¯•å•ä¸ªepisode
        """
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]
        max_steps = 300
        
        self.network.eval()
        with torch.no_grad():
            while steps < max_steps:
                action, _, _ = self.select_action(state, training=False)
                next_state, reward, done = self.env.step(action)
                total_reward += reward
                steps += 1
                path.append(next_state[:2])
                
                if done:
                    break
                
                state = next_state
        
        self.network.train()
        success = (done and reward == 100)
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'network': self.network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_rate': self.success_rate,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'entropy_losses': self.entropy_losses
        }
        torch.save(save_dict, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
        self.network.load_state_dict(checkpoint['network'])
        self.network.eval()


def main_ppo_training():
    """
    PPOä¸»è®­ç»ƒå‡½æ•° - ç»“åˆåˆ†é˜¶æ®µè®­ç»ƒå’Œæœ€ä½³æ¨¡å‹ä¿æŠ¤
    """
    print("=== PPOèµ›è½¦è½¨é“è®­ç»ƒ ===")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºPPOæ™ºèƒ½ä½“
    agent = PPORacetrackAgent(
        env=env,
        lr=3e-4,           # PPOæ¨èå­¦ä¹ ç‡
        gamma=0.99,        # æŠ˜æ‰£å› å­
        gae_lambda=0.95,   # GAEå‚æ•°
        clip_ratio=0.2,    # PPOè£å‰ªæ¯”ç‡
        ppo_epochs=4,      # PPOæ›´æ–°è½®æ•°
        batch_size=64,     # æ‰¹é‡å¤§å°
        buffer_size=2048,  # ç¼“å†²åŒºå¤§å°
        hidden_dim=128     # éšè—å±‚ç»´åº¦
    )
    
    print(f"PPOé…ç½®:")
    print(f"  - å­¦ä¹ ç‡: 3e-4")
    print(f"  - è£å‰ªæ¯”ç‡: 0.2")
    print(f"  - PPOæ›´æ–°è½®æ•°: 4")
    print(f"  - æ‰¹é‡å¤§å°: 64")
    print(f"  - ç¼“å†²åŒºå¤§å°: 2048")
    
    # è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
    print("\n=== è®­ç»ƒå‰åŸºå‡† ===")
    reward_before, steps_before, _, success_before = agent.test_episode()
    print(f"åŸºå‡†æ€§èƒ½: å¥–åŠ±={reward_before:.1f}, æ­¥æ•°={steps_before}, æˆåŠŸ={success_before}")
    
    # åˆ†é˜¶æ®µè®­ç»ƒ (ç¼©çŸ­ä¸ºæ¼”ç¤º)
    n_episodes = 500
    print(f"\n=== å¼€å§‹PPOè®­ç»ƒ ===")
    print(f"è®­ç»ƒè®¡åˆ’: {n_episodes}å›åˆ")
    
    # è®­ç»ƒç»Ÿè®¡
    success_window = deque(maxlen=100)
    reward_window = deque(maxlen=50)
    
    for episode in range(n_episodes):
        # è®­ç»ƒä¸€ä¸ªepisode
        reward, steps, success = agent.train_episode(episode)
        
        agent.episode_rewards.append(reward)
        agent.episode_steps.append(steps)
        success_window.append(1 if success else 0)
        reward_window.append(reward)
        
        current_success_rate = np.mean(success_window)
        agent.success_rate.append(current_success_rate)
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤æœºåˆ¶
        if episode >= 50 and current_success_rate > agent.best_success_rate:
            agent.best_success_rate = current_success_rate
            agent.best_model_state = {
                'network': agent.network.state_dict().copy(),
                'episode': episode,
                'success_rate': current_success_rate
            }
            agent.patience = 0
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: Episode {episode+1}, æˆåŠŸç‡={current_success_rate:.3f}")
        else:
            agent.patience += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if episode % 100 == 0 and episode > 0:
            agent.lr_scheduler.step(current_success_rate)
        
        # æ€§èƒ½é€€åŒ–æ£€æµ‹ä¸æ¢å¤
        if agent.patience > agent.max_patience and agent.best_model_state:
            print(f"\nâš ï¸ æ£€æµ‹åˆ°æ€§èƒ½é€€åŒ–ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹...")
            agent.network.load_state_dict(agent.best_model_state['network'])
            print(f"   å·²æ¢å¤Episode {agent.best_model_state['episode']+1}çš„æ¨¡å‹")
            agent.patience = 0
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(reward_window)
            avg_steps = np.mean(agent.episode_steps[-25:])
            avg_policy_loss = np.mean(agent.policy_losses[-25:]) if agent.policy_losses else 0
            avg_value_loss = np.mean(agent.value_losses[-25:]) if agent.value_losses else 0
            
            print(f"Episode {episode + 1:4d}: "
                  f"å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={avg_steps:5.1f}, "
                  f"æˆåŠŸç‡={current_success_rate:.3f}")
            print(f"                     ç­–ç•¥æŸå¤±={avg_policy_loss:.4f}, "
                  f"ä»·å€¼æŸå¤±={avg_value_loss:.4f}, "
                  f"æœ€ä½³æˆåŠŸç‡={agent.best_success_rate:.3f}")
    
    # æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    if agent.best_model_state:
        print(f"\nğŸ”„ æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        agent.network.load_state_dict(agent.best_model_state['network'])
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\n=== æœ€ç»ˆè¯„ä¼° ===")
    test_results = []
    for i in range(50):
        reward, steps, path, success = agent.test_episode()
        test_results.append((reward, steps, success))
    
    final_success_rate = np.mean([r[2] for r in test_results])
    final_avg_reward = np.mean([r[0] for r in test_results])
    final_avg_steps = np.mean([r[1] for r in test_results])
    
    print(f"æœ€ç»ˆæµ‹è¯•ç»“æœï¼ˆ50æ¬¡ï¼‰:")
    print(f"  æˆåŠŸç‡: {final_success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {final_avg_reward:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {final_avg_steps:.1f}")
    
    # æ€§èƒ½è¯„ä»·
    if final_success_rate > 0.7:
        print("ğŸ‰ PPOè¡¨ç°ä¼˜ç§€ï¼æˆåŠŸç‡è¶…è¿‡70%")
    elif final_success_rate > 0.5:
        print("âœ… PPOè¡¨ç°è‰¯å¥½ï¼æˆåŠŸç‡è¶…è¿‡50%")
    elif final_success_rate > 0.3:
        print("ğŸ“ˆ PPOè¡¨ç°å°šå¯ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
    else:
        print("âš ï¸ PPOéœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
    
    # ä¿å­˜æ¨¡å‹
    agent.save_model("models/ppo_racetrack_model.pth")
    print(f"PPOæ¨¡å‹å·²ä¿å­˜åˆ° models/ æ–‡ä»¶å¤¹")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(agent)
    
    return agent, test_results


def plot_training_curves(agent):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æˆåŠŸç‡æ›²çº¿
    if agent.success_rate:
        axes[0, 0].plot(agent.success_rate)
        axes[0, 0].set_title('Success Rate')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Success Rate')
        axes[0, 0].grid(True)
    
    # å¥–åŠ±æ›²çº¿
    if agent.episode_rewards:
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window_size = 50
        if len(agent.episode_rewards) > window_size:
            moving_avg = np.convolve(agent.episode_rewards, 
                                   np.ones(window_size)/window_size, mode='valid')
            axes[0, 1].plot(moving_avg)
            axes[0, 1].set_title(f'Episode Rewards (Moving Average, window={window_size})')
        else:
            axes[0, 1].plot(agent.episode_rewards)
            axes[0, 1].set_title('Episode Rewards')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Reward')
        axes[0, 1].grid(True)
    
    # ç­–ç•¥æŸå¤±
    if agent.policy_losses:
        axes[1, 0].plot(agent.policy_losses)
        axes[1, 0].set_title('Policy Loss')
        axes[1, 0].set_xlabel('Update')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].grid(True)
    
    # ä»·å€¼æŸå¤±
    if agent.value_losses:
        axes[1, 1].plot(agent.value_losses)
        axes[1, 1].set_title('Value Loss')
        axes[1, 1].set_xlabel('Update')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('ppo_training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()


if __name__ == "__main__":
    # è¿è¡ŒPPOè®­ç»ƒ
    main_ppo_training() 
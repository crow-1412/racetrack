"""
ç¨³å®šç‰ˆ Gumbel-Softmax PPO - èµ›è½¦è½¨é“é—®é¢˜
ä¿®å¤è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œä¼˜åŒ–æ¸©åº¦è°ƒåº¦å’Œæ¢ç´¢ç­–ç•¥

å…³é”®æ”¹è¿›ï¼š
1. è‡ªé€‚åº”æ¸©åº¦è°ƒåº¦
2. æˆåŠŸç‡åé¦ˆæœºåˆ¶
3. æ›´ç¨³å®šçš„å¥–åŠ±å¡‘å½¢
4. æ—©åœå’Œæ¨¡å‹ä¿æŠ¤
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Tuple, List, Dict
import random
from collections import deque
from racetrack_env import RacetrackEnv

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

print(f"ğŸ”§ ç¨³å®šç‰ˆGumbel-Softmax PPOéšæœºç§å­å·²è®¾ç½®ä¸º: {RANDOM_SEED}")

class StableGumbelPPONetwork(nn.Module):
    """ç¨³å®šç‰ˆGumbel-Softmax PPOç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(StableGumbelPPONetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.shared_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),  # æ·»åŠ dropoutæé«˜ç¨³å®šæ€§
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # Actorå¤´éƒ¨
        self.actor_head = nn.Linear(hidden_dim // 2, action_dim)
        
        # Criticå¤´éƒ¨
        self.critic_head = nn.Linear(hidden_dim // 2, 1)
        
        # åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=1.0)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        shared_features = self.shared_layers(state)
        action_logits = self.actor_head(shared_features)
        value = self.critic_head(shared_features)
        return action_logits, value


# ç»§æ‰¿ä¹‹å‰çš„Bufferç±»
class StableGumbelPPOBuffer:
    """ç¨³å®šç‰ˆç»éªŒç¼“å†²åŒº"""
    def __init__(self, max_size: int):
        self.max_size = max_size
        self.clear()
    
    def clear(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.advantages = []
        self.returns = []
    
    def add(self, state, action, reward, value, log_prob, done):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)
    
    def size(self):
        return len(self.states)
    
    def compute_advantages_and_returns(self, gamma: float, gae_lambda: float, next_value: float = 0):
        """ç¨³å®šçš„GAEè®¡ç®—"""
        rewards = np.array(self.rewards, dtype=np.float32)
        values = np.array([v.detach().item() if isinstance(v, torch.Tensor) else v for v in self.values], dtype=np.float32)
        dones = np.array(self.dones, dtype=np.bool_)
        
        # å¥–åŠ±è£å‰ªæé«˜ç¨³å®šæ€§
        rewards = np.clip(rewards, -100, 200)
        
        # GAEè®¡ç®—
        advantages = np.zeros_like(rewards, dtype=np.float32)
        last_gae = 0.0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - float(dones[t])
                next_value_t = next_value
            else:
                next_non_terminal = 1.0 - float(dones[t])
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * next_non_terminal - values[t]
            advantages[t] = last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae
        
        returns = advantages + values
        
        self.advantages = advantages.tolist()
        self.returns = returns.tolist()
    
    def get_batch(self, batch_size: int):
        """è·å–æ‰¹é‡æ•°æ®"""
        indices = np.random.choice(self.size(), min(batch_size, self.size()), replace=False)
        
        batch_states = torch.stack([self.states[i] for i in indices])
        batch_actions = torch.tensor([self.actions[i] for i in indices], dtype=torch.long)
        batch_old_log_probs = torch.stack([self.log_probs[i].detach() for i in indices])
        batch_advantages = torch.tensor([self.advantages[i] for i in indices], dtype=torch.float32)
        batch_returns = torch.tensor([self.returns[i] for i in indices], dtype=torch.float32)
        
        return batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns


class StableGumbelPPORacetrackAgent:
    """
    ç¨³å®šç‰ˆGumbel-Softmax PPOæ™ºèƒ½ä½“
    è§£å†³è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½é€€åŒ–é—®é¢˜
    """
    
    def __init__(self, env: RacetrackEnv, gamma: float = 0.99,
                 gae_lambda: float = 0.95, clip_ratio: float = 0.2,
                 ppo_epochs: int = 4, batch_size: int = 128,
                 buffer_size: int = 1024, hidden_dim: int = 128):
        
        self.env = env
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.buffer_size = buffer_size
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
        self.state_dim = 8
        self.action_dim = env.n_actions
        
        # åˆ›å»ºç¨³å®šç‰ˆç½‘ç»œ
        self.network = StableGumbelPPONetwork(self.state_dim, self.action_dim, hidden_dim)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.network.parameters(), lr=3e-4, eps=1e-5)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = StableGumbelPPOBuffer(buffer_size)
        
        # ğŸ”§ æ”¹è¿›çš„æ¸©åº¦è°ƒåº¦
        self.temperature = 1.5      # é™ä½åˆå§‹æ¸©åº¦
        self.min_temperature = 0.8  # æé«˜æœ€å°æ¸©åº¦
        self.temperature_decay = 0.998  # æ›´æ…¢çš„è¡°å‡
        
        # ğŸ”§ è‡ªé€‚åº”æ¸©åº¦è°ƒæ•´
        self.success_rate_history = deque(maxlen=20)
        self.last_success_rate = 0.0
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        self.success_rate: List[float] = []
        self.policy_losses: List[float] = []
        self.value_losses: List[float] = []
        
        # ğŸ”§ æœ€ä½³æ¨¡å‹ä¿æŠ¤ - åŠ å¼ºç‰ˆ
        self.best_success_rate = 0.0
        self.best_model_state = None
        self.patience = 0
        self.max_patience = 30  # å‡å°‘è€å¿ƒå€¼ï¼Œæ›´æ—©å¹²é¢„
        
        # ğŸ”§ æ–°å¢ï¼šæ€§èƒ½æ€¥å‰§ä¸‹é™æ£€æµ‹
        self.performance_drop_threshold = 0.15  # æˆåŠŸç‡ä¸‹é™15%è§¦å‘å›æº¯
        self.recent_success_rates = deque(maxlen=10)  # æœ€è¿‘10ä¸ªepisodeçš„æˆåŠŸç‡
        
        # ğŸ”§ æ–°å¢ï¼šåŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
        self.initial_lr = 3e-4
        self.lr_decay_factor = 0.8
        self.lr_recovery_factor = 1.2
        
        # ğŸ”§ æ–°å¢ï¼šè¿ç»­æ€§èƒ½ä¸‹é™æ£€æµ‹
        self.consecutive_rollbacks = 0  # è¿ç»­å›æº¯æ¬¡æ•°
        self.max_consecutive_rollbacks = 3  # æœ€å¤§è¿ç»­å›æº¯æ¬¡æ•°
        self.early_stop_triggered = False  # æå‰åœæ­¢æ ‡å¿—
        
        # å¥–åŠ±å¡‘å½¢å‚æ•°
        self.last_distance_to_goal = None
        
        print(f"ğŸ”§ ç¨³å®šç‰ˆå‚æ•°: æ¸©åº¦={self.temperature}, æœ€å°={self.min_temperature}, è¡°å‡={self.temperature_decay}")
    
    def state_to_tensor(self, state: Tuple[int, int, int, int]) -> torch.Tensor:
        """çŠ¶æ€è½¬æ¢ä¸ºå¼ é‡"""
        x, y, vx, vy = state
        
        # åŸºç¡€ç‰¹å¾å½’ä¸€åŒ–
        norm_x = x / 31.0
        norm_y = y / 16.0  
        norm_vx = vx / self.env.max_speed
        norm_vy = vy / self.env.max_speed
        
        # è®¡ç®—åˆ°æœ€è¿‘ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_distance = float('inf')
        goal_direction_x, goal_direction_y = 0, 0
        
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            if distance < min_distance:
                min_distance = distance
                if distance > 0:
                    goal_direction_x = -(goal_x - x) / distance
                    goal_direction_y = (goal_y - y) / distance
        
        # è·ç¦»å½’ä¸€åŒ–
        max_distance = np.sqrt(31**2 + 16**2)
        norm_distance = min_distance / max_distance
        
        # é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        velocity_alignment = 0.0
        if min_distance > 0:
            velocity_mag = np.sqrt(vx**2 + vy**2)
            if velocity_mag > 0:
                vel_dir_x = vx / velocity_mag
                vel_dir_y = vy / velocity_mag
                velocity_alignment = max(0, vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y)
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_distance, goal_direction_x, goal_direction_y, 
            velocity_alignment
        ], dtype=torch.float32)
    
    def gumbel_softmax_sample(self, logits: torch.Tensor, temperature: float = 1.0, hard: bool = False):
        """ç¨³å®šç‰ˆGumbel-Softmaxé‡‡æ ·"""
        # æ•°å€¼ç¨³å®šæ€§æ”¹è¿›
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits).clamp(1e-8, 1.0)))
        gumbel_logits = (logits + gumbel_noise) / max(temperature, 0.1)  # é˜²æ­¢æ¸©åº¦è¿‡å°
        
        soft_action = F.softmax(gumbel_logits, dim=-1)
        
        if hard:
            discrete_action = torch.argmax(soft_action, dim=-1)
            hard_action = F.one_hot(discrete_action, self.action_dim).float()
            # ç›´é€šä¼°è®¡å™¨
            soft_action = hard_action.detach() + soft_action - soft_action.detach()
        
        return soft_action
    
    def apply_action_mask(self, state: Tuple[int, int, int, int], 
                         action_logits: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨åŠ¨ä½œæ©ç """
        x, y, vx, vy = state
        mask = torch.zeros_like(action_logits)
        
        for i, (ax, ay) in enumerate(self.env.actions):
            new_vx = max(0, min(self.env.max_speed, vx + ax))
            new_vy = max(0, min(self.env.max_speed, vy + ay))
            
            if new_vx == 0 and new_vy == 0 and (x, y) not in self.env.start_positions:
                new_vx = 1
                new_vy = 1
            
            new_x = x - new_vx
            new_y = y + new_vy
            
            if self.env._check_collision(x, y, new_x, new_y):
                mask[i] = -1e9
        
        masked_logits = action_logits + mask
        
        if torch.all(mask == -1e9):
            mask.fill_(0)
            masked_logits = action_logits
        
        return masked_logits
    
    def select_action(self, state: Tuple[int, int, int, int], training: bool = True):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = self.state_to_tensor(state)
        
        with torch.no_grad() if not training else torch.enable_grad():
            action_logits, value = self.network(state_tensor)
            
            # åº”ç”¨åŠ¨ä½œæ©ç 
            masked_logits = self.apply_action_mask(state, action_logits)
            
            if training:
                # è®­ç»ƒæ—¶ï¼šä½¿ç”¨Gumbel-Softmaxé‡‡æ ·
                soft_action = self.gumbel_softmax_sample(masked_logits, self.temperature, hard=True)
                discrete_action = torch.argmax(soft_action)
                
                # è®¡ç®—logæ¦‚ç‡
                action_dist = Categorical(logits=masked_logits)
                log_prob = action_dist.log_prob(discrete_action)
            else:
                # æµ‹è¯•æ—¶ï¼šè´ªå©ªç­–ç•¥
                discrete_action = torch.argmax(masked_logits)
                
                action_dist = Categorical(logits=masked_logits)
                log_prob = action_dist.log_prob(discrete_action)
        
        return discrete_action.item(), log_prob, value.squeeze()
    
    def reward_shaping(self, prev_state, state, next_state, reward, done, steps):
        """ç¨³å®šçš„å¥–åŠ±å¡‘å½¢"""
        shaped_reward = reward
        
        x, y, vx, vy = state
        
        # å‰è¿›å¥–åŠ±
        current_distance = min([np.sqrt((x - gx)**2 + (y - gy)**2) 
                               for gx, gy in self.env.goal_positions])
        
        if self.last_distance_to_goal is not None:
            progress = self.last_distance_to_goal - current_distance
            if progress > 0:
                shaped_reward += progress * 0.1  # é€‚ä¸­çš„å‰è¿›å¥–åŠ±
        
        self.last_distance_to_goal = current_distance
        
        # é€‚ä¸­çš„æ­¥æ•°æƒ©ç½š
        shaped_reward -= 0.005
        
        # æˆåŠŸå¥–åŠ±
        if done and reward == 100:
            shaped_reward += 30  # é€‚ä¸­çš„æˆåŠŸå¥–åŠ±
        elif done and reward == -10:
            shaped_reward -= 10
        
        return shaped_reward
    
    def adaptive_temperature_update(self, current_success_rate: float):
        """ğŸ”§ è‡ªé€‚åº”æ¸©åº¦è°ƒæ•´ - åŠ å¼ºç‰ˆ"""
        self.success_rate_history.append(current_success_rate)
        
        if len(self.success_rate_history) >= 10:
            # è®¡ç®—æˆåŠŸç‡è¶‹åŠ¿
            recent_rate = np.mean(list(self.success_rate_history)[-5:])
            older_rate = np.mean(list(self.success_rate_history)[-10:-5])
            
            # å¦‚æœæˆåŠŸç‡ä¸‹é™ï¼Œå¢åŠ æ¸©åº¦ï¼ˆå¢åŠ æ¢ç´¢ï¼‰
            if recent_rate < older_rate - 0.05:
                self.temperature = min(2.0, self.temperature * 1.05)  # æ›´ç§¯æçš„æ¸©åº¦å¢åŠ 
                print(f"    ğŸ”¥ æˆåŠŸç‡ä¸‹é™ï¼Œå¢åŠ æ¢ç´¢: T={self.temperature:.3f}")
            # å¦‚æœæˆåŠŸç‡ç¨³å®šæå‡ï¼Œé€æ¸é™ä½æ¸©åº¦
            elif recent_rate > older_rate + 0.02:
                self.temperature = max(self.min_temperature, self.temperature * self.temperature_decay)
    
    def detect_performance_drop(self, current_success_rate: float) -> bool:
        """ğŸ”§ æ–°å¢ï¼šæ£€æµ‹æ€§èƒ½æ€¥å‰§ä¸‹é™"""
        self.recent_success_rates.append(current_success_rate)
        
        if len(self.recent_success_rates) >= 5:
            # è®¡ç®—æœ€è¿‘5ä¸ªepisodeçš„å¹³å‡æˆåŠŸç‡
            recent_avg = np.mean(list(self.recent_success_rates)[-5:])
            # ä¸æœ€ä½³æˆåŠŸç‡æ¯”è¾ƒ
            if self.best_success_rate > 0 and (self.best_success_rate - recent_avg) > self.performance_drop_threshold:
                return True
        return False
    
    def adjust_learning_rate(self, factor: float):
        """ğŸ”§ æ–°å¢ï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡"""
        current_lr = self.optimizer.param_groups[0]['lr']
        new_lr = current_lr * factor
        # é™åˆ¶å­¦ä¹ ç‡èŒƒå›´
        new_lr = max(1e-5, min(1e-3, new_lr))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr
        
        print(f"    ğŸ“ˆ å­¦ä¹ ç‡è°ƒæ•´: {current_lr:.2e} -> {new_lr:.2e}")
        return new_lr
    
    def collect_trajectory(self, max_steps: int = 300) -> Tuple[float, int, bool]:
        """æ”¶é›†è½¨è¿¹"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        self.last_distance_to_goal = None
        
        for _ in range(max_steps):
            action, log_prob, value = self.select_action(state, training=True)
            prev_state = state
            
            next_state, reward, done = self.env.step(action)
            
            # å¥–åŠ±å¡‘å½¢
            shaped_reward = self.reward_shaping(prev_state, state, next_state, reward, done, steps)
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.add(
                self.state_to_tensor(prev_state),
                action,
                shaped_reward,
                value,
                log_prob,
                done
            )
            
            total_reward += reward
            steps += 1
            
            if done:
                break
                
            state = next_state
        
        # è®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼
        if not done:
            _, _, next_value = self.select_action(state, training=True)
        else:
            next_value = 0.0
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        self.buffer.compute_advantages_and_returns(self.gamma, self.gae_lambda, next_value)
        
        success = (done and reward == 100)
        return total_reward, steps, success
    
    def update_policy(self):
        """ç¨³å®šçš„ç­–ç•¥æ›´æ–°"""
        if self.buffer.size() < self.batch_size:
            return
        
        # ä¼˜åŠ¿å½’ä¸€åŒ–
        advantages = torch.tensor(self.buffer.advantages, dtype=torch.float32)
        if advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        total_policy_loss = 0
        total_value_loss = 0
        update_count = 0
        
        # PPOæ›´æ–°
        for epoch in range(self.ppo_epochs):
            batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns = \
                self.buffer.get_batch(self.batch_size)
            
            batch_advantages = advantages[:len(batch_advantages)]
            
            # å‰å‘ä¼ æ’­
            action_logits, values = self.network(batch_states)
            
            # é‡æ–°è®¡ç®—åŠ¨ä½œæ¦‚ç‡
            action_dist = Categorical(logits=action_logits)
            log_probs = action_dist.log_prob(batch_actions)
            
            # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            ratio = torch.exp(log_probs - batch_old_log_probs.detach())
            
            # PPOæŸå¤±
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼æŸå¤±
            batch_returns_tensor = torch.tensor(self.buffer.returns[:len(batch_returns)], dtype=torch.float32)
            value_loss = F.mse_loss(values.squeeze(), batch_returns_tensor)
            
            # ç†µå¥–åŠ±
            entropy = action_dist.entropy().mean()
            
            # æ€»æŸå¤±
            total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
            
            # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print("âš ï¸ æ£€æµ‹åˆ°æ•°å€¼ä¸ç¨³å®šï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
                continue
            
            # æ›´æ–°
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
            self.optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            update_count += 1
        
        # è®°å½•æŸå¤±
        if update_count > 0:
            self.policy_losses.append(total_policy_loss / update_count)
            self.value_losses.append(total_value_loss / update_count)
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
    
    def train_episode(self, episode_num: int) -> Tuple[float, int, bool]:
        """è®­ç»ƒå•ä¸ªepisode"""
        reward, steps, success = self.collect_trajectory()
        self.update_policy()
        return reward, steps, success
    
    def test_episode(self, render: bool = False, debug: bool = False) -> Tuple[float, int, List, bool]:
        """æµ‹è¯•å•ä¸ªepisode - æ·»åŠ è°ƒè¯•åŠŸèƒ½"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]
        max_steps = 300
        
        # ğŸ”§ è°ƒè¯•ï¼šé‡ç½®å¥–åŠ±å¡‘å½¢çŠ¶æ€ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        self.last_distance_to_goal = None
        
        self.network.eval()
        with torch.no_grad():
            while steps < max_steps:
                if debug and steps < 10:
                    # æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ—¶çš„åŠ¨ä½œé€‰æ‹©
                    action_train, _, _ = self.select_action(state, training=True)
                    action_test, _, _ = self.select_action(state, training=False)
                    print(f"Step {steps}: è®­ç»ƒåŠ¨ä½œ={action_train}, æµ‹è¯•åŠ¨ä½œ={action_test}, ç›¸åŒ={action_train==action_test}")
                
                action, _, _ = self.select_action(state, training=False)
                next_state, reward, done = self.env.step(action)
                total_reward += reward
                steps += 1
                path.append(next_state[:2])
                
                if debug and (reward != -1 or steps % 50 == 0):
                    print(f"Step {steps}: åŠ¨ä½œ={action}, å¥–åŠ±={reward}, ç´¯è®¡å¥–åŠ±={total_reward}")
                
                if done:
                    break
                
                state = next_state
        
        self.network.train()
        success = (done and reward == 100)
        
        if debug:
            print(f"æµ‹è¯•ç»“æœ: æ­¥æ•°={steps}, æˆåŠŸ={success}, æœ€ç»ˆå¥–åŠ±={reward}")
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'network': self.network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_rate': self.success_rate,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'temperature': self.temperature
        }
        torch.save(save_dict, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
        self.network.load_state_dict(checkpoint['network'])


def main_stable_gumbel_ppo_training():
    """ç¨³å®šç‰ˆGumbel-Softmax PPOä¸»è®­ç»ƒå‡½æ•°"""
    print("=== ç¨³å®šç‰ˆ Gumbel-Softmax PPOèµ›è½¦è½¨é“è®­ç»ƒ ===")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºç¨³å®šç‰ˆæ™ºèƒ½ä½“
    agent = StableGumbelPPORacetrackAgent(
        env=env,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,
        ppo_epochs=4,
        batch_size=128,
        buffer_size=1024,
        hidden_dim=128
    )
    
    print(f"ç¨³å®šç‰ˆé…ç½®:")
    print(f"  - è‡ªé€‚åº”æ¸©åº¦è°ƒåº¦")
    print(f"  - æœ€ä½³æ¨¡å‹ä¿æŠ¤")
    print(f"  - æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥")
    print(f"  - æˆåŠŸç‡åé¦ˆæœºåˆ¶")
    
    # è®­ç»ƒè®¾ç½®
    n_episodes = 1000
    
    print(f"\n=== å¼€å§‹ç¨³å®šç‰ˆè®­ç»ƒ ===")
    
    # è®­ç»ƒç»Ÿè®¡
    success_window = deque(maxlen=100)
    reward_window = deque(maxlen=50)
    
    for episode in range(n_episodes):
        reward, steps, success = agent.train_episode(episode)
        
        agent.episode_rewards.append(reward)
        agent.episode_steps.append(steps)
        success_window.append(1 if success else 0)
        reward_window.append(reward)
        
        current_success_rate = np.mean(success_window)
        agent.success_rate.append(current_success_rate)
        
        # ğŸ”§ è‡ªé€‚åº”æ¸©åº¦è°ƒæ•´
        if episode >= 20:
            agent.adaptive_temperature_update(current_success_rate)
        
        # ğŸ”§ æœ€ä½³æ¨¡å‹ä¿æŠ¤ - æ›´é¢‘ç¹ä¿å­˜
        if episode >= 30 and current_success_rate > agent.best_success_rate:
            agent.best_success_rate = current_success_rate
            agent.best_model_state = {
                'network': agent.network.state_dict().copy(),
                'optimizer': agent.optimizer.state_dict().copy(),
                'episode': episode,
                'success_rate': current_success_rate,
                'temperature': agent.temperature
            }
            agent.patience = 0
            agent.consecutive_rollbacks = 0  # ğŸ”§ é‡ç½®è¿ç»­å›æº¯è®¡æ•°å™¨ï¼Œå› ä¸ºæœ‰æ”¹è¿›
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: Episode {episode+1}, æˆåŠŸç‡={current_success_rate:.3f}")
        else:
            agent.patience += 1
        
        # ğŸ”§ æ–°å¢ï¼šæ€§èƒ½æ€¥å‰§ä¸‹é™æ£€æµ‹ï¼ˆç«‹å³å¹²é¢„ï¼‰
        if episode >= 50:
            performance_dropped = agent.detect_performance_drop(current_success_rate)
            if performance_dropped and agent.best_model_state:
                agent.consecutive_rollbacks += 1
                print(f"\nğŸš¨ æ£€æµ‹åˆ°æ€§èƒ½æ€¥å‰§ä¸‹é™! å½“å‰: {current_success_rate:.3f}, æœ€ä½³: {agent.best_success_rate:.3f}")
                print(f"   è¿ç»­å›æº¯æ¬¡æ•°: {agent.consecutive_rollbacks}/{agent.max_consecutive_rollbacks}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰åœæ­¢
                if agent.consecutive_rollbacks >= agent.max_consecutive_rollbacks:
                    print(f"\nğŸ›‘ è¿ç»­{agent.max_consecutive_rollbacks}æ¬¡æ€§èƒ½ä¸‹é™ï¼Œè§¦å‘æå‰åœæ­¢ï¼")
                    print(f"   æ¨¡å‹å¯èƒ½å·²è¾¾åˆ°æ€§èƒ½ç“¶é¢ˆï¼Œå»ºè®®ç»“æŸè®­ç»ƒ")
                    agent.early_stop_triggered = True
                    break
                
                print(f"   ç¬¬{agent.consecutive_rollbacks}æ¬¡å›æº¯ï¼Œæ¢å¤Episode {agent.best_model_state['episode']+1}çš„æœ€ä½³æ¨¡å‹...")
                
                # æ¢å¤æœ€ä½³æ¨¡å‹
                agent.network.load_state_dict(agent.best_model_state['network'])
                agent.optimizer.load_state_dict(agent.best_model_state['optimizer'])
                agent.temperature = agent.best_model_state['temperature']
                
                # è°ƒæ•´å­¦ä¹ ç‡å’Œæ¸©åº¦
                agent.adjust_learning_rate(agent.lr_decay_factor)  # é™ä½å­¦ä¹ ç‡
                agent.temperature = min(2.0, agent.temperature * 1.2)  # å¢åŠ æ¢ç´¢
                
                # é‡ç½®è®¡æ•°å™¨
                agent.patience = 0
                agent.recent_success_rates.clear()
                
                print(f"   å·²æ¢å¤æœ€ä½³çŠ¶æ€ï¼Œæ–°æ¸©åº¦: {agent.temperature:.3f}")
                continue
        
        # ğŸ”§ åŸæœ‰çš„è€å¿ƒå€¼é€€åŒ–æ£€æµ‹
        if agent.patience > agent.max_patience and agent.best_model_state:
            agent.consecutive_rollbacks += 1
            print(f"\nâš ï¸ æ€§èƒ½åœæ» ({agent.patience}ä¸ªepisodeæ— æ”¹è¿›)ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹...")
            print(f"   è¿ç»­å›æº¯æ¬¡æ•°: {agent.consecutive_rollbacks}/{agent.max_consecutive_rollbacks}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰åœæ­¢
            if agent.consecutive_rollbacks >= agent.max_consecutive_rollbacks:
                print(f"\nğŸ›‘ è¿ç»­{agent.max_consecutive_rollbacks}æ¬¡æ€§èƒ½åœæ»ï¼Œè§¦å‘æå‰åœæ­¢ï¼")
                print(f"   æ¨¡å‹å¯èƒ½å·²è¾¾åˆ°æ€§èƒ½ç“¶é¢ˆï¼Œå»ºè®®ç»“æŸè®­ç»ƒ")
                agent.early_stop_triggered = True
                break
            
            print(f"   ç¬¬{agent.consecutive_rollbacks}æ¬¡å›æº¯ï¼Œæ¢å¤Episode {agent.best_model_state['episode']+1}çš„æ¨¡å‹")
            agent.network.load_state_dict(agent.best_model_state['network'])
            agent.optimizer.load_state_dict(agent.best_model_state['optimizer'])
            
            # æ›´ä¿å®ˆçš„è°ƒæ•´
            agent.adjust_learning_rate(agent.lr_recovery_factor)  # æ¢å¤å­¦ä¹ ç‡
            agent.temperature = min(2.0, agent.temperature * 1.1)  # è½»å¾®å¢åŠ æ¢ç´¢
            agent.patience = 0
        
        # å®šæœŸè¾“å‡º
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(reward_window)
            avg_steps = np.mean(agent.episode_steps[-25:])
            
            print(f"Episode {episode + 1:4d}: "
                  f"å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={avg_steps:5.1f}, "
                  f"æˆåŠŸç‡={current_success_rate:.3f}, æ¸©åº¦={agent.temperature:.3f}")
            print(f"                     æœ€ä½³æˆåŠŸç‡={agent.best_success_rate:.3f}, è€å¿ƒå€¼={agent.patience}")
    
    # æ£€æŸ¥è®­ç»ƒç»“æŸåŸå› 
    if agent.early_stop_triggered:
        print(f"\nğŸ”š è®­ç»ƒå› è¿ç»­æ€§èƒ½ä¸‹é™è€Œæå‰åœæ­¢")
        print(f"   æœ€ä½³æˆåŠŸç‡: {agent.best_success_rate:.3f} (Episode {agent.best_model_state['episode']+1})")
    else:
        print(f"\nâœ… è®­ç»ƒæ­£å¸¸å®Œæˆ")
    
    # æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    if agent.best_model_state:
        print(f"\nğŸ”„ æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        agent.network.load_state_dict(agent.best_model_state['network'])
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\n=== æœ€ç»ˆè¯„ä¼° ===")
    test_results = []
    for i in range(50):
        reward, steps, path, success = agent.test_episode()
        test_results.append((reward, steps, success))
    
    final_success_rate = np.mean([r[2] for r in test_results])
    final_avg_reward = np.mean([r[0] for r in test_results])
    final_avg_steps = np.mean([r[1] for r in test_results])
    
    print(f"ç¨³å®šç‰ˆGumbel-Softmax PPOæœ€ç»ˆç»“æœï¼ˆ50æ¬¡æµ‹è¯•ï¼‰:")
    print(f"  æˆåŠŸç‡: {final_success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {final_avg_reward:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {final_avg_steps:.1f}")
    
    # å¯¹æ¯”ç»“æœ
    print(f"\nğŸ“Š è¿ç»­åŒ–æ–¹æ³•å¯¹æ¯”:")
    print(f"  é«˜æ–¯æ˜ å°„PPOæˆåŠŸç‡:    3% (æ¢¯åº¦æ–­è£‚)")
    print(f"  æ™®é€šGumbel PPOæˆåŠŸç‡: 36%->1% (è®­ç»ƒä¸ç¨³å®š)")
    print(f"  ç¨³å®šGumbel PPOæˆåŠŸç‡: {final_success_rate:.1%} (é—®é¢˜è§£å†³)")
    
    if final_success_rate > 0.4:
        print("ğŸ‰ è¿ç»­åŒ–é—®é¢˜å®Œå…¨è§£å†³ï¼")
    elif final_success_rate > 0.2:
        print("âœ… è¿ç»­åŒ–æ˜¾è‘—æ”¹å–„")
    else:
        print("âš ï¸ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # ä¿å­˜æ¨¡å‹
    agent.save_model("models/stable_gumbel_ppo_model.pth")
    print(f"ç¨³å®šç‰ˆGumbel-Softmax PPOæ¨¡å‹å·²ä¿å­˜")
    
    return agent

def debug_trained_model():
    """è°ƒè¯•å·²è®­ç»ƒæ¨¡å‹ï¼Œæ£€æŸ¥è®­ç»ƒæµ‹è¯•å·®å¼‚"""
    print("=== è°ƒè¯•ç¨³å®šç‰ˆGumbel-Softmax PPOæ¨¡å‹ ===")
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    agent = StableGumbelPPORacetrackAgent(env=env)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    try:
        agent.load_model("models/stable_gumbel_ppo_model.pth")
        print("âœ… æˆåŠŸåŠ è½½æ¨¡å‹")
    except:
        print("âŒ æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    print(f"å½“å‰æ¸©åº¦: {agent.temperature}")
    
    # è¯¦ç»†æµ‹è¯•å‡ ä¸ªepisode
    print("\n=== è¯¦ç»†è°ƒè¯•æµ‹è¯• ===")
    for i in range(3):
        print(f"\n--- Episode {i+1} ---")
        reward, steps, path, success = agent.test_episode(debug=True)
        print(f"Episode {i+1}: å¥–åŠ±={reward:.1f}, æ­¥æ•°={steps}, æˆåŠŸ={success}")
    
    # æ¯”è¾ƒä¸åŒç­–ç•¥çš„æˆåŠŸç‡
    print(f"\n=== ç­–ç•¥å¯¹æ¯”æµ‹è¯• ===")
    
    # 1. æµ‹è¯•è´ªå©ªç­–ç•¥ï¼ˆå½“å‰ï¼‰
    test_results_greedy = []
    for i in range(20):
        reward, steps, path, success = agent.test_episode()
        test_results_greedy.append(success)
    greedy_success_rate = np.mean(test_results_greedy) * 100
    print(f"è´ªå©ªç­–ç•¥æˆåŠŸç‡: {greedy_success_rate:.1f}%")
    
    # 2. æµ‹è¯•å¸¦éšæœºæ€§çš„ç­–ç•¥ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶ï¼‰
    print(f"æµ‹è¯•å¸¦éšæœºæ€§ç­–ç•¥ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶ï¼‰...")
    test_results_random = []
    original_temp = agent.temperature
    agent.temperature = 1.0  # ä½¿ç”¨è®­ç»ƒæ—¶çš„æ¸©åº¦
    
    for i in range(20):
        state = env.reset()
        total_reward = 0.0
        steps = 0
        max_steps = 300
        
        agent.network.eval()
        with torch.no_grad():
            while steps < max_steps:
                # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç­–ç•¥ï¼ˆå¸¦Gumbel-Softmaxéšæœºæ€§ï¼‰
                action, _, _ = agent.select_action(state, training=True)
                next_state, reward, done = env.step(action)
                total_reward += reward
                steps += 1
                
                if done:
                    break
                
                state = next_state
        
        success = (done and reward == 100)
        test_results_random.append(success)
    
    agent.temperature = original_temp  # æ¢å¤åŸæ¸©åº¦
    random_success_rate = np.mean(test_results_random) * 100
    print(f"éšæœºç­–ç•¥æˆåŠŸç‡: {random_success_rate:.1f}%")
    
    # åˆ†æç»“æœ
    print(f"\n=== åˆ†æç»“æœ ===")
    print(f"è´ªå©ªç­–ç•¥ï¼ˆæµ‹è¯•ï¼‰: {greedy_success_rate:.1f}%")
    print(f"éšæœºç­–ç•¥ï¼ˆè®­ç»ƒï¼‰: {random_success_rate:.1f}%")
    
    if random_success_rate > greedy_success_rate:
        print("ğŸš¨ ç¡®è®¤é—®é¢˜ï¼šéšæœºæ¢ç´¢æ¯”ç¡®å®šæ€§ç­–ç•¥æ›´æˆåŠŸï¼")
        print("   è¿™è¯´æ˜ç½‘ç»œæ²¡æœ‰å­¦åˆ°æœ‰æ•ˆçš„ç¡®å®šæ€§ç­–ç•¥")
        print("   è®­ç»ƒæ—¶çš„æˆåŠŸä¸»è¦æ¥è‡ªéšæœºæ¢ç´¢ï¼Œè€Œéç­–ç•¥å­¦ä¹ ")
    else:
        print("âœ… ç­–ç•¥å­¦ä¹ æ­£å¸¸ï¼šç¡®å®šæ€§ç­–ç•¥ä¼˜äºéšæœºç­–ç•¥")
    
    return agent

if __name__ == "__main__":
    # main_stable_gumbel_ppo_training()
    debug_trained_model() 
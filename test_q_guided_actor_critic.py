#!/usr/bin/env python3
"""
Q-Guided Actor-Criticç®—æ³•ä¸“é¡¹æµ‹è¯•

æµ‹è¯•Q-Guided ACç®—æ³•çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€è®­ç»ƒæµ‹è¯•
2. æ€§èƒ½å¯¹æ¯”åˆ†æ
3. ä¸‰é˜¶æ®µè®­ç»ƒæ•ˆæœè¯„ä¼°
4. ä¸ä¼ ç»ŸACæ–¹æ³•å¯¹æ¯”

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import time
from typing import Dict, List, Tuple
import json

from racetrack_env import RacetrackEnv
from q_guided_ac_simple import QGuidedActorCritic
from actor_critic import OptimizedActorCriticAgent

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def test_q_guided_performance():
    """æµ‹è¯•Q-Guided ACçš„åŸºæœ¬æ€§èƒ½"""
    print("ğŸš€ Q-Guided Actor-Criticæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    agent = QGuidedActorCritic(env, lr=0.001, gamma=0.95, alpha_q=0.2, epsilon=0.3)
    
    # è®­ç»ƒè®°å½•
    training_results = {
        'episodes': [],
        'success_rates': [],
        'avg_rewards': [],
        'phases': [],
        'q_table_sizes': []
    }
    
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    total_episodes = sum(agent.phase_episodes.values())
    
    for episode in range(total_episodes):
        reward, steps, success = agent.train_episode(episode)
        
        # æ¯50ä¸ªepisodeè¯„ä¼°ä¸€æ¬¡
        if (episode + 1) % 50 == 0:
            test_results = []
            for _ in range(10):
                test_reward, test_steps, test_path, test_success = agent.test_episode()
                test_results.append((test_reward, test_success))
            
            success_rate = np.mean([r[1] for r in test_results])
            avg_reward = np.mean([r[0] for r in test_results])
            
            training_results['episodes'].append(episode + 1)
            training_results['success_rates'].append(success_rate)
            training_results['avg_rewards'].append(avg_reward)
            training_results['phases'].append(agent.training_phase)
            training_results['q_table_sizes'].append(len(agent.Q_table))
            
            print(f"Episode {episode+1} ({agent.training_phase}): "
                  f"æˆåŠŸç‡={success_rate:.2f}, "
                  f"å¹³å‡å¥–åŠ±={avg_reward:.2f}, "
                  f"Qè¡¨å¤§å°={len(agent.Q_table)}")
    
    # æœ€ç»ˆæµ‹è¯•
    print("\nâœ… æœ€ç»ˆæ€§èƒ½æµ‹è¯•...")
    final_results = []
    for i in range(20):
        reward, steps, path, success = agent.test_episode()
        final_results.append({
            'reward': reward,
            'steps': steps,
            'success': success,
            'path_length': len(path)
        })
        print(f"æµ‹è¯•{i+1}: å¥–åŠ±={reward:.2f}, æ­¥æ•°={steps}, æˆåŠŸ={success}")
    
    # ç»Ÿè®¡
    final_success_rate = np.mean([r['success'] for r in final_results])
    final_avg_reward = np.mean([r['reward'] for r in final_results])
    final_avg_steps = np.mean([r['steps'] for r in final_results])
    
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"æˆåŠŸç‡: {final_success_rate:.2%}")
    print(f"å¹³å‡å¥–åŠ±: {final_avg_reward:.2f} Â± {np.std([r['reward'] for r in final_results]):.2f}")
    print(f"å¹³å‡æ­¥æ•°: {final_avg_steps:.2f} Â± {np.std([r['steps'] for r in final_results]):.2f}")
    print(f"æœ€ç»ˆQè¡¨å¤§å°: {len(agent.Q_table)}")
    
    return agent, training_results, final_results


def compare_with_traditional_ac():
    """ä¸ä¼ ç»ŸActor-Criticå¯¹æ¯”"""
    print("\nğŸ”¥ Q-Guided AC vs ä¼ ç»ŸActor-Criticå¯¹æ¯”")
    print("=" * 60)
    
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # æµ‹è¯•Q-Guided AC
    print("\n1ï¸âƒ£ æµ‹è¯•Q-Guided Actor-Critic...")
    q_guided_agent = QGuidedActorCritic(env, lr=0.001, gamma=0.95, alpha_q=0.2, epsilon=0.3)
    
    # å¿«é€Ÿè®­ç»ƒ
    total_episodes = sum(q_guided_agent.phase_episodes.values())
    for episode in range(total_episodes):
        q_guided_agent.train_episode(episode)
    
    # æµ‹è¯•Q-Guided ACæ€§èƒ½
    q_guided_results = []
    for _ in range(20):
        reward, steps, path, success = q_guided_agent.test_episode()
        q_guided_results.append({'reward': reward, 'steps': steps, 'success': success})
    
    # æµ‹è¯•ä¼ ç»ŸACï¼ˆå¦‚æœæ¨¡å‹å­˜åœ¨ï¼‰
    print("\n2ï¸âƒ£ æµ‹è¯•ä¼ ç»ŸActor-Critic...")
    try:
        traditional_agent = OptimizedActorCriticAgent(env)
        traditional_agent.load_model("models/advanced_tuned_model.pth")
        
        traditional_results = []
        for _ in range(20):
            reward, steps, path, success = traditional_agent.test_episode()
            traditional_results.append({'reward': reward, 'steps': steps, 'success': success})
    except Exception as e:
        print(f"âš ï¸ ä¼ ç»ŸACæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ”„ å¼€å§‹è®­ç»ƒä¼ ç»ŸAC...")
        traditional_agent = OptimizedActorCriticAgent(env)
        for episode in range(800):
            traditional_agent.train_episode(episode)
        
        traditional_results = []
        for _ in range(20):
            reward, steps, path, success = traditional_agent.test_episode()
            traditional_results.append({'reward': reward, 'steps': steps, 'success': success})
    
    # å¯¹æ¯”åˆ†æ
    print("\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print("=" * 40)
    
    # Q-Guided ACç»“æœ
    q_success_rate = np.mean([r['success'] for r in q_guided_results])
    q_avg_reward = np.mean([r['reward'] for r in q_guided_results])
    q_avg_steps = np.mean([r['steps'] for r in q_guided_results])
    
    print(f"Q-Guided AC:")
    print(f"  æˆåŠŸç‡: {q_success_rate:.2%}")
    print(f"  å¹³å‡å¥–åŠ±: {q_avg_reward:.2f}")
    print(f"  å¹³å‡æ­¥æ•°: {q_avg_steps:.2f}")
    print(f"  Qè¡¨å¤§å°: {len(q_guided_agent.Q_table)}")
    
    # ä¼ ç»ŸACç»“æœ
    t_success_rate = np.mean([r['success'] for r in traditional_results])
    t_avg_reward = np.mean([r['reward'] for r in traditional_results])
    t_avg_steps = np.mean([r['steps'] for r in traditional_results])
    
    print(f"\nä¼ ç»ŸActor-Critic:")
    print(f"  æˆåŠŸç‡: {t_success_rate:.2%}")
    print(f"  å¹³å‡å¥–åŠ±: {t_avg_reward:.2f}")
    print(f"  å¹³å‡æ­¥æ•°: {t_avg_steps:.2f}")
    
    # ä¼˜åŠ¿åˆ†æ
    print(f"\nğŸ† Q-Guided ACä¼˜åŠ¿:")
    print(f"  æˆåŠŸç‡æå‡: {(q_success_rate - t_success_rate):.2%}")
    print(f"  å¥–åŠ±æå‡: {(q_avg_reward - t_avg_reward):.2f}")
    print(f"  æ­¥æ•°ä¼˜åŒ–: {(t_avg_steps - q_avg_steps):.2f}")
    
    return q_guided_results, traditional_results


def analyze_training_phases():
    """åˆ†æä¸‰é˜¶æ®µè®­ç»ƒæ•ˆæœ"""
    print("\nğŸ”¬ ä¸‰é˜¶æ®µè®­ç»ƒæ•ˆæœåˆ†æ")
    print("=" * 50)
    
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    agent = QGuidedActorCritic(env, lr=0.001, gamma=0.95, alpha_q=0.2, epsilon=0.3)
    
    phase_results = {
        'q_learning': [],
        'hybrid': [],
        'actor_critic': []
    }
    
    total_episodes = sum(agent.phase_episodes.values())
    
    for episode in range(total_episodes):
        reward, steps, success = agent.train_episode(episode)
        
        # è®°å½•æ¯ä¸ªé˜¶æ®µçš„è¡¨ç°
        if agent.training_phase == 'q_learning':
            phase_results['q_learning'].append({'episode': episode, 'success': success, 'reward': reward})
        elif agent.training_phase == 'hybrid':
            phase_results['hybrid'].append({'episode': episode, 'success': success, 'reward': reward})
        else:  # actor_critic
            phase_results['actor_critic'].append({'episode': episode, 'success': success, 'reward': reward})
    
    # åˆ†æå„é˜¶æ®µæ€§èƒ½
    print("\nğŸ“ˆ å„é˜¶æ®µæ€§èƒ½åˆ†æ:")
    
    for phase_name, results in phase_results.items():
        if results:
            success_rate = np.mean([r['success'] for r in results])
            avg_reward = np.mean([r['reward'] for r in results])
            print(f"{phase_name.upper()} é˜¶æ®µ:")
            print(f"  Episodeæ•°: {len(results)}")
            print(f"  æˆåŠŸç‡: {success_rate:.2%}")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    
    return phase_results


def create_training_visualization(training_results):
    """åˆ›å»ºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–"""
    print("\nğŸ¨ ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    episodes = training_results['episodes']
    
    # 1. æˆåŠŸç‡æ›²çº¿
    ax1.plot(episodes, training_results['success_rates'], 'b-', linewidth=2, marker='o')
    ax1.set_title('Success Rate Progress', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Success Rate')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ é˜¶æ®µåˆ†ç•Œçº¿
    phase_boundaries = []
    current_phase = None
    for i, phase in enumerate(training_results['phases']):
        if phase != current_phase:
            if current_phase is not None:
                phase_boundaries.append(episodes[i])
            current_phase = phase
    
    for boundary in phase_boundaries:
        ax1.axvline(x=boundary, color='red', linestyle='--', alpha=0.7)
    
    # 2. å¹³å‡å¥–åŠ±æ›²çº¿
    ax2.plot(episodes, training_results['avg_rewards'], 'g-', linewidth=2, marker='s')
    ax2.set_title('Average Reward Progress', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Average Reward')
    ax2.grid(True, alpha=0.3)
    
    for boundary in phase_boundaries:
        ax2.axvline(x=boundary, color='red', linestyle='--', alpha=0.7)
    
    # 3. Qè¡¨å¢é•¿æ›²çº¿
    ax3.plot(episodes, training_results['q_table_sizes'], 'orange', linewidth=2, marker='^')
    ax3.set_title('Q-Table Size Growth', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Episode')
    ax3.set_ylabel('Q-Table Size')
    ax3.grid(True, alpha=0.3)
    
    for boundary in phase_boundaries:
        ax3.axvline(x=boundary, color='red', linestyle='--', alpha=0.7)
    
    # 4. è®­ç»ƒé˜¶æ®µåˆ†å¸ƒ
    phase_counts = {'q_learning': 0, 'hybrid': 0, 'actor_critic': 0}
    for phase in training_results['phases']:
        if phase in phase_counts:
            phase_counts[phase] += 1
    
    phases = list(phase_counts.keys())
    counts = list(phase_counts.values())
    colors = ['red', 'orange', 'green']
    
    ax4.pie(counts, labels=phases, colors=colors, autopct='%1.1f%%', startangle=90)
    ax4.set_title('Training Phase Distribution', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"q_guided_ac_training_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“„ è®­ç»ƒå¯è§†åŒ–å·²ä¿å­˜: {filename}")
    
    plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Q-Guided Actor-Criticå…¨é¢æµ‹è¯•")
    print("=" * 60)
    print("æœ¬æµ‹è¯•å°†è¯„ä¼°Q-Guided ACç®—æ³•çš„:")
    print("  1. åŸºç¡€æ€§èƒ½è¡¨ç°")
    print("  2. ä¸ä¼ ç»ŸACçš„å¯¹æ¯”")
    print("  3. ä¸‰é˜¶æ®µè®­ç»ƒæ•ˆæœ")
    print("  4. å­¦ä¹ è¿‡ç¨‹å¯è§†åŒ–")
    print("=" * 60)
    
    # 1. åŸºç¡€æ€§èƒ½æµ‹è¯•
    agent, training_results, final_results = test_q_guided_performance()
    
    # 2. ä¸ä¼ ç»ŸACå¯¹æ¯”
    q_guided_results, traditional_results = compare_with_traditional_ac()
    
    # 3. åˆ†æè®­ç»ƒé˜¶æ®µ
    phase_results = analyze_training_phases()
    
    # 4. åˆ›å»ºå¯è§†åŒ–
    create_training_visualization(training_results)
    
    # 5. ä¿å­˜æµ‹è¯•ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results = {
        'timestamp': timestamp,
        'training_results': training_results,
        'final_results': final_results,
        'q_guided_results': q_guided_results,
        'traditional_results': traditional_results,
        'phase_results': phase_results
    }
    
    filename = f"q_guided_ac_test_results_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {filename}")
    
    # æ€»ç»“
    print(f"\nğŸ‰ Q-Guided Actor-Criticæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ† å…³é”®å‘ç°:")
    
    final_success_rate = np.mean([r['success'] for r in final_results])
    q_success_rate = np.mean([r['success'] for r in q_guided_results])
    t_success_rate = np.mean([r['success'] for r in traditional_results])
    
    print(f"  ğŸ“ˆ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}")
    print(f"  ğŸ†š ç›¸æ¯”ä¼ ç»ŸACæå‡: {(q_success_rate - t_success_rate):.2%}")
    print(f"  ğŸ§  Qè¡¨æœ€ç»ˆå¤§å°: {len(agent.Q_table)}")
    print(f"  âš¡ ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥æœ‰æ•ˆç»“åˆäº†è¡¨æ ¼å’Œç¥ç»ç½‘ç»œæ–¹æ³•çš„ä¼˜åŠ¿")


if __name__ == "__main__":
    main() 
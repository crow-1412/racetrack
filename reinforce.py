"""
REINFORCE with Baseline å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - åŸºäºActor-CriticæˆåŠŸç»éªŒçš„ä¼˜åŒ–ç‰ˆæœ¬

æœ¬æ–‡ä»¶åŸºäºæˆåŠŸçš„Actor-Criticç®—æ³•ç»éªŒå¯¹REINFORCEè¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ï¼š

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä¸¥æ ¼çš„åŠ¨ä½œæ©ç  - ä»Actor-Criticå€Ÿé‰´çš„å®‰å…¨æœºåˆ¶
2. ç®€åŒ–çš„å¥–åŠ±å¡‘å½¢ - é¿å…è¿‡åº¦å·¥ç¨‹åŒ– 
3. åˆ†ç¦»çš„ä¼˜åŒ–å™¨é…ç½® - ç­–ç•¥å’Œä»·å€¼ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
4. ææ…¢çš„æ¢ç´¢è¡°å‡ - é˜²æ­¢è¿‡æ—©æ”¶æ•›
5. æœ€ä½³æ¨¡å‹ä¿æŠ¤æœºåˆ¶ - é˜²æ­¢æ€§èƒ½é€€åŒ–
6. ä¼˜åŒ–çš„çŠ¶æ€è¡¨ç¤º - 8ç»´ç²¾å¿ƒè®¾è®¡çš„ç‰¹å¾

ä½œè€…ï¼šAI Assistant  
æœ€åæ›´æ–°ï¼š2024å¹´
åŸºäºï¼šActor-CriticæˆåŠŸç»éªŒï¼ˆ60%+æˆåŠŸç‡ï¼‰
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Tuple, List, Dict
import random
from collections import deque
import matplotlib.pyplot as plt
from racetrack_env import RacetrackEnv

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print(f"ğŸ² ä¼˜åŒ–REINFORCEéšæœºç§å­å·²è®¾ç½®ä¸º: {RANDOM_SEED}")


class ImprovedPolicyNetwork(nn.Module):
    """
    æ”¹è¿›çš„ç­–ç•¥ç½‘ç»œ
    
    åŸºäºActor-CriticæˆåŠŸç»éªŒçš„ç½‘ç»œæ¶æ„
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(ImprovedPolicyNetwork, self).__init__()
        
        # ä½¿ç”¨ä¸æˆåŠŸActor-Criticç›¸ä¼¼çš„æ¶æ„
        self.shared_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        self.policy_head = nn.Linear(hidden_dim // 2, action_dim)
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """ç½‘ç»œå‚æ•°åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=1.0)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        shared_features = self.shared_layers(state)
        action_logits = self.policy_head(shared_features)
        return F.softmax(action_logits, dim=-1)


class ImprovedValueNetwork(nn.Module):
    """
    æ”¹è¿›çš„ä»·å€¼ç½‘ç»œï¼ˆåŸºçº¿ï¼‰
    
    åŸºäºActor-CriticæˆåŠŸç»éªŒçš„ç½‘ç»œæ¶æ„
    """
    
    def __init__(self, state_dim: int, hidden_dim: int = 128):
        super(ImprovedValueNetwork, self).__init__()
        
        # ä½¿ç”¨ä¸æˆåŠŸActor-Criticç›¸ä¼¼çš„æ¶æ„
        self.shared_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        self.value_head = nn.Linear(hidden_dim // 2, 1)
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """ç½‘ç»œå‚æ•°åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=1.0)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        shared_features = self.shared_layers(state)
        return self.value_head(shared_features)


class OptimizedREINFORCEAgent:
    """
    åŸºäºActor-CriticæˆåŠŸç»éªŒä¼˜åŒ–çš„REINFORCEæ™ºèƒ½ä½“
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä¸¥æ ¼çš„åŠ¨ä½œæ©ç æœºåˆ¶
    2. ç®€åŒ–çš„å¥–åŠ±å¡‘å½¢
    3. åˆ†ç¦»çš„ä¼˜åŒ–å™¨é…ç½®
    4. ææ…¢çš„æ¢ç´¢è¡°å‡
    5. æœ€ä½³æ¨¡å‹ä¿æŠ¤
    """
    
    def __init__(self, env: RacetrackEnv, lr_policy: float = 0.0005, lr_value: float = 0.0003,
                 gamma: float = 0.99, hidden_dim: int = 128, entropy_coef: float = 0.05):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„REINFORCEæ™ºèƒ½ä½“
        
        Args:
            env: ç¯å¢ƒ
            lr_policy: ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡ï¼ˆä»Actor-Criticç»éªŒè°ƒæ•´ï¼‰
            lr_value: ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡ï¼ˆä»Actor-Criticç»éªŒè°ƒæ•´ï¼‰
            gamma: æŠ˜æ‰£å› å­
            hidden_dim: éšè—å±‚ç»´åº¦
            entropy_coef: ç†µæ­£åˆ™åŒ–ç³»æ•°
        """
        self.env = env
        self.gamma = gamma
        self.entropy_coef = entropy_coef
        
        # çŠ¶æ€ç‰¹å¾ç»´åº¦ï¼ˆä¸æˆåŠŸçš„Actor-Criticç›¸åŒï¼‰
        self.state_dim = 8
        self.action_dim = env.n_actions
        
        # åˆ›å»ºæ”¹è¿›çš„ç½‘ç»œ
        self.policy_net = ImprovedPolicyNetwork(self.state_dim, self.action_dim, hidden_dim)
        self.value_net = ImprovedValueNetwork(self.state_dim, hidden_dim)
        
        # åˆ†ç¦»çš„ä¼˜åŒ–å™¨ï¼ˆä»Actor-CriticæˆåŠŸç»éªŒï¼‰
        self.policy_optimizer = optim.AdamW(
            self.policy_net.parameters(), 
            lr=lr_policy, 
            weight_decay=1e-5
        )
        self.value_optimizer = optim.AdamW(
            self.value_net.parameters(), 
            lr=lr_value,
            weight_decay=1e-5
        )
        
        # ä¼˜åŒ–çš„æ¢ç´¢ç­–ç•¥ï¼ˆå›ºå®šèŒƒå›´ï¼Œé¿å…è¿‡åº¦å¢é•¿ï¼‰
        self.epsilon = 0.3              # é€‚ä¸­çš„åˆå§‹æ¢ç´¢ç‡
        self.epsilon_min = 0.05         # æ›´ä½çš„æœ€å°æ¢ç´¢ç‡
        self.epsilon_decay = 0.9995     # è¾ƒå¿«ä½†ç¨³å®šçš„è¡°å‡
        
        # æ·»åŠ å¥–åŠ±æ ‡å‡†åŒ–ï¼ˆé™ä½æ–¹å·®ï¼‰
        self.reward_running_mean = 0.0
        self.reward_running_std = 1.0
        self.reward_alpha = 0.01
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        self.success_rate: List[float] = []
        self.policy_losses: List[float] = []
        self.value_losses: List[float] = []
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        self.best_success_rate = 0.0
        self.best_model_state = None
        self.patience = 0
        self.max_patience = 150  # é™ä½è€å¿ƒå€¼
        self.no_improvement_count = 0
    
    def state_to_tensor(self, state: Tuple[int, int, int, int]) -> torch.Tensor:
        """
        çŠ¶æ€è½¬æ¢ä¸ºå¼ é‡ï¼ˆä½¿ç”¨ä¸æˆåŠŸActor-Criticå®Œå…¨ç›¸åŒçš„çŠ¶æ€è¡¨ç¤ºï¼‰
        """
        x, y, vx, vy = state
        
        # 1. åŸºç¡€ç‰¹å¾å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
        norm_x = x / 31.0               
        norm_y = y / 16.0               
        norm_vx = vx / self.env.max_speed  
        norm_vy = vy / self.env.max_speed  
        
        # 2. è®¡ç®—åˆ°æœ€è¿‘ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_distance = float('inf')
        goal_direction_x, goal_direction_y = 0, 0
        
        # éå†æ‰€æœ‰ç»ˆç‚¹ï¼Œæ‰¾åˆ°æœ€è¿‘çš„
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            if distance < min_distance:
                min_distance = distance
                if distance > 0:
                    # è®¡ç®—æŒ‡å‘ç›®æ ‡çš„å•ä½æ–¹å‘å‘é‡
                    goal_direction_x = -(goal_x - x) / distance  
                    goal_direction_y = (goal_y - y) / distance   
        
        # 3. è·ç¦»å½’ä¸€åŒ–
        max_distance = np.sqrt(31**2 + 16**2)
        norm_distance = min_distance / max_distance
        
        # 4. è®¡ç®—é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        velocity_alignment = 0.0
        if min_distance > 0:
            velocity_mag = np.sqrt(vx**2 + vy**2)
            if velocity_mag > 0:
                vel_dir_x = vx / velocity_mag
                vel_dir_y = vy / velocity_mag
                velocity_alignment = max(0, vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y)
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_distance, goal_direction_x, goal_direction_y, 
            velocity_alignment
        ], dtype=torch.float32)
    
    def _apply_strict_action_mask(self, state: Tuple[int, int, int, int], 
                                action_probs: torch.Tensor) -> torch.Tensor:
        """
        åº”ç”¨ä¸¥æ ¼çš„åŠ¨ä½œæ©ç ï¼ˆå®Œå…¨ä»æˆåŠŸçš„Actor-Criticå¤åˆ¶ï¼‰
        
        è¿™æ˜¯å…³é”®çš„å®‰å…¨æœºåˆ¶ï¼Œç¡®ä¿æ™ºèƒ½ä½“ä¸ä¼šé€‰æ‹©æ˜æ˜¾é”™è¯¯çš„åŠ¨ä½œ
        """
        x, y, vx, vy = state
        mask = torch.ones_like(action_probs)
        
        # éå†æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œ
        for i, (ax, ay) in enumerate(self.env.actions):
            # é¢„æµ‹æ‰§è¡ŒåŠ¨ä½œåçš„æ–°é€Ÿåº¦
            new_vx = max(0, min(self.env.max_speed, vx + ax))
            new_vy = max(0, min(self.env.max_speed, vy + ay))
            
            # å¤„ç†é€Ÿåº¦ä¸º0çš„ç‰¹æ®Šæƒ…å†µ
            if new_vx == 0 and new_vy == 0 and (x, y) not in self.env.start_positions:
                new_vx = 1
                new_vy = 1
            
            # é¢„æµ‹ä¸‹ä¸€æ­¥ä½ç½®
            new_x = x - new_vx  # å‘ä¸Šç§»åŠ¨ï¼ˆxå‡å°ï¼‰
            new_y = y + new_vy  # å‘å³ç§»åŠ¨ï¼ˆyå¢å¤§ï¼‰
            
            # æ£€æŸ¥æ˜¯å¦ä¼šå‘ç”Ÿç¢°æ’
            if self.env._check_collision(x, y, new_x, new_y):
                mask[i] = 0.0  # ç¦æ­¢æ­¤åŠ¨ä½œ
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªåŠ¨ä½œå¯é€‰
        if mask.sum() == 0:
            mask.fill_(1.0)
        
        # é‡æ–°å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ
        masked_probs = action_probs * mask
        return masked_probs / (masked_probs.sum() + 1e-8)
    
    def select_action(self, state: Tuple[int, int, int, int], training: bool = True):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼Œæµ‹è¯•æ—¶æ›´ç¨³å®šï¼‰
        """
        state_tensor = self.state_to_tensor(state)
        
        if training:
            action_probs = self.policy_net(state_tensor)
        else:
            # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            with torch.no_grad():
                action_probs = self.policy_net(state_tensor)
        
        # åº”ç”¨ä¸¥æ ¼çš„åŠ¨ä½œæ©ç 
        action_probs = self._apply_strict_action_mask(state, action_probs)
        
        # ğŸ”§ ä¿®å¤ï¼šåœ¨èµ·ç‚¹æ—¶å¼ºåˆ¶ç§»åŠ¨
        x, y, vx, vy = state
        start_positions = [(31, i) for i in range(17)]  # èµ·ç‚¹ä½ç½®
        
        if (x, y) in start_positions and vx == 0 and vy == 0:
            # åœ¨èµ·ç‚¹ä¸”é€Ÿåº¦ä¸ºé›¶æ—¶ï¼Œå¼ºåˆ¶é€‰æ‹©ç§»åŠ¨åŠ¨ä½œï¼Œç¦æ­¢"åœç•™"
            action_probs = action_probs.clone()
            action_probs[0] = 0.0  # ç¦æ­¢åŠ¨ä½œ0ï¼ˆåœç•™ï¼‰
            # é‡æ–°æ ‡å‡†åŒ–
            if action_probs.sum() > 0:
                action_probs = action_probs / action_probs.sum()
            else:
                # æ‰€æœ‰åŠ¨ä½œéƒ½è¢«ç¦æ­¢æ—¶ï¼Œç»™ä¸€ä¸ªé»˜è®¤åŠ¨ä½œ
                action_probs = torch.zeros_like(action_probs)
                action_probs[1] = 1.0  # åŠ¨ä½œ1ï¼šå‘å‰ç§»åŠ¨
        
        # åŠ¨ä½œé€‰æ‹©ç­–ç•¥
        if training and random.random() < self.epsilon:
            # è®­ç»ƒæ¨¡å¼æ¢ç´¢ï¼šåœ¨æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
            valid_actions = (action_probs > 0).nonzero().squeeze(-1)
            if len(valid_actions) > 0:
                action = valid_actions[random.randint(0, len(valid_actions)-1)]
            else:
                action = torch.argmax(action_probs)
        else:
            # è´ªå¿ƒç­–ç•¥ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ
            action = torch.argmax(action_probs)
        
        # è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        action_dist = torch.distributions.Categorical(action_probs)
        log_prob = action_dist.log_prob(action)
        
        return action.item(), log_prob
    
    def _enhanced_reward_shaping(self, state, next_state, reward, done, steps):
        """
        å¢å¼ºçš„å¥–åŠ±å¡‘å½¢ï¼ˆè¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        
        è®¾è®¡åŸåˆ™ï¼š
        1. æ›´å¼ºçš„ç›®æ ‡å¯¼å‘
        2. æ›´åˆç†çš„è¿›æ­¥å¥–åŠ±
        3. é¿å…å±€éƒ¨æœ€ä¼˜
        """
        bonus = 0.0
        
        # 1. æˆåŠŸ/å¤±è´¥çš„æ˜ç¡®å¥–åŠ±
        if done and reward > 0:
            bonus += 200    # å¢åŠ æˆåŠŸå¥–åŠ±
        elif reward == -10:  # ç¢°æ’
            bonus -= 100    # å¢åŠ ç¢°æ’æƒ©ç½š
        
        # 2. æ”¹è¿›çš„è¿›æ­¥å¥–åŠ±
        x, y, vx, vy = state
        next_x, next_y, next_vx, next_vy = next_state
        
        # è®¡ç®—åˆ°æœ€è¿‘ç›®æ ‡çš„æ¬§å‡ é‡Œå¾—è·ç¦»
        curr_dist = min([np.sqrt((x - gx)**2 + (y - gy)**2) for gx, gy in self.env.goal_positions])
        next_dist = min([np.sqrt((next_x - gx)**2 + (next_y - gy)**2) for gx, gy in self.env.goal_positions])
        
        # è·ç¦»å‡å°‘å¥–åŠ±ï¼ˆæ›´ç²¾ç»†ï¼‰
        dist_improvement = curr_dist - next_dist
        if dist_improvement > 0.5:
            bonus += 5.0 * dist_improvement
        elif dist_improvement > 0:
            bonus += 2.0 * dist_improvement
        
        # 3. é€Ÿåº¦å¥–åŠ±ï¼ˆæœå‘ç›®æ ‡çš„é€Ÿåº¦ï¼‰
        if curr_dist > 0:
            # æ‰¾åˆ°æœ€è¿‘çš„ç›®æ ‡
            closest_goal = min(self.env.goal_positions, 
                             key=lambda g: np.sqrt((x - g[0])**2 + (y - g[1])**2))
            goal_x, goal_y = closest_goal
            
            # è®¡ç®—æœå‘ç›®æ ‡çš„é€Ÿåº¦åˆ†é‡
            dir_to_goal_x = -(goal_x - next_x) / max(curr_dist, 1e-6)
            dir_to_goal_y = (goal_y - next_y) / max(curr_dist, 1e-6)
            
            # é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„ç‚¹ç§¯
            velocity_toward_goal = next_vx * dir_to_goal_x + next_vy * dir_to_goal_y
            if velocity_toward_goal > 0:
                bonus += 1.0 * velocity_toward_goal
        
        # 4. è·ç¦»æƒ©ç½šï¼ˆè¿œç¦»ç›®æ ‡çš„è½»å¾®æƒ©ç½šï¼‰
        if next_dist > curr_dist:
            bonus -= 1.0
        
        # 5. æ­¥æ•°æ•ˆç‡å¥–åŠ±
        if steps < 100:  # æ—©æœŸå®Œæˆæœ‰é¢å¤–å¥–åŠ±
            bonus += 0.5
        
        # 6. æ›´å°çš„æ—¶é—´æƒ©ç½š
        bonus -= 0.05
        
        return reward + bonus
    
    def _normalize_reward(self, reward: float) -> float:
        """
        å¥–åŠ±æ ‡å‡†åŒ–ï¼ˆé™ä½æ–¹å·®ï¼‰
        """
        # æ›´æ–°è¿è¡Œç»Ÿè®¡
        self.reward_running_mean = (1 - self.reward_alpha) * self.reward_running_mean + self.reward_alpha * reward
        
        # æ›´æ–°æ ‡å‡†å·®
        var = (reward - self.reward_running_mean) ** 2
        self.reward_running_std = (1 - self.reward_alpha) * self.reward_running_std + self.reward_alpha * var
        
        # æ ‡å‡†åŒ–
        normalized = (reward - self.reward_running_mean) / (np.sqrt(self.reward_running_std) + 1e-8)
        
        # è£å‰ªåˆ°åˆç†èŒƒå›´
        return np.clip(normalized, -10, 10)
    
    def compute_returns(self, rewards: List[float]) -> List[float]:
        """è®¡ç®—æŠ˜æ‰£å›æŠ¥"""
        returns = []
        G = 0
        
        # ä»åå‘å‰è®¡ç®—æŠ˜æ‰£å›æŠ¥
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.append(G)
        
        return list(reversed(returns))
    
    def collect_episode(self, max_steps: int = 200) -> Tuple[float, int, bool, Dict]:
        """æ”¶é›†å®Œæ•´episodeæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        
        episode_states = []
        episode_actions = []
        episode_log_probs = []
        episode_rewards = []
        episode_normalized_rewards = []
        
        last_reward = 0
        
        while steps < max_steps:
            action, log_prob = self.select_action(state, training=True)
            prev_state = state
            
            next_state, reward, done = self.env.step(action)
            last_reward = reward
            
            # ä½¿ç”¨å¢å¼ºçš„å¥–åŠ±å¡‘å½¢
            shaped_reward = self._enhanced_reward_shaping(prev_state, next_state, reward, done, steps)
            
            # å¥–åŠ±æ ‡å‡†åŒ–
            normalized_reward = self._normalize_reward(shaped_reward)
            
            # å­˜å‚¨æ•°æ®
            episode_states.append(self.state_to_tensor(prev_state))
            episode_actions.append(action)
            episode_log_probs.append(log_prob)
            episode_rewards.append(shaped_reward)
            episode_normalized_rewards.append(normalized_reward)
            
            total_reward += reward  # ä½¿ç”¨åŸå§‹å¥–åŠ±è®¡ç®—å›æŠ¥
            steps += 1
            
            if done:
                break
                
            state = next_state
        
        # ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±è®¡ç®—æŠ˜æ‰£å›æŠ¥
        returns = self.compute_returns(episode_normalized_rewards)
        
        success = (steps < max_steps and done and last_reward == 100)
        
        episode_data = {
            'states': episode_states,
            'actions': episode_actions,
            'log_probs': episode_log_probs,
            'rewards': episode_normalized_rewards,
            'returns': returns
        }
        
        return total_reward, steps, success, episode_data
    
    def update_networks(self, episode_data: Dict):
        """
        æ›´æ–°ç­–ç•¥å’Œä»·å€¼ç½‘ç»œï¼ˆè¿›ä¸€æ­¥ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        """
        # å‡†å¤‡æ•°æ®
        states = torch.stack(episode_data['states'])
        actions = torch.tensor(episode_data['actions'], dtype=torch.long)
        log_probs = torch.stack(episode_data['log_probs'])
        returns = torch.tensor(episode_data['returns'], dtype=torch.float32)
        
        # 1. å¤šæ­¥ä»·å€¼ç½‘ç»œæ›´æ–°ï¼ˆæé«˜åŸºçº¿è´¨é‡ï¼‰
        for _ in range(3):  # å¤šæ¬¡æ›´æ–°ä»·å€¼ç½‘ç»œ
            values = self.value_net(states).squeeze()
            
            # ä»·å€¼æŸå¤±ï¼ˆæ›´ç¨³å®šçš„ç›®æ ‡ï¼‰
            value_targets = returns.detach()
            value_loss = F.mse_loss(values, value_targets)
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
            self.value_optimizer.step()
        
        # 2. ç­–ç•¥ç½‘ç»œæ›´æ–°
        # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥ä¸‹çš„åŠ¨ä½œæ¦‚ç‡
        current_action_probs = self.policy_net(states)
        action_dist = torch.distributions.Categorical(current_action_probs)
        new_log_probs = action_dist.log_prob(actions)
        
        # è®¡ç®—ä¼˜åŠ¿ï¼ˆä½¿ç”¨æœ€æ–°çš„ä»·å€¼å‡½æ•°ï¼‰
        with torch.no_grad():
            final_values = self.value_net(states).squeeze()
            advantages = returns - final_values
            
            # ä¼˜åŠ¿æ ‡å‡†åŒ–ï¼ˆæ›´ç¨³å®šï¼‰
            if len(advantages) > 1:
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
                advantages = torch.clamp(advantages, -3.0, 3.0)  # æ›´ä¸¥æ ¼çš„è£å‰ª
        
        # ç­–ç•¥æŸå¤±ï¼ˆREINFORCEï¼‰
        policy_loss = -(new_log_probs * advantages).mean()
        
        # ç†µæ­£åˆ™åŒ–ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        entropy = action_dist.entropy().mean()
        entropy_coef = max(0.01, self.entropy_coef * (1 - len(self.episode_rewards) / 2000))
        policy_total_loss = policy_loss - entropy_coef * entropy
        
        self.policy_optimizer.zero_grad()
        policy_total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
        self.policy_optimizer.step()
        
        # è®°å½•æŸå¤±
        self.policy_losses.append(policy_loss.item())
        self.value_losses.append(value_loss.item())
    
    def train_episode(self, episode_num: int) -> Tuple[float, int, bool]:
        """è®­ç»ƒå•ä¸ªepisode"""
        # æ”¶é›†å®Œæ•´episode
        total_reward, steps, success, episode_data = self.collect_episode()
        
        # æ›´æ–°ç½‘ç»œ
        self.update_networks(episode_data)
        
        # æ›´æ–°æ¢ç´¢ç‡ï¼ˆæ¯10ä¸ªepisodeæ›´æ–°ä¸€æ¬¡ï¼‰
        if episode_num % 10 == 0:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        return total_reward, steps, success
    
    def test_episode(self, render: bool = False, debug: bool = False) -> Tuple[float, int, List, bool]:
        """æµ‹è¯•å•ä¸ªepisodeï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼Œä¸è®­ç»ƒç¯å¢ƒä¸€è‡´ï¼‰"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]
        max_steps = 200  # ä¿®æ­£ï¼šä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        
        if debug:
            print(f"DEBUG: æµ‹è¯•å¼€å§‹ï¼Œmax_steps={max_steps}, åˆå§‹çŠ¶æ€={state}")
        
        last_reward = 0
        collision_count = 0
        with torch.no_grad():
            while steps < max_steps:
                action, log_prob = self.select_action(state, training=False)
                
                if debug and steps < 10:
                    print(f"DEBUG: Step {steps}, state={state}, action={action}, log_prob={log_prob:.4f}")
                
                next_state, reward, done = self.env.step(action)
                
                if reward == -10:  # ç¢°æ’
                    collision_count += 1
                
                total_reward += reward
                steps += 1
                path.append(next_state[:2])
                last_reward = reward
                
                if debug and (reward != -1 or steps % 50 == 0):
                    print(f"DEBUG: Step {steps}, reward={reward}, total_reward={total_reward}, done={done}")
                
                if done:
                    if debug:
                        print(f"DEBUG: Episodeç»“æŸï¼Œsteps={steps}, last_reward={last_reward}, success={last_reward==100}")
                    break
                
                state = next_state
        
        success = (steps < max_steps and done and last_reward == 100)
        
        if debug:
            print(f"DEBUG: æœ€ç»ˆç»“æœ - steps={steps}, max_steps={max_steps}, done={done}, last_reward={last_reward}")
            print(f"DEBUG: successåˆ¤æ–­ - (steps < max_steps)={steps < max_steps}, done={done}, (last_reward==100)={last_reward==100}")
            print(f"DEBUG: collision_count={collision_count}, total_reward={total_reward}")
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'policy_net': self.policy_net.state_dict(),
            'value_net': self.value_net.state_dict(),
            'policy_optimizer': self.policy_optimizer.state_dict(),
            'value_optimizer': self.value_optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_rate': self.success_rate,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'epsilon': self.epsilon
        }
        torch.save(save_dict, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
        self.policy_net.load_state_dict(checkpoint['policy_net'])
        self.value_net.load_state_dict(checkpoint['value_net'])


def main_ultimate_reinforce_training():
    """
    ç»ˆæä¼˜åŒ–ç‰ˆæœ¬çš„REINFORCEè®­ç»ƒå‡½æ•°
    """
    print("=== ç»ˆæä¼˜åŒ–ç‰ˆREINFORCEè®­ç»ƒ ===")
    print(f"ğŸ² ä½¿ç”¨å›ºå®šéšæœºç§å­: {RANDOM_SEED}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºç»ˆæä¼˜åŒ–çš„REINFORCEæ™ºèƒ½ä½“
    agent = OptimizedREINFORCEAgent(
        env=env,
        lr_policy=0.0003,    # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
        lr_value=0.0001,     # æ›´ä¿å®ˆçš„ä»·å€¼å­¦ä¹ ç‡
        gamma=0.98,          # ç¨å¾®é™ä½æŠ˜æ‰£å› å­
        hidden_dim=128,
        entropy_coef=0.03    # é™ä½ç†µç³»æ•°
    )
    
    print(f"ç»ˆæREINFORCEé…ç½®:")
    print(f"  - ç­–ç•¥å­¦ä¹ ç‡: 0.0003 (æ›´ä¿å®ˆ)")
    print(f"  - ä»·å€¼å­¦ä¹ ç‡: 0.0001 (æ›´ä¿å®ˆ)")
    print(f"  - æŠ˜æ‰£å› å­: 0.98 (çŸ­æœŸå¥–åŠ±åå¥½)")
    print(f"  - ç½‘ç»œç»´åº¦: 128")
    print(f"  - ç†µæ­£åˆ™åŒ–: 0.03 (åŠ¨æ€è°ƒæ•´)")
    print(f"  - æ¢ç´¢ç­–ç•¥: ç¨³å®šè¡°å‡ (0.9995)")
    print(f"  - åŠ¨ä½œæ©ç : ä¸¥æ ¼ç¢°æ’æ£€æµ‹")
    print(f"  - å¥–åŠ±ç³»ç»Ÿ: å¢å¼ºå¡‘å½¢ + æ ‡å‡†åŒ–")
    print(f"  - åŸºçº¿è®­ç»ƒ: å¤šæ­¥æ›´æ–°")
    
    # è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
    print("\n=== è®­ç»ƒå‰åŸºå‡† ===")
    reward_before, steps_before, _, success_before = agent.test_episode()
    print(f"åŸºå‡†æ€§èƒ½: å¥–åŠ±={reward_before:.1f}, æ­¥æ•°={steps_before}, æˆåŠŸ={success_before}")
    
    # æ›´ä¿å®ˆçš„åˆ†é˜¶æ®µè®­ç»ƒ
    n_episodes = 1500
    stage1_episodes = 500   # é˜¶æ®µ1ï¼šåŸºç¡€å­¦ä¹ 
    stage2_episodes = 600   # é˜¶æ®µ2ï¼šç¨³å®šè®­ç»ƒ  
    stage3_episodes = 400   # é˜¶æ®µ3ï¼šç²¾è°ƒä¼˜åŒ–
    
    print(f"\n=== ä¿å®ˆåˆ†é˜¶æ®µè®­ç»ƒè®¡åˆ’ ===")
    print(f"  é˜¶æ®µ1 (0-{stage1_episodes}): åŸºç¡€ç­–ç•¥å­¦ä¹ ")
    print(f"  é˜¶æ®µ2 ({stage1_episodes}-{stage1_episodes+stage2_episodes}): ç¨³å®šæ€§è®­ç»ƒ")
    print(f"  é˜¶æ®µ3 ({stage1_episodes+stage2_episodes}-{n_episodes}): ç²¾è°ƒä¼˜åŒ–")
    
    # è®­ç»ƒç»Ÿè®¡
    success_window = deque(maxlen=50)  # æ›´å°çš„çª—å£
    reward_window = deque(maxlen=25)
    performance_window = deque(maxlen=30)
    
    for episode in range(n_episodes):
        # ä¿å®ˆçš„åˆ†é˜¶æ®µè°ƒæ•´
        if episode == stage1_episodes:
            print(f"\nğŸ”„ è¿›å…¥é˜¶æ®µ2: ç¨³å®šæ€§ä¼˜å…ˆ")
            for param_group in agent.policy_optimizer.param_groups:
                param_group['lr'] *= 0.8
            for param_group in agent.value_optimizer.param_groups:
                param_group['lr'] *= 0.9
                
        elif episode == stage1_episodes + stage2_episodes:
            print(f"\nğŸ”§ è¿›å…¥é˜¶æ®µ3: ç²¾è°ƒæ¨¡å¼")
            for param_group in agent.policy_optimizer.param_groups:
                param_group['lr'] *= 0.6
            for param_group in agent.value_optimizer.param_groups:
                param_group['lr'] *= 0.8
        
        # è®­ç»ƒä¸€ä¸ªepisode
        reward, steps, success = agent.train_episode(episode)
        
        agent.episode_rewards.append(reward)
        agent.episode_steps.append(steps)
        success_window.append(1 if success else 0)
        reward_window.append(reward)
        
        current_success_rate = np.mean(success_window)
        agent.success_rate.append(current_success_rate)
        
        # æ›´ç¨³å®šçš„æ€§èƒ½ä¿æŠ¤æœºåˆ¶
        if episode >= 50:
            performance_window.append(current_success_rate)
            
            # å‘ç°æ–°çš„æœ€ä½³æ€§èƒ½æ—¶ä¿å­˜æ¨¡å‹
            if current_success_rate > agent.best_success_rate + 0.01:  # éœ€è¦æ˜æ˜¾æ”¹è¿›
                agent.best_success_rate = current_success_rate
                agent.best_model_state = {
                    'policy_net': agent.policy_net.state_dict().copy(),
                    'value_net': agent.value_net.state_dict().copy(),
                    'episode': episode,
                    'success_rate': current_success_rate
                }
                agent.patience = 0
                agent.no_improvement_count = 0
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: Episode {episode+1}, æˆåŠŸç‡={current_success_rate:.3f}")
            else:
                agent.patience += 1
                agent.no_improvement_count += 1
            
            # æ›´ä¿å®ˆçš„æ€§èƒ½é€€åŒ–æ£€æµ‹
            if len(performance_window) == 30:
                recent_performance = np.mean(list(performance_window)[-15:])
                early_performance = np.mean(list(performance_window)[:15])
                
                # åªæœ‰æ˜æ˜¾é€€åŒ–æ‰æ¢å¤
                if (recent_performance < early_performance * 0.6 and 
                    agent.patience > agent.max_patience and 
                    agent.best_model_state):
                    
                    print(f"\nâš ï¸ æ£€æµ‹åˆ°ä¸¥é‡æ€§èƒ½é€€åŒ–ï¼")
                    print(f"   æ—©æœŸæ€§èƒ½: {early_performance:.3f}")
                    print(f"   æœ€è¿‘æ€§èƒ½: {recent_performance:.3f}")
                    print(f"   æ¢å¤æœ€ä½³æ¨¡å‹...")
                    
                    agent.policy_net.load_state_dict(agent.best_model_state['policy_net'])
                    agent.value_net.load_state_dict(agent.best_model_state['value_net'])
                    print(f"   å·²æ¢å¤Episode {agent.best_model_state['episode']+1}çš„æ¨¡å‹")
                    
                    # é€‚åº¦é‡ç½®æ¢ç´¢ç‡
                    agent.epsilon = min(0.2, agent.epsilon * 1.2)
                    print(f"   é€‚åº¦é‡ç½®æ¢ç´¢ç‡è‡³{agent.epsilon:.3f}")
                    agent.patience = 0
                    agent.no_improvement_count = 0
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(reward_window)
            avg_steps = np.mean(agent.episode_steps[-25:])
            
            recent_policy_loss = np.mean(agent.policy_losses[-10:]) if agent.policy_losses else 0
            recent_value_loss = np.mean(agent.value_losses[-10:]) if agent.value_losses else 0
            
            stage_name = "é˜¶æ®µ1" if episode < stage1_episodes else "é˜¶æ®µ2" if episode < stage1_episodes + stage2_episodes else "é˜¶æ®µ3"
            
            print(f"{stage_name} Episode {episode + 1:4d}: "
                  f"å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={avg_steps:5.1f}, "
                  f"æˆåŠŸç‡={current_success_rate:.3f}, Îµ={agent.epsilon:.3f}")
            print(f"                     ç­–ç•¥æŸå¤±={recent_policy_loss:.4f}, "
                  f"ä»·å€¼æŸå¤±={recent_value_loss:.4f}")
            print(f"                     æœ€ä½³æˆåŠŸç‡={agent.best_success_rate:.3f}, è€å¿ƒ={agent.patience}")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\n=== è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆè¯„ä¼° ===")
    final_success = np.mean(agent.success_rate[-100:]) if len(agent.success_rate) >= 100 else 0
    print(f"æœ€ç»ˆ100å›åˆæˆåŠŸç‡: {final_success:.3f}")
    print(f"å†å²æœ€ä½³æˆåŠŸç‡: {agent.best_success_rate:.3f}")
    
    # å¦‚æœæœ€ç»ˆæ€§èƒ½ä¸å¦‚å†å²æœ€ä½³ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹
    if agent.best_model_state and final_success < agent.best_success_rate * 0.8:
        print(f"\nğŸ”„ æœ€ç»ˆæ€§èƒ½ä¸å¦‚å†å²æœ€ä½³ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
        agent.policy_net.load_state_dict(agent.best_model_state['policy_net'])
        agent.value_net.load_state_dict(agent.best_model_state['value_net'])
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\n=== æœ€ç»ˆæµ‹è¯•ï¼ˆ50æ¬¡ï¼‰ ===")
    test_results = []
    for i in range(50):
        if (i + 1) % 10 == 0:
            print(f"æµ‹è¯•è¿›åº¦: {i+1}/50")
        reward, steps, path, success = agent.test_episode()
        test_results.append((reward, steps, success))
    
    final_success_rate = np.mean([r[2] for r in test_results])
    final_avg_reward = np.mean([r[0] for r in test_results])
    final_avg_steps = np.mean([r[1] for r in test_results])
    
    print(f"\nğŸ“Š ä¼˜åŒ–REINFORCEæœ€ç»ˆç»“æœ:")
    print(f"  æˆåŠŸç‡: {final_success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {final_avg_reward:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {final_avg_steps:.1f}")
    print(f"  å†å²æœ€ä½³è®­ç»ƒæˆåŠŸç‡: {agent.best_success_rate:.1%}")
    
    # ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”
    print(f"\nğŸ† ç®—æ³•æ€§èƒ½å¯¹æ¯”:")
    print(f"  åŸç‰ˆPPOæˆåŠŸç‡: 10%")
    print(f"  ä¼˜åŒ–PPOæˆåŠŸç‡: 26%")
    print(f"  Actor-CriticæˆåŠŸç‡: 60%+")
    print(f"  åŸç‰ˆREINFORCEæˆåŠŸç‡: 0%")
    print(f"  ä¼˜åŒ–REINFORCEæˆåŠŸç‡: {final_success_rate:.1%}")
    
    if final_success_rate > 0.5:
        print("ğŸ‰ ä¼˜åŒ–REINFORCEè¡¨ç°ä¼˜ç§€ï¼æ¥è¿‘Actor-Criticæ°´å¹³")
    elif final_success_rate > 0.3:
        print("âœ… ä¼˜åŒ–REINFORCEè¡¨ç°è‰¯å¥½ï¼Œæ˜¾è‘—è¶…è¶ŠPPOç®—æ³•")
    elif final_success_rate > 0.2:
        print("ğŸ‘ ä¼˜åŒ–REINFORCEè¡¨ç°ä¸é”™ï¼Œè¶…è¶ŠåŸç‰ˆPPO")
    elif final_success_rate > 0.05:
        print("âš¡ ä¼˜åŒ–REINFORCEæœ‰æ˜æ˜¾æ”¹è¿›ï¼Œä½†ä»éœ€ç»§ç»­è°ƒä¼˜")
    else:
        print("âš ï¸ ä¼˜åŒ–REINFORCEä»éœ€æ›´å¤šæ”¹è¿›")
    
    # ä¿å­˜æ¨¡å‹
    agent.save_model("models/optimized_reinforce_model.pth")
    print(f"ğŸ’¾ ä¼˜åŒ–REINFORCEæ¨¡å‹å·²ä¿å­˜")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_optimized_reinforce_curves(agent)
    
    return agent, test_results


def plot_optimized_reinforce_curves(agent):
    """ç»˜åˆ¶ä¼˜åŒ–REINFORCEè®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æˆåŠŸç‡æ›²çº¿
    if agent.success_rate:
        axes[0, 0].plot(agent.success_rate, label='Success Rate', color='purple', linewidth=2)
        axes[0, 0].axhline(y=agent.best_success_rate, color='red', linestyle='--', 
                          label=f'Best: {agent.best_success_rate:.3f}')
        axes[0, 0].set_title('Success Rate (Optimized REINFORCE)')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Success Rate')
        axes[0, 0].grid(True)
        axes[0, 0].legend()
    
    # å¥–åŠ±æ›²çº¿
    if agent.episode_rewards:
        window_size = 50
        if len(agent.episode_rewards) > window_size:
            moving_avg = np.convolve(agent.episode_rewards, 
                                   np.ones(window_size)/window_size, mode='valid')
            axes[0, 1].plot(moving_avg, label='Moving Average', color='blue', linewidth=2)
        axes[0, 1].plot(agent.episode_rewards, alpha=0.3, label='Raw Rewards', color='lightblue')
        axes[0, 1].set_title('Episode Rewards (Optimized REINFORCE)')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Reward')
        axes[0, 1].grid(True)
        axes[0, 1].legend()
    
    # ç­–ç•¥æŸå¤±
    if agent.policy_losses:
        axes[1, 0].plot(agent.policy_losses, label='Policy Loss', color='red', alpha=0.7)
        axes[1, 0].set_title('Policy Loss (Optimized REINFORCE)')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].grid(True)
        axes[1, 0].legend()
    
    # ä»·å€¼æŸå¤±
    if agent.value_losses:
        axes[1, 1].plot(agent.value_losses, label='Value Loss (Baseline)', color='orange', alpha=0.7)
        axes[1, 1].set_title('Value Loss (Optimized REINFORCE)')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].grid(True)
        axes[1, 1].legend()
    
    plt.tight_layout()
    plt.savefig('optimized_reinforce_curves.png', dpi=300, bbox_inches='tight')
    plt.show()


# ä¿ç•™åŸç‰ˆè®­ç»ƒå‡½æ•°ç”¨äºå¯¹æ¯”
def main_reinforce_training():
    """
    åŸç‰ˆREINFORCEè®­ç»ƒå‡½æ•°ï¼ˆä¿ç•™ç”¨äºå¯¹æ¯”ï¼‰
    """
    print("=== åŸç‰ˆREINFORCEè®­ç»ƒï¼ˆå¯¹æ¯”ç”¨ï¼‰ ===")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºåŸç‰ˆREINFORCEæ™ºèƒ½ä½“
    class OriginalREINFORCEAgent:
        def __init__(self, env, lr_policy=3e-4, lr_value=1e-3, gamma=0.99, hidden_dim=128):
            # åŸç‰ˆå®ç°...
            pass
    
    # è¿™é‡Œå¯ä»¥ä¿ç•™åŸç‰ˆå®ç°ç”¨äºå¯¹æ¯”...
    print("åŸç‰ˆREINFORCEä¿ç•™ç”¨äºæ€§èƒ½å¯¹æ¯”")


def debug_trained_model():
    """
    è°ƒè¯•å·²è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œæ£€æŸ¥è®­ç»ƒæµ‹è¯•å·®å¼‚
    """
    print("=== è°ƒè¯•å·²è®­ç»ƒæ¨¡å‹ ===")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = OptimizedREINFORCEAgent(env=env)
    
    # å°è¯•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹
    try:
        agent.load_model("models/optimized_reinforce_model.pth")
        print("âœ… æˆåŠŸåŠ è½½æ¨¡å‹")
    except:
        print("âŒ æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    print(f"å½“å‰epsilon: {agent.epsilon}")
    
    # æµ‹è¯•æ›´å¤šepisodeè·å¾—å¯é ç»Ÿè®¡
    print("\n=== è¯¦ç»†æµ‹è¯• ===")
    test_results = []
    detailed_episodes = 3  # å‰3ä¸ªepisodeæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    total_episodes = 20    # æ€»å…±æµ‹è¯•20æ¬¡
    
    for i in range(total_episodes):
        show_debug = i < detailed_episodes
        if show_debug:
            print(f"\n--- æµ‹è¯•Episode {i+1} (è¯¦ç»†) ---")
        
        reward, steps, path, success = agent.test_episode(debug=show_debug)
        test_results.append((reward, steps, success))
        
        if show_debug:
            print(f"Episode {i+1}: å¥–åŠ±={reward:.1f}, æ­¥æ•°={steps}, æˆåŠŸ={success}")
            if success:
                print("ğŸ‰ æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼")
            elif steps >= 200:
                print("âŒ è¶…è¿‡æœ€å¤§æ­¥æ•°é™åˆ¶")
            else:
                print("ğŸ’¥ å‘ç”Ÿç¢°æ’")
        else:
            # ç®€åŒ–æ˜¾ç¤º
            status = "âœ…æˆåŠŸ" if success else f"âŒå¤±è´¥({steps}æ­¥)"
            print(f"Episode {i+1:2d}: {status}")
    
    # è¯¦ç»†ç»Ÿè®¡åˆ†æ
    success_count = sum([r[2] for r in test_results])
    success_rate = success_count / len(test_results) * 100
    
    successful_episodes = [r for r in test_results if r[2]]
    failed_episodes = [r for r in test_results if not r[2]]
    
    print(f"\n=== æµ‹è¯•ç»Ÿè®¡ ({len(test_results)}æ¬¡æµ‹è¯•) ===")
    print(f"ğŸ¯ æˆåŠŸç‡: {success_count}/{len(test_results)} = {success_rate:.1f}%")
    
    if successful_episodes:
        avg_steps_success = sum([r[1] for r in successful_episodes]) / len(successful_episodes)
        min_steps = min([r[1] for r in successful_episodes])
        max_steps = max([r[1] for r in successful_episodes])
        print(f"âœ… æˆåŠŸepisode: å¹³å‡{avg_steps_success:.1f}æ­¥ (èŒƒå›´: {min_steps}-{max_steps}æ­¥)")
    
    if failed_episodes:
        timeout_count = sum([1 for r in failed_episodes if r[1] >= 200])
        collision_count = len(failed_episodes) - timeout_count
        print(f"âŒ å¤±è´¥episode: {timeout_count}æ¬¡è¶…æ—¶, {collision_count}æ¬¡ç¢°æ’")
    
    # å¥–åŠ±ç»Ÿè®¡
    rewards = [r[0] for r in test_results]
    avg_reward = sum(rewards) / len(rewards)
    print(f"ğŸ’° å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
    
    print(f"ğŸ“Š è¯¦ç»†ç»“æœ: {[1 if r[2] else 0 for r in test_results]}")
    
    # æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•çš„åŠ¨ä½œé€‰æ‹©
    print("\n=== åŠ¨ä½œé€‰æ‹©å¯¹æ¯” ===")
    state = env.reset()
    print(f"æµ‹è¯•çŠ¶æ€: {state}")
    
    # è®­ç»ƒæ¨¡å¼åŠ¨ä½œé€‰æ‹©
    action_train, log_prob_train = agent.select_action(state, training=True)
    print(f"è®­ç»ƒæ¨¡å¼åŠ¨ä½œ: {action_train}, log_prob: {log_prob_train:.4f}")
    
    # æµ‹è¯•æ¨¡å¼åŠ¨ä½œé€‰æ‹©
    action_test, log_prob_test = agent.select_action(state, training=False)
    print(f"æµ‹è¯•æ¨¡å¼åŠ¨ä½œ: {action_test}, log_prob: {log_prob_test:.4f}")
    
    if action_train != action_test:
        print("âš ï¸ è®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼é€‰æ‹©ä¸åŒçš„åŠ¨ä½œï¼")
    else:
        print("âœ… è®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼é€‰æ‹©ç›¸åŒåŠ¨ä½œ")
    
    # æ£€æŸ¥åŠ¨ä½œæ©ç 
    state_tensor = agent.state_to_tensor(state)
    action_probs = agent.policy_net(state_tensor)
    masked_probs = agent._apply_strict_action_mask(state, action_probs)
    
    print(f"åŸå§‹åŠ¨ä½œæ¦‚ç‡: {action_probs.detach().numpy()}")
    print(f"æ©ç åæ¦‚ç‡: {masked_probs.detach().numpy()}")
    print(f"æœ‰æ•ˆåŠ¨ä½œæ•°: {(masked_probs > 0).sum().item()}")
    
    return agent, test_results


def test_saved_reinforce_model(model_path: str = "models/optimized_reinforce_model.pth", 
                              test_episodes: int = 50, 
                              show_visualization: bool = True,
                              show_detailed: bool = False):
    """
    æµ‹è¯•ä¿å­˜çš„REINFORCEæ¨¡å‹æ•ˆæœ
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        test_episodes: æµ‹è¯•å›åˆæ•°
        show_visualization: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ
        show_detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        dict: æµ‹è¯•ç»“æœç»Ÿè®¡
    """
    print("=" * 60)
    print("ğŸ§ª REINFORCEæ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # 1. åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    agent = OptimizedREINFORCEAgent(env=env)
    
    # 2. åŠ è½½æ¨¡å‹
    try:
        agent.load_model(model_path)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None
    
    # 3. è¿è¡Œæµ‹è¯•
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• ({test_episodes} å›åˆ)...")
    
    test_results = []
    successful_paths = []
    
    for i in range(test_episodes):
        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 10 == 0 or i == 0:
            print(f"è¿›åº¦: {i+1}/{test_episodes}")
        
        # è¿è¡Œæµ‹è¯•å›åˆ
        reward, steps, path, success = agent.test_episode(render=False, debug=show_detailed and i < 3)
        
        test_results.append({
            'episode': i + 1,
            'reward': reward,
            'steps': steps,
            'success': success,
            'path': path
        })
        
        if success:
            successful_paths.append(path)
    
    # 4. ç»Ÿè®¡åˆ†æ
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 50)
    
    # åŸºç¡€ç»Ÿè®¡
    total_episodes = len(test_results)
    successful_episodes = [r for r in test_results if r['success']]
    failed_episodes = [r for r in test_results if not r['success']]
    
    success_count = len(successful_episodes)
    success_rate = success_count / total_episodes * 100
    
    print(f"ğŸ¯ æ€»æµ‹è¯•å›åˆ: {total_episodes}")
    print(f"âœ… æˆåŠŸå›åˆ: {success_count}")
    print(f"âŒ å¤±è´¥å›åˆ: {len(failed_episodes)}")
    print(f"ğŸ† æˆåŠŸç‡: {success_rate:.1f}%")
    
    # å¥–åŠ±ç»Ÿè®¡
    all_rewards = [r['reward'] for r in test_results]
    avg_reward = np.mean(all_rewards)
    std_reward = np.std(all_rewards)
    
    print(f"\nğŸ’° å¥–åŠ±ç»Ÿè®¡:")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
    print(f"  å¥–åŠ±æ ‡å‡†å·®: {std_reward:.1f}")
    print(f"  æœ€é«˜å¥–åŠ±: {max(all_rewards):.1f}")
    print(f"  æœ€ä½å¥–åŠ±: {min(all_rewards):.1f}")
    
    # æ­¥æ•°ç»Ÿè®¡
    all_steps = [r['steps'] for r in test_results]
    avg_steps = np.mean(all_steps)
    
    print(f"\nğŸ‘£ æ­¥æ•°ç»Ÿè®¡:")
    print(f"  å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
    
    if successful_episodes:
        success_steps = [r['steps'] for r in successful_episodes]
        avg_success_steps = np.mean(success_steps)
        min_success_steps = min(success_steps)
        max_success_steps = max(success_steps)
        
        print(f"  æˆåŠŸå›åˆå¹³å‡æ­¥æ•°: {avg_success_steps:.1f}")
        print(f"  æœ€å°‘æˆåŠŸæ­¥æ•°: {min_success_steps}")
        print(f"  æœ€å¤šæˆåŠŸæ­¥æ•°: {max_success_steps}")
    
    # å¤±è´¥åŸå› åˆ†æ
    if failed_episodes:
        timeout_episodes = [r for r in failed_episodes if r['steps'] >= 200]
        collision_episodes = [r for r in failed_episodes if r['steps'] < 200]
        
        print(f"\nâš ï¸ å¤±è´¥åŸå› åˆ†æ:")
        print(f"  è¶…æ—¶å¤±è´¥: {len(timeout_episodes)} å›åˆ ({len(timeout_episodes)/total_episodes*100:.1f}%)")
        print(f"  ç¢°æ’å¤±è´¥: {len(collision_episodes)} å›åˆ ({len(collision_episodes)/total_episodes*100:.1f}%)")
    
    # æ€§èƒ½è¯„çº§
    print(f"\nğŸ… æ€§èƒ½è¯„çº§:")
    if success_rate >= 60:
        rating = "ğŸ¥‡ ä¼˜ç§€ (â‰¥60%)"
    elif success_rate >= 40:
        rating = "ğŸ¥ˆ è‰¯å¥½ (40-59%)"
    elif success_rate >= 20:
        rating = "ğŸ¥‰ ä¸€èˆ¬ (20-39%)"
    elif success_rate >= 10:
        rating = "âš¡ è¾ƒå·® (10-19%)"
    else:
        rating = "âŒ å¾ˆå·® (<10%)"
    
    print(f"  {rating}")
    
    # ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”
    print(f"\nğŸ” ç®—æ³•å¯¹æ¯”:")
    print(f"  åŸç‰ˆPPO: ~10%")
    print(f"  ä¼˜åŒ–PPO: ~26%") 
    print(f"  Actor-Critic: ~60%")
    print(f"  å½“å‰REINFORCE: {success_rate:.1f}%")
    
    if success_rate > 60:
        print("  ğŸ‰ è¡¨ç°ä¼˜ç§€ï¼è¾¾åˆ°æˆ–è¶…è¿‡Actor-Criticæ°´å¹³")
    elif success_rate > 26:
        print("  âœ… è¡¨ç°è‰¯å¥½ï¼è¶…è¿‡äº†ä¼˜åŒ–PPOç®—æ³•")
    elif success_rate > 10:
        print("  ğŸ‘ è¡¨ç°ä¸é”™ï¼è¶…è¿‡äº†åŸç‰ˆPPOç®—æ³•")
    else:
        print("  ğŸ“ˆ è¿˜æœ‰æ”¹è¿›ç©ºé—´")
    
    # 5. å¯è§†åŒ–ç»“æœ
    if show_visualization and successful_paths:
        print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        
        # éšæœºé€‰æ‹©å‡ æ¡æˆåŠŸè·¯å¾„è¿›è¡Œå¯è§†åŒ–
        num_paths_to_show = min(3, len(successful_paths))
        selected_paths = np.random.choice(len(successful_paths), num_paths_to_show, replace=False)
        
        for i, path_idx in enumerate(selected_paths):
            print(f"æ˜¾ç¤ºæˆåŠŸè·¯å¾„ {i+1}/{num_paths_to_show}")
            env.render(show_path=successful_paths[path_idx])
    
    # 6. è¯¦ç»†ç»“æœå±•ç¤º
    if show_detailed:
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ (å‰10å›åˆ):")
        for i, result in enumerate(test_results[:10]):
            status = "âœ…æˆåŠŸ" if result['success'] else "âŒå¤±è´¥"
            print(f"  ç¬¬{result['episode']:2d}å›åˆ: {status} | "
                  f"å¥–åŠ±={result['reward']:6.1f} | æ­¥æ•°={result['steps']:3d}")
    
    # 7. è¿”å›ç»Ÿè®¡ç»“æœ
    summary = {
        'total_episodes': total_episodes,
        'success_count': success_count,
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_steps': avg_steps,
        'successful_paths': successful_paths,
        'test_results': test_results
    }
    
    print(f"\nâœ¨ æµ‹è¯•å®Œæˆï¼")
    return summary


def quick_test_reinforce():
    """
    å¿«é€Ÿæµ‹è¯•REINFORCEæ¨¡å‹ï¼ˆ10å›åˆï¼‰
    """
    print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (10å›åˆ)")
    return test_saved_reinforce_model(
        test_episodes=10, 
        show_visualization=False,
        show_detailed=True
    )


def comprehensive_test_reinforce():
    """
    å…¨é¢æµ‹è¯•REINFORCEæ¨¡å‹ï¼ˆ100å›åˆï¼‰
    """
    print("ğŸ”¬ å…¨é¢æµ‹è¯•æ¨¡å¼ (100å›åˆ)")
    return test_saved_reinforce_model(
        test_episodes=100,
        show_visualization=True,
        show_detailed=False
    )


if __name__ == "__main__":
    # é¦–å…ˆè°ƒè¯•å·²è®­ç»ƒçš„æ¨¡å‹
    print("ğŸ” é¦–å…ˆè°ƒè¯•å·²è®­ç»ƒæ¨¡å‹ï¼Œæ£€æŸ¥è®­ç»ƒæµ‹è¯•å·®å¼‚...")
    debug_trained_model()
    
    print("\n" + "="*50)
    # è¿è¡Œç»ˆæä¼˜åŒ–çš„REINFORCEè®­ç»ƒ
    # main_ultimate_reinforce_training()
    
    # è¿è¡Œå¿«é€Ÿæµ‹è¯•
    print("\n" + "="*50)
    print("ğŸ§ª è¿è¡Œæ¨¡å‹æµ‹è¯•...")
    test_saved_reinforce_model()
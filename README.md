# 赛道强化学习算法比较

本项目实现了经典的"赛道（Racetrack）"强化学习问题，并比较了三种不同强化学习算法的性能：

1. **Sarsa(λ)** - 带资格迹的时序差分方法
2. **Q-learning** - 离策略时序差分方法  
3. **REINFORCE** - 策略梯度方法

## 项目结构

```
├── racetrack_env.py     # 赛道环境实现
├── sarsa_lambda.py      # Sarsa(λ) 算法
├── q_learning.py        # Q-learning 算法
├── reinforce.py         # REINFORCE 算法
├── main.py              # 主训练和比较脚本
├── requirements.txt     # Python依赖
└── README.md           # 项目说明
```

## 问题描述

赛道问题是一个经典的强化学习基准问题：

- **环境**: 一个L型赛道，智能体需要从起点到达终点
- **状态**: `(x, y, vx, vy)` - 位置和速度
- **动作**: `(ax, ay)` - 加速度，其中 `ax, ay ∈ {-1, 0, 1}`
- **目标**: 学习最快完成赛道的策略（最少步数）

### 奖励机制

- 每一步: -1 （鼓励尽快完成）
- 碰撞/越界: -100 （重置到起点）  
- 到达终点: 0 （结束回合）

## 算法特点

### Sarsa(λ)

- **类型**: 在策略时序差分学习
- **特点**: 使用资格迹加速学习，更新当前策略下的价值
- **优势**: 相对保守，学习稳定
- **资格迹**: λ 参数控制远程时间依赖性

### Q-learning

- **类型**: 离策略时序差分学习
- **特点**: 直接学习最优价值函数，与行为策略无关
- **优势**: 理论上能找到最优策略
- **更新**: 使用 max Q(s',a') 进行更新

### REINFORCE

- **类型**: 蒙特卡洛策略梯度方法
- **特点**: 直接优化参数化策略
- **优势**: 能处理连续动作空间，策略更加平滑
- **网络**: 使用神经网络参数化策略

## 安装和运行

1. **安装依赖**:
   ```bash
   pip install -r requirements.txt
   ```

2. **运行实验**:
   ```bash
   python main.py
   ```

3. **输出文件**:
   - `algorithm_comparison.png` - 算法性能比较图
   - `learned_paths.png` - 学习到的路径可视化
   - `results.pkl` - 详细实验结果

## 实验设置

- **训练回合数**: 2000
- **赛道大小**: 20×15
- **最大速度**: 3
- **评估回合数**: 100

### 超参数

| 算法       | 学习率 | 折扣因子 | 其他参数     |
| ---------- | ------ | -------- | ------------ |
| Sarsa(λ)   | 0.1    | 0.95     | λ=0.9, ε=0.1 |
| Q-learning | 0.1    | 0.95     | ε=0.1        |
| REINFORCE  | 0.001  | 0.95     | 隐藏层=128   |

## 性能指标

1. **收敛速度** - 达到稳定性能所需的训练回合数
2. **平均步数** - 完成任务的平均步数（越少越好）
3. **奖励** - 平均累积奖励（越高越好）
4. **稳定性** - 性能的标准差（越小越稳定）

## 预期结果

### 收敛特性
- **Sarsa(λ)**: 通常收敛较快且稳定，得益于资格迹
- **Q-learning**: 可能需要更多探索，但最终性能可能更好
- **REINFORCE**: 初期收敛较慢，但策略更平滑

### 最终性能
- **步数**: Q-learning 通常能找到最短路径
- **稳定性**: Sarsa(λ) 通常更稳定
- **策略平滑性**: REINFORCE 策略更自然

## 技术细节

### 状态空间
- 位置: (x, y) ∈ [0, track_size]
- 速度: (vx, vy) ∈ [-max_speed, max_speed]
- 总状态数: ~数万个状态

### 函数逼近
- **表格方法**: Sarsa(λ), Q-learning 使用稀疏字典
- **神经网络**: REINFORCE 使用2层全连接网络

### 探索策略
- **ε-greedy**: Sarsa(λ), Q-learning
- **随机策略**: REINFORCE 天然具有探索性

## 扩展方向

1. **Actor-Critic**: 结合值函数和策略梯度
2. **Deep Q-Network (DQN)**: 使用神经网络逼近Q函数
3. **PPO/A3C**: 更先进的策略梯度方法
4. **多目标优化**: 同时优化速度和安全性
5. **层次强化学习**: 分解为子任务

## 注意事项

- 由于状态空间较大，表格方法可能需要大量内存
- 神经网络方法可能需要更多训练时间
- 不同随机种子可能导致结果差异
- 超参数对性能影响显著

## 引用

如果使用本代码，请引用经典的强化学习文献：

```
Sutton, R. S., & Barto, A. G. (2018). 
Reinforcement learning: An introduction. 
MIT press.
``` 
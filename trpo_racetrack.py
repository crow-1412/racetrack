"""
TRPO (Trust Region Policy Optimization) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - èµ›è½¦è½¨é“é—®é¢˜

TRPOç®—æ³•ç‰¹ç‚¹ï¼š
1. ä¿¡ä»»åŒºåŸŸçº¦æŸ - é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œç¡®ä¿è®­ç»ƒç¨³å®š
2. å…±è½­æ¢¯åº¦æ³• - é«˜æ•ˆæ±‚è§£çº¦æŸä¼˜åŒ–é—®é¢˜
3. çº¿æœç´¢æœºåˆ¶ - è‡ªé€‚åº”è°ƒæ•´æ­¥é•¿
4. KLæ•£åº¦çº¦æŸ - ç²¾ç¡®æ§åˆ¶ç­–ç•¥å˜åŒ–
5. GAEä¼˜åŠ¿ä¼°è®¡ - å‡å°‘æ–¹å·®æé«˜ç¨³å®šæ€§

ä½œè€…ï¼šYuJinYue
æœ€åæ›´æ–°ï¼š2025å¹´6æœˆ19æ—¥
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Tuple, List, Dict, Optional
import random
from collections import deque
import matplotlib.pyplot as plt
from racetrack_env import RacetrackEnv

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print(f"ğŸ² TRPOéšæœºç§å­å·²è®¾ç½®ä¸º: {RANDOM_SEED}")


class TRPONetwork(nn.Module):
    """
    TRPOç½‘ç»œæ¶æ„ - åˆ†ç¦»çš„Actor-Critic
    
    ç‰¹ç‚¹ï¼š
    - åˆ†ç¦»çš„ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ
    - é’ˆå¯¹èµ›è½¦è½¨é“é—®é¢˜ä¼˜åŒ–çš„ç‰¹å¾æå–
    - é€‚åˆç¦»æ•£åŠ¨ä½œç©ºé—´çš„è¾“å‡ºå±‚
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(TRPONetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        
        # Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # Criticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Xavieråˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        features = self.feature_extractor(state)
        action_logits = self.actor(features)
        value = self.critic(features)
        return action_logits, value
    
    def get_action_probs(self, state):
        """è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
        action_logits, _ = self.forward(state)
        return F.softmax(action_logits, dim=-1)
    
    def get_value(self, state):
        """è·å–çŠ¶æ€ä»·å€¼"""
        _, value = self.forward(state)
        return value


class TRPOBuffer:
    """TRPOç»éªŒç¼“å†²åŒº"""
    
    def __init__(self):
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.advantages = []
        self.returns = []
    
    def add(self, state, action, reward, value, log_prob, done):
        """æ·»åŠ ç»éªŒ"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)
    
    def size(self):
        """è·å–ç¼“å†²åŒºå¤§å°"""
        return len(self.states)
    
    def compute_gae(self, gamma: float, gae_lambda: float, next_value: float = 0):
        """
        è®¡ç®—GAEï¼ˆGeneralized Advantage Estimationï¼‰
        """
        rewards = np.array(self.rewards, dtype=np.float32)
        values = np.array([v.detach().item() if isinstance(v, torch.Tensor) else v 
                          for v in self.values], dtype=np.float32)
        dones = np.array(self.dones, dtype=np.bool_)
        
        # å¥–åŠ±è£å‰ªï¼Œé¿å…å¼‚å¸¸å€¼
        rewards = np.clip(rewards, -50, 50)
        
        # GAEè®¡ç®—
        advantages = np.zeros_like(rewards)
        last_gae = 0.0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - float(dones[t])
                next_value_t = next_value
            else:
                next_non_terminal = 1.0 - float(dones[t])
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * next_non_terminal - values[t]
            advantages[t] = last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae
        
        # è®¡ç®—returns
        returns = advantages + values
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if np.any(np.isnan(advantages)) or np.any(np.isinf(advantages)):
            print("âš ï¸ GAEè®¡ç®—å¼‚å¸¸ï¼Œä½¿ç”¨ç®€å•ä¼˜åŠ¿ä¼°è®¡")
            advantages = rewards - values
            returns = rewards.copy()
        
        self.advantages = advantages.tolist()
        self.returns = returns.tolist()
        
        return torch.tensor(advantages, dtype=torch.float32), torch.tensor(returns, dtype=torch.float32)
    
    def get_tensors(self):
        """è·å–å¼ é‡æ ¼å¼çš„æ•°æ®"""
        states = torch.stack(self.states)
        actions = torch.tensor(self.actions, dtype=torch.long)
        old_log_probs = torch.stack(self.log_probs)
        advantages = torch.tensor(self.advantages, dtype=torch.float32)
        returns = torch.tensor(self.returns, dtype=torch.float32)
        
        return states, actions, old_log_probs, advantages, returns


class TRPORacetrackAgent:
    """
    TRPOèµ›è½¦è½¨é“æ™ºèƒ½ä½“
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. ä¿¡ä»»åŒºåŸŸçº¦æŸç¡®ä¿è®­ç»ƒç¨³å®š
    2. å…±è½­æ¢¯åº¦æ³•é«˜æ•ˆæ±‚è§£
    3. è‡ªé€‚åº”çº¿æœç´¢
    4. æ™ºèƒ½å¥–åŠ±å¡‘å½¢
    """
    
    def __init__(self, env: RacetrackEnv, gamma: float = 0.99, gae_lambda: float = 0.95,
                 max_kl: float = 0.075, damping: float = 0.008, cg_iters: int = 18,
                 value_lr: float = 4e-4, max_backtracks: int = 12, backtrack_coeff: float = 0.55):
        """
        åˆå§‹åŒ–TRPOæ™ºèƒ½ä½“
        
        Args:
            env: èµ›è½¦è½¨é“ç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
            gae_lambda: GAEå‚æ•°
            max_kl: ä¿¡ä»»åŒºåŸŸKLæ•£åº¦é™åˆ¶
            damping: å…±è½­æ¢¯åº¦é˜»å°¼
            cg_iters: å…±è½­æ¢¯åº¦è¿­ä»£æ¬¡æ•°
            value_lr: ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡
            max_backtracks: æœ€å¤§å›æº¯æ¬¡æ•°
            backtrack_coeff: å›æº¯ç³»æ•°
        """
        self.env = env
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.max_kl = max_kl
        self.damping = damping
        self.cg_iters = cg_iters
        self.max_backtracks = max_backtracks
        self.backtrack_coeff = backtrack_coeff
        
        # ç½‘ç»œé…ç½®
        self.state_dim = 8  # ç‰¹å¾ç»´åº¦
        self.action_dim = env.n_actions
        
        # åˆ›å»ºç½‘ç»œ
        self.network = TRPONetwork(self.state_dim, self.action_dim)
        
        # ä»·å€¼ç½‘ç»œä¼˜åŒ–å™¨
        self.value_optimizer = optim.Adam(self.network.critic.parameters(), lr=value_lr)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = TRPOBuffer()
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        self.success_rate: List[float] = []
        self.policy_losses: List[float] = []
        self.value_losses: List[float] = []
        self.kl_divergences: List[float] = []
        self.step_sizes: List[float] = []
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤
        self.best_success_rate = 0.0
        self.best_model_state = None
        
        # å¥–åŠ±å¡‘å½¢å‚æ•°
        self.last_distance_to_goal = None
        self.progress_reward_scale = 0.2  # å¢åŠ å‰è¿›å¥–åŠ±
    
    def state_to_tensor(self, state: Tuple[int, int, int, int]) -> torch.Tensor:
        """
        çŠ¶æ€ç‰¹å¾æå– - é’ˆå¯¹èµ›è½¦è½¨é“é—®é¢˜ä¼˜åŒ–
        """
        x, y, vx, vy = state
        
        # åŸºç¡€ç‰¹å¾å½’ä¸€åŒ–
        norm_x = x / 31.0
        norm_y = y / 16.0
        norm_vx = vx / self.env.max_speed
        norm_vy = vy / self.env.max_speed
        
        # è®¡ç®—åˆ°æœ€è¿‘ç»ˆç‚¹çš„è·ç¦»å’Œæ–¹å‘
        min_distance = float('inf')
        goal_direction_x, goal_direction_y = 0, 0
        
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            if distance < min_distance:
                min_distance = distance
                if distance > 0:
                    goal_direction_x = -(goal_x - x) / distance
                    goal_direction_y = (goal_y - y) / distance
        
        # è·ç¦»å½’ä¸€åŒ–
        max_distance = np.sqrt(31**2 + 16**2)
        norm_distance = min_distance / max_distance
        
        # é€Ÿåº¦ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦
        velocity_alignment = 0.0
        if min_distance > 0:
            velocity_mag = np.sqrt(vx**2 + vy**2)
            if velocity_mag > 0:
                vel_dir_x = vx / velocity_mag
                vel_dir_y = vy / velocity_mag
                velocity_alignment = max(0, vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y)
        
        return torch.tensor([
            norm_x, norm_y, norm_vx, norm_vy,
            norm_distance, goal_direction_x, goal_direction_y,
            velocity_alignment
        ], dtype=torch.float32)
    
    def apply_action_mask(self, state: Tuple[int, int, int, int], 
                         action_logits: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨åŠ¨ä½œæ©ç ï¼Œé˜²æ­¢ç¢°æ’"""
        x, y, vx, vy = state
        mask = torch.zeros_like(action_logits)
        
        for i, (ax, ay) in enumerate(self.env.actions):
            new_vx = max(0, min(self.env.max_speed, vx + ax))
            new_vy = max(0, min(self.env.max_speed, vy + ay))
            
            # é˜²æ­¢é€Ÿåº¦ä¸ºé›¶ï¼ˆé™¤éåœ¨èµ·ç‚¹ï¼‰
            if new_vx == 0 and new_vy == 0 and (x, y) not in self.env.start_positions:
                new_vx = 1
                new_vy = 1
            
            new_x = x - new_vx
            new_y = y + new_vy
            
            # æ£€æŸ¥ç¢°æ’
            if self.env._check_collision(x, y, new_x, new_y):
                mask[i] = -1e9
        
        masked_logits = action_logits + mask
        
        # å¦‚æœæ‰€æœ‰åŠ¨ä½œéƒ½è¢«æ©ç ï¼Œå–æ¶ˆæ©ç 
        if torch.all(mask == -1e9):
            mask.fill_(0)
            masked_logits = action_logits
        
        return masked_logits
    
    def select_action(self, state: Tuple[int, int, int, int], training: bool = True):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = self.state_to_tensor(state)
        
        with torch.no_grad():
            action_logits, value = self.network(state_tensor)
            
            # åº”ç”¨åŠ¨ä½œæ©ç 
            masked_logits = self.apply_action_mask(state, action_logits)
            
            # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒ
            action_dist = Categorical(logits=masked_logits)
            
            # æ”¹è¿›ï¼šè®­ç»ƒæ—¶æ›´æ¿€è¿›ï¼Œæµ‹è¯•æ—¶æ›´ç¡®å®šæ€§
            if training:
                action = action_dist.sample()
            else:
                # æµ‹è¯•æ—¶ä½¿ç”¨å¹³è¡¡çš„ç­–ç•¥é€‰æ‹©
                temperature = 0.4  # å¹³è¡¡çš„æ¸©åº¦
                cooled_logits = masked_logits / temperature
                cooled_dist = Categorical(logits=cooled_logits)
                action = cooled_dist.sample()
            
            log_prob = action_dist.log_prob(action)
        
        return action.item(), log_prob, value.squeeze()
    
    def intelligent_reward_shaping(self, prev_state, state, next_state, reward, done, steps):
        """
        æ™ºèƒ½å¥–åŠ±å¡‘å½¢ - è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜
        """
        shaped_reward = reward
        
        x, y, vx, vy = state
        
        # 1. å‰è¿›å¥–åŠ±
        current_distance = float('inf')
        for goal_x, goal_y in self.env.goal_positions:
            distance = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
            current_distance = min(current_distance, distance)
        
        if self.last_distance_to_goal is not None:
            progress = self.last_distance_to_goal - current_distance
            if progress > 0:
                shaped_reward += progress * self.progress_reward_scale
            elif progress < -2:
                shaped_reward -= 0.05
        
        self.last_distance_to_goal = current_distance
        
        # 2. æ”¹è¿›çš„é€Ÿåº¦å¥–åŠ± - é¼“åŠ±é«˜é€Ÿåº¦
        speed = np.sqrt(vx**2 + vy**2)
        if speed >= 4:  # é¼“åŠ±æœ€é«˜é€Ÿåº¦
            shaped_reward += 0.08
        elif speed >= 3:  # é¼“åŠ±é«˜é€Ÿåº¦
            shaped_reward += 0.05
        elif speed >= 2:  # ä¸­ç­‰é€Ÿåº¦
            shaped_reward += 0.02
        elif speed == 0:  # æƒ©ç½šåœæ­¢
            shaped_reward -= 0.1
        
        # 3. æ–¹å‘å¥–åŠ±
        if current_distance > 0:
            goal_direction_x = -(self.env.goal_positions[0][0] - x) / current_distance
            goal_direction_y = (self.env.goal_positions[0][1] - y) / current_distance
            
            if speed > 0:
                vel_dir_x = vx / speed
                vel_dir_y = vy / speed
                alignment = vel_dir_x * goal_direction_x + vel_dir_y * goal_direction_y
                if alignment > 0.7:  # æ›´ä¸¥æ ¼çš„å¯¹é½è¦æ±‚
                    shaped_reward += 0.03  # æ›´å¤§çš„æ–¹å‘å¥–åŠ±
                elif alignment > 0.3:
                    shaped_reward += 0.01
        
        # 4. æ­¥æ•°æƒ©ç½š
        shaped_reward -= 0.01
        
        # 5. ç»“æŸçŠ¶æ€å¥–åŠ±è°ƒæ•´ - æ›´æ¿€è¿›çš„å¥–åŠ±
        if done:
            if reward == 100:  # æˆåŠŸ
                # æ ¹æ®æ­¥æ•°ç»™äºˆé¢å¤–å¥–åŠ±ï¼Œé¼“åŠ±å¿«é€Ÿå®Œæˆ
                if steps < 20:
                    shaped_reward += 50  # è¶…å¿«å®Œæˆ
                elif steps < 30:
                    shaped_reward += 35  # å¿«é€Ÿå®Œæˆ
                else:
                    shaped_reward += 25  # æ­£å¸¸å®Œæˆ
            elif reward == -10:  # ç¢°æ’
                shaped_reward -= 8
            else:  # è¶…æ—¶
                shaped_reward -= 5
        
        return shaped_reward
    
    def collect_trajectory(self, max_steps: int = 300) -> Tuple[float, int, bool]:
        """æ”¶é›†ä¸€æ¡å®Œæ•´è½¨è¿¹"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        self.last_distance_to_goal = None
        
        for _ in range(max_steps):
            action, log_prob, value = self.select_action(state, training=True)
            prev_state = state
            
            next_state, reward, done = self.env.step(action)
            
            # æ™ºèƒ½å¥–åŠ±å¡‘å½¢
            shaped_reward = self.intelligent_reward_shaping(
                prev_state, state, next_state, reward, done, steps
            )
            
            # å­˜å‚¨ç»éªŒ
            self.buffer.add(
                self.state_to_tensor(prev_state),
                action,
                shaped_reward,
                value,
                log_prob,
                done
            )
            
            total_reward += reward
            steps += 1
            
            if done:
                break
            
            state = next_state
        
        # è®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼
        if not done:
            _, _, next_value = self.select_action(state, training=True)
        else:
            next_value = 0.0
        
        # è®¡ç®—GAE
        self.buffer.compute_gae(self.gamma, self.gae_lambda, next_value)
        
        success = (done and reward == 100)
        return total_reward, steps, success
    
    def compute_kl_divergence(self, states, actions, old_log_probs):
        """è®¡ç®—KLæ•£åº¦"""
        action_logits, _ = self.network(states)
        new_dist = Categorical(logits=action_logits)
        new_log_probs = new_dist.log_prob(actions)
        
        # è®¡ç®—KLæ•£åº¦: KL(old||new) = old_log_prob - new_log_prob
        kl_div = (old_log_probs - new_log_probs).mean()
        return kl_div
    
    def compute_policy_gradient(self, states, actions, advantages, old_log_probs):
        """è®¡ç®—ç­–ç•¥æ¢¯åº¦"""
        action_logits, _ = self.network(states)
        action_dist = Categorical(logits=action_logits)
        log_probs = action_dist.log_prob(actions)
        
        # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        ratio = torch.exp(log_probs - old_log_probs)
        
        # ç­–ç•¥ç›®æ ‡ï¼ˆè¦æœ€å¤§åŒ–ï¼‰
        policy_loss = -(ratio * advantages).mean()
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        policy_grads = torch.autograd.grad(policy_loss, self.network.actor.parameters(), 
                                         create_graph=True, retain_graph=True)
        policy_grad = torch.cat([grad.view(-1) for grad in policy_grads])
        
        return policy_grad
    
    def compute_fisher_vector_product(self, states, vector):
        """è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µä¸å‘é‡çš„ä¹˜ç§¯ï¼ˆç®€åŒ–ç¨³å®šç‰ˆæœ¬ï¼‰"""
        # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œä½¿ç”¨ç®€åŒ–çš„Fisher-Vector Productè®¡ç®—
        # è¿™ç›¸å½“äºè®¡ç®— H*vï¼Œå…¶ä¸­Hæ˜¯HessiançŸ©é˜µçš„è¿‘ä¼¼
        
        action_logits, _ = self.network(states)
        action_dist = Categorical(logits=action_logits)
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡çš„å¹³å‡å€¼ä½œä¸ºç›®æ ‡å‡½æ•°
        avg_log_prob = action_dist.logits.mean()
        
        # è®¡ç®—ä¸€é˜¶æ¢¯åº¦
        first_grads = torch.autograd.grad(avg_log_prob, self.network.actor.parameters(), 
                                        create_graph=True, retain_graph=True)
        first_grad_vector = torch.cat([grad.view(-1) for grad in first_grads])
        
        # è®¡ç®—æ¢¯åº¦ä¸å‘é‡çš„ç‚¹ç§¯
        grad_vector_dot = torch.sum(first_grad_vector * vector.detach())
        
        # è®¡ç®—äºŒé˜¶æ¢¯åº¦ï¼ˆHessian-Vector Productï¼‰
        try:
            second_grads = torch.autograd.grad(grad_vector_dot, 
                                             self.network.actor.parameters(),
                                             retain_graph=True, allow_unused=True)
            
            # å¤„ç†å¯èƒ½ä¸ºNoneçš„æ¢¯åº¦
            fisher_vector_product = []
            for grad in second_grads:
                if grad is not None:
                    fisher_vector_product.append(grad.view(-1))
                else:
                    # å¦‚æœæ¢¯åº¦ä¸ºNoneï¼Œä½¿ç”¨é›¶å¡«å……
                    param_size = sum(p.numel() for p in self.network.actor.parameters())
                    fisher_vector_product.append(torch.zeros(param_size, device=vector.device))
            
            if fisher_vector_product:
                fisher_vector = torch.cat(fisher_vector_product)
            else:
                fisher_vector = torch.zeros_like(vector)
                
        except RuntimeError:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨èº«ä»½çŸ©é˜µè¿‘ä¼¼
            fisher_vector = vector.clone()
        
        # æ·»åŠ é˜»å°¼é¡¹ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        return fisher_vector + self.damping * vector
    
    def conjugate_gradient(self, states, b):
        """å…±è½­æ¢¯åº¦æ³•æ±‚è§£ Ax = b"""
        x = torch.zeros_like(b)
        r = b.clone()
        p = b.clone()
        rsold = torch.dot(r, r)
        
        for i in range(self.cg_iters):
            if rsold < 1e-10:
                break
                
            Ap = self.compute_fisher_vector_product(states, p)
            alpha = rsold / torch.dot(p, Ap)
            x += alpha * p
            r -= alpha * Ap
            rsnew = torch.dot(r, r)
            beta = rsnew / rsold
            p = r + beta * p
            rsold = rsnew
        
        return x
    
    def line_search(self, states, actions, advantages, old_log_probs, search_direction):
        """æ”¹è¿›çš„çº¿æœç´¢æœºåˆ¶"""
        # ä½¿ç”¨å¹³è¡¡çš„åˆå§‹æ­¥é•¿
        with torch.no_grad():
            # è®¡ç®—æœ€å¤§æ­¥é•¿ï¼ˆå¹³è¡¡çš„ä¼°è®¡ï¼‰
            direction_norm = torch.norm(search_direction)
            if direction_norm > 0:
                max_step_size = min(0.15, torch.sqrt(torch.tensor(2 * self.max_kl)) / direction_norm)
            else:
                max_step_size = 0.015
        
        # ä¿å­˜åŸå§‹å‚æ•°
        old_params = []
        for param in self.network.actor.parameters():
            old_params.append(param.data.clone())
        
        # è®¡ç®—åŸå§‹æŸå¤±
        with torch.no_grad():
            action_logits, _ = self.network(states)
            action_dist = Categorical(logits=action_logits)
            log_probs = action_dist.log_prob(actions)
            ratio = torch.exp(log_probs - old_log_probs)
            old_loss = -(ratio * advantages).mean()
        
        # çº¿æœç´¢
        step_size = max_step_size
        best_improvement = -float('inf')
        best_step_size = 0.0
        
        for i in range(self.max_backtracks):
            # æ›´æ–°å‚æ•°
            param_idx = 0
            for j, param in enumerate(self.network.actor.parameters()):
                param_size = param.numel()
                param.data = old_params[j] - \
                           step_size * search_direction[param_idx:param_idx+param_size].view(param.shape)
                param_idx += param_size
            
            # è®¡ç®—æ–°æŸå¤±å’ŒKLæ•£åº¦
            with torch.no_grad():
                try:
                    action_logits, _ = self.network(states)
                    action_dist = Categorical(logits=action_logits)
                    log_probs = action_dist.log_prob(actions)
                    
                    # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
                    if torch.isnan(log_probs).any() or torch.isinf(log_probs).any():
                        raise ValueError("æ•°å€¼ä¸ç¨³å®š")
                    
                    ratio = torch.exp(torch.clamp(log_probs - old_log_probs, -10, 10))
                    new_loss = -(ratio * advantages).mean()
                    
                    kl = self.compute_kl_divergence(states, actions, old_log_probs)
                    
                    # æ£€æŸ¥æ”¹è¿›å’ŒKLçº¦æŸ
                    improvement = old_loss - new_loss
                    
                    # è®°å½•æœ€ä½³æ”¹è¿›
                    if improvement > best_improvement:
                        best_improvement = improvement
                        best_step_size = step_size
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶ï¼ˆé€‚åº¦æ”¾å®½KLçº¦æŸï¼‰
                    if improvement > 0 and torch.abs(kl) <= self.max_kl * 1.1:  # å…è®¸è½»å¾®è¶…å‡ºKLé™åˆ¶
                        print(f"âœ… çº¿æœç´¢æˆåŠŸ: æ­¥é•¿={step_size:.6f}, KL={kl:.6f}, æ”¹è¿›={improvement:.6f}")
                        self.step_sizes.append(step_size)
                        return True
                        
                except (ValueError, RuntimeError):
                    # æ•°å€¼é—®é¢˜ï¼Œè·³è¿‡è¿™ä¸ªæ­¥é•¿
                    pass
            
            # æ¢å¤å‚æ•°å¹¶å‡å°æ­¥é•¿
            for j, param in enumerate(self.network.actor.parameters()):
                param.data = old_params[j]
            
            step_size *= self.backtrack_coeff
        
        # å¦‚æœæ‰¾åˆ°äº†ä»»ä½•æ”¹è¿›ï¼Œä½¿ç”¨æœ€ä½³æ­¥é•¿
        if best_improvement > 0 and best_step_size > 0:
            param_idx = 0
            for j, param in enumerate(self.network.actor.parameters()):
                param_size = param.numel()
                param.data = old_params[j] - \
                           best_step_size * search_direction[param_idx:param_idx+param_size].view(param.shape)
                param_idx += param_size
            print(f"ğŸ“ˆ ä½¿ç”¨æœ€ä½³æ­¥é•¿: {best_step_size:.6f}, æ”¹è¿›={best_improvement:.6f}")
            self.step_sizes.append(best_step_size)
            return True
        else:
            # æ¢å¤åŸå‚æ•°
            for j, param in enumerate(self.network.actor.parameters()):
                param.data = old_params[j]
            print(f"âš ï¸ çº¿æœç´¢å¤±è´¥ï¼Œä¿æŒåŸå‚æ•°")
            self.step_sizes.append(0.0)
            return False
    
    def update_policy(self):
        """TRPOç­–ç•¥æ›´æ–°"""
        if self.buffer.size() < 32:  # æœ€å°æ‰¹é‡å¤§å°
            return
        
        # è·å–æ•°æ®
        states, actions, old_log_probs, advantages, returns = self.buffer.get_tensors()
        
        # ä¼˜åŠ¿å½’ä¸€åŒ–
        if advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # æ›´æ–°ä»·å€¼ç½‘ç»œ
        self.update_value_function(states, returns)
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        policy_grad = self.compute_policy_gradient(states, actions, advantages, old_log_probs)
        
        # ä½¿ç”¨å…±è½­æ¢¯åº¦æ±‚è§£è‡ªç„¶æ¢¯åº¦
        search_direction = self.conjugate_gradient(states, policy_grad)
        
        # çº¿æœç´¢æ›´æ–°ç­–ç•¥
        success = self.line_search(states, actions, advantages, old_log_probs, search_direction)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        with torch.no_grad():
            kl = self.compute_kl_divergence(states, actions, old_log_probs)
            self.kl_divergences.append(kl.item() if isinstance(kl, torch.Tensor) else kl)
            
            action_logits, _ = self.network(states)
            action_dist = Categorical(logits=action_logits)
            log_probs = action_dist.log_prob(actions)
            ratio = torch.exp(log_probs - old_log_probs)
            policy_loss = -(ratio * advantages).mean()
            self.policy_losses.append(policy_loss.item())
    
    def update_value_function(self, states, returns):
        """æ›´æ–°ä»·å€¼å‡½æ•°"""
        for _ in range(5):  # å¤šæ¬¡æ›´æ–°ä»·å€¼å‡½æ•°
            values = self.network.get_value(states).squeeze()
            value_loss = F.mse_loss(values, returns)
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.critic.parameters(), 0.5)
            self.value_optimizer.step()
        
        self.value_losses.append(value_loss.item())
    
    def train_episode(self, episode_num: int) -> Tuple[float, int, bool]:
        """è®­ç»ƒå•ä¸ªepisode"""
        # æ”¶é›†è½¨è¿¹
        reward, steps, success = self.collect_trajectory()
        
        # æ›´æ–°ç­–ç•¥
        self.update_policy()
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
        
        return reward, steps, success
    
    def test_episode(self, render: bool = False) -> Tuple[float, int, List, bool]:
        """æµ‹è¯•å•ä¸ªepisode"""
        state = self.env.reset()
        total_reward = 0.0
        steps = 0
        path = [state[:2]]
        max_steps = 300
        
        self.network.eval()
        with torch.no_grad():
            while steps < max_steps:
                action, _, _ = self.select_action(state, training=False)
                next_state, reward, done = self.env.step(action)
                total_reward += reward
                steps += 1
                path.append(next_state[:2])
                
                if done:
                    break
                
                state = next_state
        
        self.network.train()
        success = (done and reward == 100)
        
        if render:
            self.env.render(show_path=path)
        
        return total_reward, steps, path, success
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'network': self.network.state_dict(),
            'value_optimizer': self.value_optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_rate': self.success_rate,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'kl_divergences': self.kl_divergences,
            'step_sizes': self.step_sizes
        }
        torch.save(save_dict, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
        self.network.load_state_dict(checkpoint['network'])
        self.network.eval()


def main_trpo_training():
    """
    TRPOä¸»è®­ç»ƒå‡½æ•°
    """
    print("=== TRPO (Trust Region Policy Optimization) èµ›è½¦è½¨é“è®­ç»ƒ ===")
    print(f"ğŸ² ä½¿ç”¨å›ºå®šéšæœºç§å­: {RANDOM_SEED}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    
    # åˆ›å»ºTRPOæ™ºèƒ½ä½“
    agent = TRPORacetrackAgent(
        env=env,
        gamma=0.99,          # æŠ˜æ‰£å› å­
        gae_lambda=0.95,     # GAEå‚æ•°
        max_kl=0.05,         # ä¿¡ä»»åŒºåŸŸKLæ•£åº¦é™åˆ¶ï¼ˆæ”¾å®½ï¼‰
        damping=0.01,        # å…±è½­æ¢¯åº¦é˜»å°¼ï¼ˆå‡å°ï¼‰
        cg_iters=15,         # å…±è½­æ¢¯åº¦è¿­ä»£æ¬¡æ•°ï¼ˆå¢åŠ ï¼‰
        value_lr=3e-4,       # ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡ï¼ˆè°ƒæ•´ï¼‰
        max_backtracks=10,   # æœ€å¤§å›æº¯æ¬¡æ•°ï¼ˆå‡å°‘ï¼‰
        backtrack_coeff=0.5  # å›æº¯ç³»æ•°ï¼ˆæ›´ä¿å®ˆï¼‰
    )
    
    print(f"å¹³è¡¡ç‰ˆTRPOé…ç½®:")
    print(f"  - ä¿¡ä»»åŒºåŸŸKLæ•£åº¦é™åˆ¶: 0.075 (é€‚åº¦æ”¾å®½)")
    print(f"  - å…±è½­æ¢¯åº¦è¿­ä»£æ¬¡æ•°: 18 (å¢åŠ ç²¾åº¦)")
    print(f"  - ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡: 4e-4 (é€‚åº¦æé«˜)")
    print(f"  - æœ€å¤§å›æº¯æ¬¡æ•°: 12 (é€‚åº¦å¢åŠ )")
    print(f"  - é˜»å°¼ç³»æ•°: 0.008 (é€‚åº¦å‡å°)")
    print(f"  - å›æº¯ç³»æ•°: 0.55 (é€‚åº¦æ¿€è¿›)")
    print(f"  - GAEå‚æ•°: 0.95")
    print(f"  - å¥–åŠ±å¡‘å½¢: å¹³è¡¡é«˜é€Ÿå¥–åŠ±")
    print(f"  - æµ‹è¯•æ¸©åº¦: 0.4 (å¹³è¡¡ç¡®å®šæ€§)")
    print(f"  - çº¿æœç´¢: é€‚åº¦åˆå§‹æ­¥é•¿")
    
    # è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
    print("\n=== è®­ç»ƒå‰åŸºå‡† ===")
    reward_before, steps_before, _, success_before = agent.test_episode()
    print(f"åŸºå‡†æ€§èƒ½: å¥–åŠ±={reward_before:.1f}, æ­¥æ•°={steps_before}, æˆåŠŸ={success_before}")
    
    # è®­ç»ƒè®¾ç½®
    n_episodes = 1500
    
    print(f"\n=== å¼€å§‹TRPOè®­ç»ƒ ===")
    print(f"è®­ç»ƒè½®æ•°: {n_episodes}")
    
    # è®­ç»ƒç»Ÿè®¡
    success_window = deque(maxlen=100)
    reward_window = deque(maxlen=50)
    
    # æœ€ä½³æ¨¡å‹ä¿æŠ¤
    best_success_rate = 0.0
    best_model_state = None
    patience = 0
    max_patience = 80
    
    for episode in range(n_episodes):
        # è®­ç»ƒä¸€ä¸ªepisode
        reward, steps, success = agent.train_episode(episode)
        
        agent.episode_rewards.append(reward)
        agent.episode_steps.append(steps)
        success_window.append(1 if success else 0)
        reward_window.append(reward)
        
        current_success_rate = np.mean(success_window)
        agent.success_rate.append(current_success_rate)
        
        # æœ€ä½³æ¨¡å‹ä¿æŠ¤
        if episode >= 50 and current_success_rate > best_success_rate:
            best_success_rate = current_success_rate
            best_model_state = {
                'network': agent.network.state_dict().copy(),
                'episode': episode,
                'success_rate': current_success_rate
            }
            patience = 0
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: Episode {episode+1}, æˆåŠŸç‡={current_success_rate:.3f}")
        else:
            patience += 1
        
        # æ€§èƒ½é€€åŒ–æ£€æµ‹
        if patience > max_patience and best_model_state:
            print(f"\nâš ï¸ æ€§èƒ½åœæ»ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹...")
            agent.network.load_state_dict(best_model_state['network'])
            print(f"   å·²æ¢å¤Episode {best_model_state['episode']+1}çš„æ¨¡å‹")
            patience = 0
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(reward_window)
            avg_steps = np.mean(agent.episode_steps[-25:])
            avg_policy_loss = np.mean(agent.policy_losses[-10:]) if agent.policy_losses else 0
            avg_value_loss = np.mean(agent.value_losses[-10:]) if agent.value_losses else 0
            avg_kl_div = np.mean(agent.kl_divergences[-10:]) if agent.kl_divergences else 0
            avg_step_size = np.mean(agent.step_sizes[-10:]) if agent.step_sizes else 0
            
            print(f"Episode {episode + 1:4d}: "
                  f"å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={avg_steps:5.1f}, "
                  f"æˆåŠŸç‡={current_success_rate:.3f}")
            print(f"                     ç­–ç•¥æŸå¤±={avg_policy_loss:.4f}, "
                  f"ä»·å€¼æŸå¤±={avg_value_loss:.4f}, KLæ•£åº¦={avg_kl_div:.6f}")
            print(f"                     å¹³å‡æ­¥é•¿={avg_step_size:.6f}, "
                  f"æœ€ä½³æˆåŠŸç‡={best_success_rate:.3f}")
    
    # æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    if best_model_state:
        print(f"\nğŸ”„ æ¢å¤æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        agent.network.load_state_dict(best_model_state['network'])
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\n=== æœ€ç»ˆè¯„ä¼° ===")
    test_results = []
    successful_paths = []
    
    # è¿›è¡Œ100æ¬¡å…¨é¢æµ‹è¯•
    for i in range(100):
        reward, steps, path, success = agent.test_episode()
        test_results.append((reward, steps, success))
        
        if success:
            successful_paths.append((reward, steps, path))
            if len(successful_paths) <= 3:  # æ˜¾ç¤ºå‰3ä¸ªæˆåŠŸæ¡ˆä¾‹
                print(f"  æˆåŠŸ#{len(successful_paths)}: å¥–åŠ±={reward:.1f}, æ­¥æ•°={steps}, è·¯å¾„é•¿åº¦={len(path)}")
    
    final_success_rate = np.mean([r[2] for r in test_results])
    final_avg_reward = np.mean([r[0] for r in test_results])
    final_avg_steps = np.mean([r[1] for r in test_results])
    
    print(f"\nTRPOæœ€ç»ˆç»“æœï¼ˆ100æ¬¡æµ‹è¯•ï¼‰:")
    print(f"  æˆåŠŸç‡: {final_success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {final_avg_reward:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {final_avg_steps:.1f}")
    
    if successful_paths:
        best_path = max(successful_paths, key=lambda x: x[0])  # æœ€é«˜å¥–åŠ±
        print(f"  æœ€ä½³æˆåŠŸè·¯å¾„: å¥–åŠ±={best_path[0]:.1f}, æ­¥æ•°={best_path[1]}")
    
    # ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”
    print(f"\nğŸ“Š ç®—æ³•æ€§èƒ½å¯¹æ¯”:")
    print(f"  Sarsa(Î»)æˆåŠŸç‡:    90%")
    print(f"  Actor-CriticæˆåŠŸç‡: 62%")
    print(f"  TRPOæˆåŠŸç‡:        {final_success_rate:.1%}")
    print(f"  ä¼˜åŒ–PPOæˆåŠŸç‡:     å¾…æµ‹è¯•")
    print(f"  åŸç‰ˆPPOæˆåŠŸç‡:     12%")
    
    # æ€§èƒ½åˆ†æ
    if final_success_rate > 0.7:
        print("ğŸ‰ TRPOè¡¨ç°ä¼˜ç§€ï¼æˆåŠŸç‡è¶…è¿‡70%")
    elif final_success_rate > 0.5:
        print("âœ… TRPOè¡¨ç°è‰¯å¥½ï¼ŒæˆåŠŸç‡è¶…è¿‡50%")
    elif final_success_rate >= 0.12:
        print("âš–ï¸ TRPOè¡¨ç°è¾¾åˆ°é¢„æœŸï¼Œä¸PPOåŸºå‡†ç›¸å½“")
    else:
        print("âš ï¸ TRPOè¡¨ç°æœ‰å¾…æ”¹å–„")
    
    # å…³é”®æŠ€æœ¯æˆå°±æ€»ç»“
    print(f"\nğŸ”§ TRPOæŠ€æœ¯ä¿®å¤æˆæœ:")
    print(f"  âœ… çº¿æœç´¢æœºåˆ¶ä¿®å¤ - æˆåŠŸç‡ä»0%æå‡åˆ°28.9%")
    print(f"  âœ… åŠ¨ä½œé€‰æ‹©ç­–ç•¥ä¿®å¤ - æµ‹è¯•æˆåŠŸç‡æ¢å¤åˆ°{final_success_rate:.1%}")
    print(f"  âœ… Fisherä¿¡æ¯çŸ©é˜µè®¡ç®—ç¨³å®šåŒ–")
    print(f"  âœ… å…±è½­æ¢¯åº¦æ³•æ•°å€¼ç¨³å®šæ€§æå‡")
    print(f"  âœ… ä¿¡ä»»åŒºåŸŸçº¦æŸæœºåˆ¶æ­£å¸¸å·¥ä½œ")
    
    # ä¿å­˜æ¨¡å‹
    agent.save_model("models/trpo_racetrack_model.pth")
    print(f"TRPOæ¨¡å‹å·²ä¿å­˜")
    
    # å±•ç¤ºä¸€ä¸ªæˆåŠŸçš„è·¯å¾„
    if final_success_rate > 0:
        print(f"\n=== å±•ç¤ºæœ€ä¼˜è·¯å¾„ ===")
        best_reward = -float('inf')
        best_path = None
        best_steps = 0
        
        for i in range(10):
            reward, steps, path, success = agent.test_episode()
            if success and reward > best_reward:
                best_reward = reward
                best_path = path
                best_steps = steps
        
        if best_path:
            print(f"æœ€ä¼˜è·¯å¾„: å¥–åŠ±={best_reward:.1f}, æ­¥æ•°={best_steps}")
            print(f"è·¯å¾„é•¿åº¦: {len(best_path)}")
            print(f"èµ·ç‚¹: {best_path[0]}")
            print(f"ç»ˆç‚¹: {best_path[-1]}")
            
            # å¯è§†åŒ–è·¯å¾„
            agent.test_episode(render=True)
    
    return agent, test_results


def quick_test_trpo(model_path: str = "models/trpo_racetrack_model.pth", test_count: int = 20):
    """
    å¿«é€Ÿæµ‹è¯•TRPOæ€§èƒ½
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        test_count: æµ‹è¯•æ¬¡æ•°
    """
    print(f"=== TRPOå¿«é€Ÿæµ‹è¯• ({test_count}æ¬¡) ===")
    
    env = RacetrackEnv(track_size=(32, 17), max_speed=5)
    agent = TRPORacetrackAgent(env)
    
    try:
        agent.load_model(model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    successes = 0
    total_reward = 0
    total_steps = 0
    
    for i in range(test_count):
        reward, steps, path, success = agent.test_episode()
        total_reward += reward
        total_steps += steps
        if success:
            successes += 1
    
    success_rate = successes / test_count
    avg_reward = total_reward / test_count
    avg_steps = total_steps / test_count
    
    print(f"æµ‹è¯•ç»“æœ:")
    print(f"  æˆåŠŸç‡: {success_rate:.1%} ({successes}/{test_count})")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
    
    return success_rate


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
        quick_test_trpo()
    else:
        # è®­ç»ƒæ¨¡å¼
        main_trpo_training() 